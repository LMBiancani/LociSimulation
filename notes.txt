NOTES - LociSimulation
github: https://github.com/LMBiancani/LociSimulation.git
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"

2025.10.09

ws_allocate -m biancani@uri.edu -r 5 -G pi_rsschwartz_uri_edu LociSimulation 30

Info: creating workspace.
/scratch4/workspace/biancani_uri_edu-LociSimulation
remaining extensions  : 5
remaining time in days: 30

Starting with mammal loci from https://github.com/LMBiancani/PlacentalPolytomy
Original Path to aligned loci:
"/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"

2025.10.09

cd /scratch4/workspace/biancani_uri_edu-LociSimulation

copy mammal loci to scratch space:

transfer.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="tar_transfer"
#SBATCH --time=48:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -p uri-cpu
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=6G

# Define source and destination directories
SOURCE_DIR="/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"
DEST_DIR="/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci"

# Make sure the destination exists
mkdir -p "$DEST_DIR"
echo "Started at $(date)"

# Stream the directory (preserving its name) to scratch
tar -cf - -C "$(dirname "$SOURCE_DIR")" "$(basename "$SOURCE_DIR")" | pv | tar -xf - -C "$DEST_DIR"

echo "Finished at $(date)"
---------------------------------
sbatch transfer.sh
Submitted batch job 45655769

sacct -j 45655769 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
45655769          0:0   00:01:48
45655769.ba+      0:0   00:01:48   2114772K
45655769.ex+      0:0   00:01:48          0

mkdir /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation

github: https://github.com/LMBiancani/LociSimulation.git

2025.10.20

Use git subtree to copy Molly's updated simulation/machine learning scripts (and preserve commit history)
Molly's repo: https://github.com/mollyodonnellan/PML.git

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
git subtree add --prefix=Scripts https://github.com/mollyodonnellan/PML.git main

Mammal data is too large for AMAS so script needs updating to run through loci in batches.

run_amas.py is a custom batch concatenation wrapper around AMAS.
It processes FASTA alignments in chunks of 1000 files at a time (to avoid overloading AMAS input limitations), concatenates them, and appends the results into cumulative files.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano run_amas.py
---------------------------------
#!/usr/bin/env python3
"""
run_amas.py
------------
Concatenates FASTA alignments in batches using AMAS to avoid memory overload.
Creates a concatenated alignment file and a corresponding partition file.

Usage:
    python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>
"""

import sys
import glob
import subprocess
from pathlib import Path

# -----------------------------
# Parse command-line arguments
# -----------------------------
if len(sys.argv) != 4:
    sys.exit("Usage: python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>")

fasta_folder = Path(sys.argv[1])
total_cores = sys.argv[2]
amas = sys.argv[3]

# -----------------------------
# Gather all FASTA files
# -----------------------------
files = sorted(glob.glob(str(fasta_folder / "*.fasta")))
if not files:
    sys.exit(f"No FASTA files found in {fasta_folder}")

# -----------------------------
# Prepare output filenames
# -----------------------------
concat_out = "concatenated.fasta"
part_out = "concatenated_partitions.txt"

# Clean up any old results
subprocess.run(["rm", "-f", concat_out, part_out])

# -----------------------------
# Define a helper to run AMAS
# -----------------------------
def run_amas_batch(batch_files, first_batch):
    """Run AMAS concat on a batch of files and append results."""
    cmd = [
        "python", amas, "concat",
        "-c", total_cores,
        "-t", "amas_output_temp.fasta",
        "-f", "fasta",
        "-d", "dna",
        "--out-format", "fasta",
        "--part-format", "raxml",
        "-p", "partitions_temp.txt",
        "-i"
    ] + batch_files
    subprocess.run(cmd, check=True)

    if first_batch:
        # First batch: initialize output files
        subprocess.run(["sed", "-e", "$a\\", "amas_output_temp.fasta"], stdout=open(concat_out, "w"))
        subprocess.run(["sed", "-e", "$a\\", "partitions_temp.txt"], stdout=open(part_out, "w"))
    else:
        # Subsequent batches: append excluding the first line
        with open(concat_out, "a") as cat_out:
            subprocess.run("sed -e 1d amas_output_temp.fasta | sed -e '$a\\'", shell=True, stdout=cat_out)
        with open(part_out, "a") as part_cat:
            subprocess.run("sed -e 1d partitions_temp.txt | sed -e '$a\\'", shell=True, stdout=part_cat)

# -----------------------------
# Process files in batches of 1000
# -----------------------------
batch_size = 1000
batch = []
first_batch = True

for f in files:
    batch.append(f)
    if len(batch) == batch_size:
        run_amas_batch(batch, first_batch)
        first_batch = False
        batch = []

# Process remaining files
if batch:
    run_amas_batch(batch, first_batch)

print("AMAS concatenation complete.")
---------------------------------

1_run_amas.sh runs AMAS on an empirical dataset using a helper Python script (run_amas.py), which wraps around the AMAS.py concat command.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano 1_run_amas.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="AMAS"
#SBATCH --time=96:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=12   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH -p uri-cpu
#SBATCH --mail-user="leann.biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

## Number of processor cores specified above:
Cores=12
## Path to project directory
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
## Path to scripts:
Scripts="$Project/LociSimulation/Scripts/0_data_prep"
## Path to output folder (will be created if necessary):
Output="$Project/output/mammals"
## Path to empirical dataset in fasta format:
Data="$Project/mammal_loci"
## Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

date

mkdir -p ${Output}
cd ${Output}

python3 ${Scripts}/run_amas.py ${Data} ${Cores} ${AMAS}

rm amas_output_temp.fasta
rm partitions_temp.txt

date
---------------------------------
sbatch 1_run_amas.sh
Submitted batch job 47030087

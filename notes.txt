NOTES - LociSimulation
github: https://github.com/LMBiancani/LociSimulation.git
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"

2025.10.09

ws_allocate -m biancani@uri.edu -r 5 -G pi_rsschwartz_uri_edu LociSimulation 30

Info: creating workspace.
/scratch4/workspace/biancani_uri_edu-LociSimulation
remaining extensions  : 5
remaining time in days: 30

Starting with mammal loci from https://github.com/LMBiancani/PlacentalPolytomy
Original Path to aligned loci:
"/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"

2025.10.09

cd /scratch4/workspace/biancani_uri_edu-LociSimulation

copy mammal loci to scratch space:

transfer.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="tar_transfer"
#SBATCH --time=48:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -p uri-cpu
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=6G

# Define source and destination directories
SOURCE_DIR="/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"
DEST_DIR="/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci"

# Make sure the destination exists
mkdir -p "$DEST_DIR"
echo "Started at $(date)"

# Stream the directory (preserving its name) to scratch
tar -cf - -C "$(dirname "$SOURCE_DIR")" "$(basename "$SOURCE_DIR")" | pv | tar -xf - -C "$DEST_DIR"

echo "Finished at $(date)"
---------------------------------
sbatch transfer.sh
Submitted batch job 45655769

sacct -j 45655769 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
45655769          0:0   00:01:48
45655769.ba+      0:0   00:01:48   2114772K
45655769.ex+      0:0   00:01:48          0

mkdir /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation

github: https://github.com/LMBiancani/LociSimulation.git

2025.10.20

Use git subtree to copy Molly's updated simulation/machine learning scripts (and preserve commit history)
Molly's repo: https://github.com/mollyodonnellan/PML.git

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
git subtree add --prefix=Scripts https://github.com/mollyodonnellan/PML.git main

Mammal data is too large for AMAS so script needs updating to run through loci in batches.

run_amas.py is a custom batch concatenation wrapper around AMAS.
It processes FASTA alignments in chunks of 1000 files at a time (to avoid overloading AMAS input limitations), concatenates them, and appends the results into cumulative files.
Outputs:
concatenated.fasta
partitions.txt

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano run_amas.py
---------------------------------
#!/usr/bin/env python3
"""
run_amas.py
------------
Concatenates FASTA alignments in batches using AMAS (to avoid overloading AMAS input limitations).
Creates a concatenated alignment file and a corresponding partition file:
concatenated.fasta
partitions.txt

Usage:
    python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>
"""

import sys
import glob
import subprocess
import os

# --- Input arguments ---
fasta_folder = sys.argv[1]
total_cores = sys.argv[2]
amas = sys.argv[3]

# --- Collect all fasta files ---
files = glob.glob(os.path.join(fasta_folder, "*.fasta"))
files.sort()  # ensure consistent, reproducible order

batch_size = 1000
batch_outputs = []
batch_parts = []

count = 0
fileList = []
batch_num = 1

# --- Process loci in batches ---
for f in files:
    fileList.append(f)
    count += 1

    if count == batch_size:
        batch_fasta = f"amas_batch_{batch_num}.fasta"
        batch_part = f"partitions_batch_{batch_num}.txt"

        cmd = (
            f"python3 {amas} concat "
            f"-f fasta -d dna --out-format fasta --part-format raxml "
            f"-i {' '.join(fileList)} "
            f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
        )
        print(f"\n=== Running AMAS on batch {batch_num} ({len(fileList)} files) ===")
        subprocess.call(cmd, shell=True)

        batch_outputs.append(batch_fasta)
        batch_parts.append(batch_part)
        fileList = []
        count = 0
        batch_num += 1

# --- Final partial batch (if any) ---
if len(fileList) > 0:
    batch_fasta = f"amas_batch_{batch_num}.fasta"
    batch_part = f"partitions_batch_{batch_num}.txt"

    cmd = (
        f"python3 {amas} concat "
        f"-f fasta -d dna --out-format fasta --part-format raxml "
        f"-i {' '.join(fileList)} "
        f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
    )
    print(f"\n=== Running AMAS on final batch {batch_num} ({len(fileList)} files) ===")
    subprocess.call(cmd, shell=True)

    batch_outputs.append(batch_fasta)
    batch_parts.append(batch_part)

# --- Final concatenation across all batches ---
print("\n=== Performing final concatenation across batches ===")

cmd_final = (
    f"python3 {amas} concat "
    f"-f fasta -d dna --out-format fasta --part-format raxml "
    f"-i {' '.join(batch_outputs)} "
    f"-c {total_cores} -t concatenated.fasta -p partitions.txt"
)
subprocess.call(cmd_final, shell=True)

# --- Cleanup temporary batch files ---
print("\n=== Cleaning up intermediate files ===")
for f in batch_outputs + batch_parts:
    try:
        os.remove(f)
    except OSError:
        print(f"Warning: could not remove {f}")

print("\n=== run_amas.py execution completed. ===")
print("Output files:")
print(" - concatenated.fasta")
print(" - partitions.txt\n")
---------------------------------

0.0_amas_concat.sh runs AMAS on an empirical dataset to concatenate input fasta files and prepare partitions ahead of IQTree run.
Uses a helper Python script (run_amas.py), which wraps around the AMAS.py concat command.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano 0.0_amas_concat.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="AMAS"
#SBATCH --time=2:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory (will be created if necessary)
Output="$Project/output/mammals"
# Path to aligned loci in fasta format:
Data="$Project/mammal_loci/01_SISRS_loci_filtered"
# Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"
# Number of processor cores per node:
Cores=$(echo $SLURM_TASKS_PER_NODE | sed 's/(x.*)//')

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

date
mkdir -p ${Output}/0.0_concatenated
cd ${Output}/0.0_concatenated

#Concatenate input fasta files and prepare partitions ahead of IQTree run
python3 ${Scripts}/0_data_prep/run_amas.py ${Data} ${Cores} ${AMAS}

date
---------------------------------
sbatch 0.0_amas_concat.sh
Submitted batch job 49112018
sacct -j 49112018 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
49112018          0:0   00:01:39
49112018.ba+      0:0   00:01:39   5584204K
49112018.ex+      0:0   00:01:40          0

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

0.1_iqtree_empirical.sh will use iqtree to infer an empirical tree.

nano 0.1_iqtree_empirical.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="IQTREE"
#SBATCH --time=96:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=24
#SBATCH --mem=250G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to output directory
Output="$Project/output/mammals"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"
# Path to output files from 0.0_amas_concat.sh
Input="$Output/0.0_concatenated"
# Number of cpus per task:
Threads=${SLURM_CPUS_PER_TASK}

module purge

date
mkdir -p ${Output}/0.1_empirical_tree
cd ${Output}/0.1_empirical_tree

# --- Check for input files produced by 0.0_amas_concat.sh---

if [[ ! -f "$Input/concatenated.fasta" || ! -f "$Input/partitions.txt" ]]; then
    echo "Error: concatenated.fasta or partitions.txt not found in ${Input}"
    exit 1
fi

# --- Run IQ-TREE ---
# Flags:
#   -nt: number of CPU threads
#   -spp: partition file allowing different evolutionary rates per partition
#   -pre: prefix for output files
#   -m MFP: ModelFinder Plus for best-fit model selection
#   -bb: ultrafast bootstrap replicates
#   -alrt: SH-like approximate likelihood test replicates

${IQTREE} -nt ${Threads} \
    -s $Input/concatenated.fasta \
    -spp $Input/partitions.txt \
    -pre inferenceEmpirical \
    -m MFP -bb 1000 -alrt 1000

date
---------------------------------
sbatch 0.1_iqtree_empirical.sh
Submitted batch job 47065851
sacct -j 47065851 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
47065851          0:0 2-17:22:51
47065851.ba+      0:0 2-17:22:51 149134624K
47065851.ex+      0:0 2-17:22:51       256K

2025.11.21

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano install.packages.R
---------------------------------
# Capture command-line arguments (package names to check and install)
args <- commandArgs(trailingOnly = TRUE)

if (length(args) == 0) {
  stop("Error: No package names provided as arguments. Exiting.", call. = FALSE)
}

# The packages to install are now in the 'args' vector
packages_to_install <- args

# Function to check, install, and load packages
install_and_load <- function(pkg) {
  # Check if the package is installed
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Package", pkg, "not found. Installing..."))
    # Install the package from CRAN
    install.packages(pkg, dependencies = TRUE, repos = "https://cloud.r-project.org")
  } else {
    message(paste("Package", pkg, "is already installed."))
  }

  # Load the package (library function)
  library(pkg, character.only = TRUE)
  message(paste("Package", pkg, "loaded successfully."))
}

# Apply the function to the list of packages
invisible(sapply(packages_to_install, install_and_load))
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano empirical_tree_processor.R
### ---------------------------------
### ## Capture Command Line Arguments
###
### # Arguments must be passed in the following order:
### # [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
### # [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
### # [3] format_tree_out     (Base directory for output files)
### # [4] out_tip             (The tip name to use as the outgroup for rooting)
### # [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
### # [6] gen_time            (Estimated generation time in years for all species in the tree)
### # [7] simphy_seed1        (First random number seed for generate_params.txt)
### # [8] simphy_seed2        (Second random number seed for generate_params.txt)
###
### args <- commandArgs(trailingOnly = TRUE)
###
### # Check if the correct number of arguments was provided
### if (length(args) < 8) {
###   stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
### }
###
### # Assign arguments to variables
### mod_write_tree2 <- args[1]
### treefile <- args[2]
### format_tree_out <- args[3]
### out_tip <- args[4]
### simphy_seed1 <- args[7]
### simphy_seed2 <- args[8]
###
### # Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
### scale_factor <- as.numeric(args[5])/as.numeric(args[6])
###
### # --- 2. Load Libraries and Setup ---
###
### library(ape)
### library(ggplot2)
### library(geiger)
### library(ggtree)
###
### ## Force a headless friendly bitmap device for the session
### options(bitmapType = "cairo")
###
### # Adjust path to modified.write.tree2.R
### source(mod_write_tree2)
### assignInNamespace(".write.tree2", .write.tree2, "ape")
###
### # --- 3. Process and Save Empirical Tree ---
###
### # Empirical tree:
### tree <- read.tree(treefile)
###
### # Root the tree and save the check image
### tree <- root(tree, outgroup = out_tip)
### p <- ggtree(tree) + theme_tree2() + geom_tiplab()
### ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))
###
### # Check the tree rooted correctly. Then transform to ultrametric and rescale
###
### tree_um <- chronos(tree)
### class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
### tree_um <- rescale(tree_um, model = "depth", scale_factor)
###
### # Save the ultrametric tree check image
### q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
### ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))
###
### # --- 4. Prepare for SimPhy and Write Outputs ---
###
### # Replace labels with numbers, strip node labels, and write out the tree
### tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
### tree_um$node.label <- NULL
### write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)
###
### # Write the seeds for subsequent dataset parameter simulations
### write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))
###
### print("Tree processing complete and output files saved.")
### ---------------------------------

## INPUTS for empirical_tree_processor.R

# generation scaling factor: Number of generations in the tree: absolute tree age in years divided by average generation time of species in the tree.
total tree depth (years) / average generation time (years)

# total tree depth: age of most recent common ancestor of all species in the tree.
For this mammal data, this would be the age of Theria or age of the common ancestor of placental mammals (ingroup) and marsupials (outgroup)
total tree depth (years) is estimated to be 168 mya
Citations:
2019 - Upham NS, Esselstyn JA, Jetz W (2019) Inferring the mammal tree: Species-level sets of phylogenies for questions in ecology, evolution, and conservation. PLoS Biol 17(12): e3000494.https://doi.org/10.1371/journal.pbio.3000494
168 Fig 1
2017 - Thomas L. Dunwell, Jordi Paps, Peter W. H. Holland; Novel and divergent genes in the evolution of placental mammals. Proc Biol Sci 1 October 2017; 284 (1864): 20171357. https://doi.org/10.1098/rspb.2017.1357
140-191 "The common ancestor of placentals and marsupials dates to approximately 140 – 191 million years ago (Ma)"

tree_depth=168000000

# average generation time (years)
We used a estimated generation time of 4.5 years for Theria. This value is chosen not as the arithmetic mean or median of all extant species (which is skewed by small-bodied rodents and bats), but as a phylogenetically-informed effective generation time that is consistent with the estimated rate of mutation at the deepest nodes of the mammalian tree. This approach accounts for the allometric relationship between body mass and generation length, a relationship thoroughly characterized across Mammalia [cite Pacifici et al., 2013]. Our chosen rate falls within the range utilized by major phylogenomic studies (e.g., Meredith et al., 2011), which established evolutionary rates for ancestral nodes that are consistent with a moderately sized stem mammal ancestor and a generation time in the 4–5 year range."
Citation:
Pacifici, Michela, et al. "Generation length for mammals." Nature Conservation 5 (2013): 89-94.
Meredith, Robert W., et al. "Impacts of the Cretaceous Terrestrial Revolution and KPg extinction on mammal diversification." science 334.6055 (2011): 521-524.

gen_time=4.5

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano 1.0_format_tree.sh - OLD VERSION
### ---------------------------------
### #!/bin/bash
### #SBATCH --job-name="process_tree"
### #SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
### #SBATCH --nodes=1                      # Number of nodes
### #SBATCH --ntasks=1                     # Total number of tasks (processes)
### #SBATCH --cpus-per-task=2              # Number of CPU cores per task
### #SBATCH --mem-per-cpu=6G               # Memory per cpu
### #SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
### #SBATCH --mail-type=ALL
### #SBATCH -p uri-cpu                     # Partition/queue to submit job to
###
### # --- Variables ---
### # Path to project directory:
### Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
### # Path to scripts directory:
### Scripts="$Project/LociSimulation/Scripts"
### # Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
### mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
### # Path to output directory
### Output="$Project/output/mammals"
### # Path to treefile generated by 0.1_iqtree_empirical.sh
### treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
### # List of necessary R packages (separated by spaces)
### R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger"
### # Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
### tree_depth=168000000
### # Generation time: Estimated generation time (in years) of all species in the tree:
### gen_time=4.5
### # Taxon (tip name) to use as the outgroup for rooting
### out_tip="Didelphis_virginiana"
### # Random number seeds for generate_params.txt
### simphy_seed1=12345
### simphy_seed2=67890
###
### # create output subdirectory:
### format_tree_out=$Output/1.0_formatted_empirical_tree
### mkdir -p $format_tree_out
### cd $format_tree_out
###
### module purge
### module load uri/main
### module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
### module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
### module load R/4.3.2-gfbf-2023a # Loads updated R version
### # This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
### # This must be done AFTER module loading to override potential downgrades.
### export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
### export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
###
### ## Install R packages
###
### # add local space for R packages (won't ask about install location):
### mkdir -p ~/R-packages
### export R_LIBS=~/R-packages
###
### # install R packages
### Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages
###
###
### ## Process Empirical tree
### Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $out_tip $tree_depth $gen_time $simphy_seed1 $simphy_seed2
###
### date
### ---------------------------------
### sbatch 1.0_format_tree.sh

Submitted batch job 50314567 - module Error
Error: failing to load the Rcpp.so library due to a missing GLIBCXX_3.4.32 symbol, was caused by a compiler mismatch on the HPC cluster. The R package Rcpp (required by ape) had been compiled using a newer version of the GNU Compiler Collection (GCC) than the one available to R at runtime (GCCcore/11.2.0). This was resolved by updating the Slurm script's module load strategy: we first load the newest available toolchain (foss/2024a) to provide the necessary, updated C++ shared libraries (libstdc++.so.6), and then load a compatible R module (R/4.3.2-gfbf-2023a). Crucially, we also forced the deletion and reinstallation of all local R packages (rm -rf ~/R-packages/*) to ensure they were compiled cleanly using the newly loaded, compatible foss/2024a toolchain.
FIX: Resolve GLIBCXX compiler mismatch in R environment - Updates 1.0_format_tree.sh to load the latest foss toolchain and forces R package reinstallation to fix dependency loading error.

sbatch 1.0_format_tree.sh
Submitted batch job 50314943 - module error
Error: The R package installation failed for geiger and phytools because two of their indirect dependencies, MASS and clusterGeneration, were not found or did not install correctly during the automated dependency resolution. This kind of failure is common on HPC systems when packages have many dependencies. The fix was to explicitly include the missing top-level dependency packages (MASS, clusterGeneration, and phytools) alongside the target packages (ape, ggplot2, geiger, ggtree) in the $R\_packages variable in the Slurm script, forcing the clean reinstallation of all necessary components.
FIX: Explicitly list R package dependencies - Adds MASS, clusterGeneration, and phytools to R_packages variable in 1.0_format_tree.sh to resolve dependency failures for geiger during installation.

sbatch 1.0_format_tree.sh
Submitted batch job 50315543 - module Error
Error: The R package installation failed because the latest version of the MASS package requires R version ≥4.4.0, which is incompatible with the cluster's available R 4.3.2 module. The previous attempt to pin the version failed because install.packages() ignores the version argument when using the main CRAN mirror.
The definitive fix required opening an interactive session (see below) to install MASS version 7.3-60 (a compatible version) by pointing directly to its source file on the CRAN Archive URL (repos = NULL). This forces the installation of the correct package version, resolving the R version compatibility conflict and allowing dependent packages like geiger to install successfully.

# --- Install MASS separately with a pinned, compatible version for R 4.3.2 ---
# Version 7.3-60 is compatible with R 4.3.x
salloc # open interactive session
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export R_LIBS=~/R-packages
R
# --- Install MASS using the direct archive URL (CRITICAL FIX) ---
# This forces the download of the compatible version (7.3-60) for R 4.3.2.
install.packages("https://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.tar.gz",
                 repos = NULL, # MUST be NULL when installing from a URL
                 type = "source",
                 dependencies = TRUE)

sbatch 1.0_format_tree.sh
Submitted batch job 50315977 - Error
The installation of the R package magick failed because the compiler could not find the external system development files (Magick++.h and the corresponding libraries).
Error: <stdin>:1:10: fatal error: Magick++.h: No such file or directory
Cause: The ImageMagick system library and its development headers are not in your environment's default search paths (neither via a simple module nor the base system).
The R package installation encountered two simultaneous failures:
System Dependency Failure: The R package magick failed to compile because its external system dependency, the ImageMagick development library (Magick++.h), was not found in the environment's default search path.
Recurring Compiler Mismatch: The GLIBCXX_3.4.32 error reappeared for phytools (via igraph) because the conflicting uri/main module was loaded before the necessary foss/2024a toolchain, resulting in a library downgrade.
The fix required a final comprehensive adjustment to the module load order in the Slurm script: explicitly loading the ImageMagick/7.1.1-15-GCCcore-12.3.0 module to provide the missing system headers, and ensuring the foss/2024a toolchain is loaded last to correctly define the newest C++ library environment. This ensures all low-level, system, and R-version-specific dependencies are correctly satisfied.
FIX: Resolve final compilation/dependency errors - Loads ImageMagick module to provide system headers for R 'magick' package, and finalizes module load order (foss/2024a last) to fix recurring GLIBCXX compiler downgrade error

sbatch 1.0_format_tree.sh
Submitted batch job 50316544 - Error
More dependency issues. Try adding igraph to list of R packages

sbatch 1.0_format_tree.sh
Submitted batch job 50316710 - Error
The core problem was a persistent compiler linking error (GLIBCXX_3.4.32 not found), which manifested across multiple R package installations (igraph, phangorn, phytools). Although the correct, newest compiler toolchain (foss/2024a) was loaded, the conflicting uri/main system module or standard library search paths kept defaulting to an older, incompatible library (GCCcore/12.3.0). The definitive fix was to use the path retrieved via module show to manually update the linker path. The Slurm script now explicitly sets LD\_LIBRARY\_PATH to prioritize the directory containing the newest C++ libraries (/modules/uri\_apps/software/GCCcore/13.3.0/lib64), forcing the system to link all new R packages correctly and resolving the chronic loading error.
FIX: Resolve chronic GLIBCXX linking error - Forces the LD_LIBRARY_PATH in 1.0_format_tree.sh to the newest GCCcore path (/modules/uri_apps/software/GCCcore/13.3.0/lib64). This overrides system defaults and fixes the recurring GLIBCXX_3.4.32 linker failure across all compiled R packages.

sbatch 1.0_format_tree.sh
Submitted batch job 50316861 - Error

# --- Install devtools and ggtree (via interactive session) ---
salloc
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages
R
# 1. Install devtools (needed for install_github)
if (!requireNamespace("devtools", quietly = TRUE)) {
    install.packages("devtools", dependencies = TRUE, repos = "https://cloud.r-project.org")
}
# 2. Load devtools
library(devtools)
# 3. Install ggtree from GitHub (this often bypasses strict CRAN/Bioconductor version checks)
# Note: You may also need to install the 'BiocManager' package first if this fails.
install_github("YuLab-SMU/ggtree")

sbatch 1.0_format_tree.sh
Submitted batch job 50317295 - worked!!

sacct -j 50317295 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50317295          0:0   00:00:24
50317295.ba+      0:0   00:00:24    428496K
50317295.ex+      0:0   00:00:24          0

* Check re-rooted and ultrametric trees! - Issue. only rooting with one taxa instead of entire outgroup.
Thoughts:
As of now the tree is being rooted by specifying out_tip (tip name to use as the outgroup for rooting). However, many trees have a clade as an outgroup, not a single taxa. I know ggtree can root using a node instead of a taxon:
rooted_tree <- root(tree, node=109, resolve.root = TRUE, edgelabel=TRUE)
However, I don't know the node number for my outgroup.
The following code would produce a tree where the nodes are labeled with their node numbers, but I'd need to manually look at the tree and see what the outgroup node is labeled.
t <- ggtree(tree, layout="rectangular", size=1, branch.length="none") + geom_text(aes(label=node)) + geom_tiplab(align=TRUE, hjust=-.15)
What I would like to do, is write an R ggtree function or similar that takes a list of all the outgroup taxa and returns the number of the node the represents the common ancestor for those taxa (which would subsequently be used for rooting the tree)
To root a phylogenetic tree using an entire outgroup clade instead of a single taxon, the manual lookup of node numbers in a ggtree plot is unnecessary. The required functionality is available in the ape package, which is already loaded in the script. The function to use is getMRCA() (Most Recent Common Ancestor). This function efficiently takes two arguments: the phylogenetic tree object and a vector containing the names of all the outgroup tip labels. It then automatically returns the specific node index corresponding to the common ancestor of that entire outgroup clade. This node index can then be passed directly to the root() function for accurate clade-based rooting.

To transition from rooting the phylogenetic tree using a single taxon to using an entire outgroup clade, the script was updated to leverage the ape::getMRCA() function. This involved two main changes:
Input Change: The command-line argument $out_tip was replaced with a string of comma-separated outgroup taxa (e.g., Didelphis_virginiana,Monodelphis_domestica).
R Logic: The empirical_tree_processor.R script was modified to:
Parse the input string into an R vector of tip labels using strsplit().
Use getMRCA(tree, tip = outgroup_taxa) to programmatically find the integer node index corresponding to the most recent common ancestor of all specified outgroup tips.
Replace the single-taxon root(..., outgroup=...) call with the root(..., node=outgroup_node) call, ensuring the tree is rooted correctly on the ancestral node of the entire outgroup clade.

Making updates scripts...

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.0_prep_R_env.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="R_pkgs"
#SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=1              # Number of CPU cores per task
#SBATCH --mem-per-cpu=6G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# List of necessary R packages (separated by spaces)
R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger MultiRNG EnvStats extraDistr tidyverse castor"

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

## Install R packages

# add local space for R packages (won't ask about install location):
mkdir -p ~/R-packages
export R_LIBS=~/R-packages

# install R packages
Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages

date
---------------------------------
sbatch 1.0_prep_R_env.sh
Submitted batch job 50606562
sacct -j 50606562 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50606562          0:0   00:17:31
50606562.ba+      0:0   00:17:31   1498740K
50606562.ex+      0:0   00:17:31       256K

FEAT: Implement clade-based tree rooting via MRCA
Replaces single-taxon rooting with support for multi-taxa outgroups. Uses ape::getMRCA() to find the common ancestor node index for a comma-separated list of outgroup tips, ensuring accurate clade-based tree rooting.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano empirical_tree_processor.R
---------------------------------
## Capture Command Line Arguments

# Arguments must be passed in the following order:
# [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
# [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
# [3] format_tree_out     (Base directory for output files)
# [4] outgroup            (List of comma separated outgroup taxa for tree rooting)
# [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
# [6] gen_time            (Estimated generation time in years for all species in the tree)
# [7] simphy_seed1        (First random number seed for generate_params.txt)
# [8] simphy_seed2        (Second random number seed for generate_params.txt)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 8) {
  stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
}

# Assign arguments to variables
mod_write_tree2 <- args[1]
treefile <- args[2]
format_tree_out <- args[3]
outgroup <- args[4]
simphy_seed1 <- args[7]
simphy_seed2 <- args[8]

# Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
scale_factor <- as.numeric(args[5])/as.numeric(args[6])

# --- 2. Load Libraries and Setup ---

library(ape)
library(ggplot2)
library(geiger)
library(ggtree)

## Force a headless friendly bitmap device for the session
options(bitmapType = "cairo")

# Adjust path to modified.write.tree2.R
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# --- 3. Process and Save Empirical Tree ---

# Empirical tree:
tree <- read.tree(treefile)

# 1. Convert the comma-separated string into a vector of tip labels
outgroup_taxa <- unlist(strsplit(outgroup, ","))

# 2. Check that the taxa are actually in the tree
if (!all(outgroup_taxa %in% tree$tip.label)) {
    # Check if any taxa in the list are missing from the tree
    missing_taxa <- setdiff(outgroup_taxa, tree$tip.label)
    stop(paste("Error: The following outgroup taxa were not found in the tree:", paste(missing_taxa, collapse=", ")), call.=FALSE)
}

# 3. Find the Most Recent Common Ancestor (MRCA) node index
# getMRCA returns the node index that is ancestral to all tips in the vector
outgroup_node <- getMRCA(phy = tree, tip = outgroup_taxa)

# 4. Root the tree using the identified MRCA node index
# Use the node= argument instead of outgroup=
tree <- root(tree, node = outgroup_node, resolve.root = TRUE)

# Save the check image
p <- ggtree(tree) + theme_tree2() + geom_tiplab()
ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))

# Check the tree rooted correctly. Then transform to ultrametric and rescale

tree_um <- chronos(tree)
class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
tree_um <- rescale(tree_um, model = "depth", scale_factor)

# Save the ultrametric tree check image
q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))

# --- 4. Prepare for SimPhy and Write Outputs ---

taxon_map <- data.frame(number = 1:length(tree_um$tip.label), name = tree_um$tip.label)
write.csv(taxon_map, paste0(format_tree_out, "/taxon_map.csv"), row.names = FALSE)

# Replace labels with numbers, strip node labels, and write out the tree
tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
tree_um$node.label <- NULL
write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)

# Write the seeds for subsequent dataset parameter simulations
write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))

print("Tree processing complete and output files saved.")
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.1_format_tree.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="process_tree"
#SBATCH --time=00:20:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=1              # Number of CPU cores per task
#SBATCH --mem-per-cpu=4G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to treefile generated by 0.1_iqtree_empirical.sh
treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
# List of outgroup taxa for tree rooting (comma separated)
outgroup="Wallabia_bicolor,Potorous_gilbertii,Pseudochirops_corinnae,Gymnobelideus_leadbeateri,Phalanger_gymnotis,Vombatus_ursinus,Phascolarctos_cinereus,Thylacinus_cynocephalus,Sarcophilus_harrisii,Didelphis_virginiana"
# Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
tree_depth=168000000
# Generation time: Estimated generation time (in years) of all species in the tree:
gen_time=4.5
# Random number seeds for generate_params.txt
simphy_seed1=12345
simphy_seed2=67890

# create output subdirectory:
format_tree_out=$Output/1.1_formatted_empirical_tree
mkdir -p $format_tree_out
cd $format_tree_out

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

## Process Empirical tree
Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $outgroup $tree_depth $gen_time $simphy_seed1 $simphy_seed2

date
---------------------------------
sbatch 1.1_format_tree.sh
Submitted batch job 50824267
sacct -j 50824267 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824267          0:0   00:00:35
50824267.ba+      0:0   00:00:35    465276K
50824267.ex+      0:0   00:00:35

2025.12.11

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

Skipping simulation_prep.sh and run_sptree_SimPhy.R because they generate simulated species trees which I don’t need for now. I just need loci simulated from the empirical tree above.

2025.12.18

I streamlined the 2.0_prep_simphy.sh bash script by replacing nested loops and relative pathing with a variable-driven structure that utilizes absolute paths and accepts three specific command-line arguments: the rescaled treefile, the parameter file, and the ape patch script.
I restructured the generate_sim_properties.R script to capture these arguments via commandArgs(). I updated the average branch length (abl) sampling range to a natural log scale of -20 to -18, which aligns with estimated mammalian substitution rates.

FEAT: Refactor SimPhy prep workflow for single-tree execution
Streamlined the `2.0_prep_simphy.sh` bash script and `generate_sim_properties.R` to remove nested loops and relative path dependencies. Key updates include:
- Implemented `commandArgs()` in R to ingest tree, parameter, and patch files directly from Bash variables.
- Updated substitution rate (ABL) sampling range to -20 to -18 (natural log) to match mammalian empirical data.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano generate_sim_properties.R
---------------------------------
# generate_sim_properties.R
# Purpose: Generate stochastic evolutionary parameters for 2000 simulated loci
# based on an empirical species tree and mammalian rate ranges.
# The resulting df.csv will serve as the master "blueprint" for Run_SimPhy.R
# df.csv contains every stochastic variable needed to generate gene trees

# --- 1. Capture Command Line Arguments ---
# Arguments must be passed in the following order:
# [1] treefile         (Path to ultrametric species tree rescaled in generations)
# [2] params           (Path to generate_params.txt containing reproducibility seed and Ne)
# [3] mod_write_tree2  (Path to modified.write.tree2.R - custom ape patch for NEXUS output)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 3) {
  stop("Error: Missing arguments. Expected: [1] treefile [2] params [3] mod_write_tree2", call. = FALSE)
}

# Assign arguments to variables
tree_path       <- args[1]
params_path     <- args[2]
mod_write_tree2 <- args[3]

# --- 2. Load Simulation Libraries ---
library(ape)        # Phylogenetics core
library(geiger)     # Tree transformation
library(MultiRNG)   # Multivariate random number generation
library(EnvStats)   # Statistical distributions
library(extraDistr) # Zero-inflated Poisson (rzip) for paralogs/contaminants

# --- 3. Setup and Data Loading ---

# Load the scaled, ultrametric species tree
sptree <- read.tree(tree_path)
ntaxa <- length(sptree$tip.label)

# Apply ape patch to ensure SimPhy-compatible NEXUS formatting
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# Extract simulation-wide constants from parameter input file (Seed and Population Size)
params_vals <- unlist(strsplit(readLines(params_path), " "))
random_seed <- as.numeric(params_vals[1])
Ne          <- as.numeric(params_vals[2])

# Set global seed for exact replication of the 2000-locus parameter set
set.seed(random_seed)

# --- 4. Parameter Generation ---
## Parameters:              Value/Range:     Scientific Context:
## Ave Branch Length (abl)  -20 to -18       natural log range estimated for mammals
## Loci Count (nloci)       2000             Standard for the PML training/testing datasets.
## Locus Length	            200–2000 bp      Longer loci are identified as a top feature for higher RF similarity.
## Heterotachy (vbl)        0.5–2.5          Introduces substitution rate variance, a key interaction factor in wRF models.
## ILS Level (Ne)           in params file   Scales discordance based on the effective population size of the empirical group.
## random seed              in params file   For reproducibility

nloci <- 2000
df <- data.frame(loci = paste0("loc_", as.character(1:nloci)))

# [ABL] Average Branch Length (Substitution Rate)
# Using natural log range -20 to -18 as identified for mammals
abl <- round(runif(nloci, min = -20, max = -18), 3)
df$abl <- abl

# Write Species Tree to NEXUS with the [&R] Rooted tag required by SimPhy
write("#NEXUS", file="sptree.nex")
write("begin trees;", file="sptree.nex", append=T)
write(paste0("\ttree tree_1 = [&R] ", write.tree(sptree, digits=8, file="")), file="sptree.nex", append=T)
write("end;", file="sptree.nex", append=T)

# [VBL] Variance in branch length: Heterotachy (Rate Variance)
# Models rate variation across the tree
vbl <- round(runif(nloci,min=0.5,max=2.5),3)
df <- cbind(df, vbl)

# [Coding Status] CDS? Determines if locus evolves under NUCLEOTIDE or CODON models
proteinCoding <- sample(c(TRUE,FALSE), nloci, TRUE)
df <- cbind(df, proteinCoding)

# [Model Seed] Unique seed for individual locus evolution models
modelseed <- sample(10000:99999,nloci, replace=F)
df <- cbind(df, modelseed)

# [Locus Length] 200-2000 bp.
# Longer loci strongly correlate with improved RF similarity/utility
loclen <- sample(200:2000,nloci, replace=T)
df <- cbind(df, loclen)

# [LambdaPS] Pagel's Lambda
# Scales proportion of phylogenetic signal on internal branches
lambdaPS <- round(runif(nloci,min=0.75,max=1.0),5)
df <- cbind(df, lambdaPS)

# [ILS] Proportional to Effective Population Size (Ne)
# Dictates the level of Incomplete Lineage Sorting
Ne <- rep(Ne, nloci)
df <- cbind(df, Ne)

# [Seeds] Specific seeds for downstream software execution
df$seed1 <- sample(10000:99999, nloci, replace = FALSE) # SimPhy seed
df$seed2 <- ifelse(df$proteinCoding, 54321, 12345)      # INDELible seed

# --- 5. Missing Data Simulation ---
# Simulates stochastic and systematic data loss

# Entirely missing taxa (up to 50% of total taxa)
ntaxa_missing <- sample(0:round(ntaxa/2), nloci, replace = TRUE)
taxa_missing <- list()
remaining_taxa <- list()

for (f in ntaxa_missing){
  txm <- sample(c(1:ntaxa), f, replace = FALSE)
	taxa_missing <- c(taxa_missing, list(txm))
	remaining_taxa <- c(remaining_taxa, list(setdiff(c(1:ntaxa), txm)))
}
df$remaining_taxa <- remaining_taxa
df$taxa_missing <- taxa_missing

# Partially missing segments within remaining taxa
taxa_missing_segments <- lapply(remaining_taxa, function(x) sample(x, round(length(x)/2)))
df$taxa_missing_segments <- taxa_missing_segments
df$missing_segments_prop <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0.2, 0.6), 3))
df$missing_segments_bias <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0, 1), 2))

# --- 6. Paralogy and Contamination ---
# Simulates non-orthologous signal that can confound species tree error[cite: 35, 36, 556].

# Deep paralogs via Zero-Inflated Poisson distribution
nremaining_taxa <- lapply(remaining_taxa, length)
df$paralog_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$paralog_branch_mod <- round(runif(nloci, 1.0, 10.0), 2)
df$paralog_taxa <- apply(df, 1, function(x) sample(x$remaining_taxa, x$paralog_cont))

# Contaminant groups
df$cont_pair_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$cont_pairs <- apply(df, 1, function(x) sample(x$remaining_taxa, x$cont_pair_cont * 2))

# --- 7. Save Blueprint ---
# Convert lists to character strings to prevent CSV formatting errors
df <- as.data.frame(df)
df$remaining_taxa <- gsub("\n", " ", as.character(df$remaining_taxa))
df_out <- apply(df, 2, as.character)

write.csv(df_out, "df.csv", row.names = FALSE)
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.0_prep_simphy.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="prep_simphy"
#SBATCH --time=00:20:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu
#SBATCH -c 1
#SBATCH --mem-per-cpu=2G

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to reformatted ultrametric treefile and parameter file generated by 1.1_format_tree.sh
treefile=$Output/1.1_formatted_empirical_tree/s_tree.trees
params=$Output/1.1_formatted_empirical_tree/generate_params.txt

# create output subdirectory:
out_dir=$Output/2.0_simphy_prep
mkdir -p $out_dir
cd $out_dir

# --- Module Management ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

# --- Critical Environment Fixes ---
# 1. Fix for the C++ library (GLIBCXX) errors
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

# 2. Point R to your custom package library
export R_LIBS=~/R-packages

# --- generate_sim_properties.R ---

Rscript $Scripts/2_simulation_scripts/generate_sim_properties.R $treefile $params $mod_write_tree2

date
---------------------------------
sbatch 2.0_prep_simphy.sh
Submitted batch job 50824315
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824315          0:0   00:00:17
50824315.ba+      0:0   00:00:17    290704K
50824315.ex+      0:0   00:00:17

2025.12.19

Install SimPhy:
Installation instructions for SimPhy can be found at https://github.com/adamallo/SimPhy/wiki/Manual

# 1. Create the directory:
mkdir -p /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy
cd /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy

# 2. Download the 64-bit Linux binary directly from the official release
wget https://github.com/adamallo/SimPhy/releases/download/v1.0.2/SimPhy_1.0.2.tar.gz

# 3. Extract the package
tar -xzvf SimPhy_1.0.2.tar.gz

# 4.
cd SimPhy_1.0.2/bin/

# 5. Make the 64-bit Linux binary executable
chmod +x simphy_lnx64

# 2. Test it
./simphy_lnx64 -h

Path:
/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

2025.12.23

Summary of Changes to run_SimPhy.R:
The refactored run_SimPhy.R has been transformed from an execution script into a dedicated Command Factory.
I removed the internal system() calls and the file-cleaning logic to focus strictly on generating simulation parameters.
The script now accepts exactly five command-line arguments, allowing it to be fully controlled by a parent Bash script.
By utilizing a tidyverse approach with mutate and paste0, it maps the variables from the parameter data frame (df.csv) directly into raw SimPhy shell commands.
This output is written to a plain-text file, which serves as a "blueprint" for parallel execution, increasing the flexibility and scalability of the pipeline.

nano run_SimPhy.R
---------------------------------
# ==============================================================================
# Script: Run_SimPhy.R
# Purpose: Generate a list of SimPhy commands based on generated parameters.
# ==============================================================================

library(tidyverse)

args <- commandArgs(trailingOnly = TRUE)

# Check for the 5 arguments passed from the shell script
if (length(args) < 5) {
  stop("Usage: Rscript Run_SimPhy.R <sptree_path> <df_path> <output_list> <loci_dir> <simphy_path>", call. = FALSE)
}

species_tree_path <- args[1]
df_path           <- args[2]
output_list_path  <- args[3]
loci_dir          <- args[4]
simphy            <- args[5]


# Load the parameter blueprint generated in the previous step
df <- read.csv(df_path)
nloci <- nrow(df)

# Generate the command strings
# Using mutate and paste0 for a clean 'tidyverse' approach
df_cmds <- df %>%
  mutate(command = paste0(
    simphy,
    " -rl f:1",                          # Replicates per locus
    " -sr ", species_tree_path,          # Input species tree
    " -sp f:", Ne,                       # Population size for ILS
    " -su ln:", abl, ",0.1",             # Subst. rate (ln scale)
    " -hs ln:", vbl, ",1",               # Heterotachy (vbl)
    " -cs ", seed1,                      # Random seed
    " -o ", loci_dir, loci               # Locus-specific output folder
  ))

# Write the command list to the text file
writeLines(df_cmds$command, con = output_list_path)

cat(paste("Successfully generated  ", nloci, "SimPhy commands in:", output_list_path, "\n"))
---------------------------------

Summary of Changes to 2.1_simphy_commands.sh:
The 2.1_simphy_commands.sh script was redesigned to serve as the Bridge between parameter generation and execution.
I cleaned up the environment setup by adding cluster-specific fixes, such as the GLIBCXX library path export and the R_LIBS pointer.
I replaced the complex "grep-and-split" logic used in older versions with a clean hand-off to the R generator.
The script now handles directory infrastructure (creating the 2.1 and 2.2 output folders) and passes absolute paths to the R script to avoid any ambiguity.
This separation ensures that the 2,000 commands are validated and stored safely before a single simulation core is even engaged.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.1_simphy_commands.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_cmd
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH -p uri-cpu
#SBATCH --time=0:20:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory
Output="$Project/output/mammals"
# Path to SimPhy executable:
simphy_exe=/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

# Input files (Generated by 2.0_prep_simphy.sh)
tree_file=$Output/2.0_simphy_prep/sptree.nex
df_file=$Output/2.0_simphy_prep/df.csv

# create output subdirectory:
out_dir=$Output/2.1_commands_simphy
mkdir -p $out_dir
cd $out_dir

# Output files (To be generated)
cmd_list=$out_dir/simphy_command_list.txt
loci_out_dir=$Output/2.2_simulated_loci/ ## note: terminal "/" is necessary! ##

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
# Fix for the C++ library (GLIBCXX) errors:
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
# Point R to your custom package library
export R_LIBS=~/R-packages

# --- run_SimPhy.R ---

Rscript "$Scripts/2_simulation_scripts/run_SimPhy.R" \
    "$tree_file" \
    "$df_file" \
    "$cmd_list" \
    "$loci_out_dir" \
    "$simphy_exe"

date
---------------------------------
sbatch 2.1_simphy_commands.sh
Submitted batch job 50824346
sacct -j 50824346 -o JobID,ExitCode,Elapsed,MaxRSS
sacct -j 50824346 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824346          0:0   00:00:14
50824346.ba+      0:0   00:00:14    189344K
50824346.ex+      0:0   00:00:14

Git commit message:
feat: refactor SimPhy pipeline for modular parallel execution

- Refactor Run_SimPhy.R to act as a command generator instead of an executor.
- Update Run_SimPhy.R to accept 5 CLI arguments for better path management.
- Standardize 2.1_simphy_commands.sh to utilize absolute paths and HPC environment fixes.
- Remove legacy grep/split logic in favor of a unified command list for GNU Parallel.
- Align Bash-to-R variable hand-off for Ne, seed, and branch length parameters.

Summary of Changes to 2.2_run_simulations.sh:
The 2.2_run_simulations.sh script has been transformed from an execution script into a self-correcting pipeline.
Key changes include the integration of GNU Parallel with a --joblog and --progress monitor to maximize CPU efficiency.
I also implemented a Validation and Auto-Recovery Loop that audits each of the 2,000 expected outputs to ensure files are present and non-zero in size; any failures are automatically extracted from the master command list and re-executed.
Finally, I replaced the find harvesting method with a Sequential Harvesting Loop.
This ensures that the final gene_trees.tre file is perfectly ordered from Locus 1 to 2000, creating a reliable 1:1 mapping between the tree data and the simulation metadata (seeds and Ne)


cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.2_run_simulations.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_run
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # Number of simulations to run in parallel
#SBATCH --mem=8G                 # Adjust based on node availability
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512       # Required for foss/2024a on this cluster
#SBATCH --time=00:30:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to project output directory
Output="$Project/output/mammals"

# Input file (generated by 2.1_simphy_commands.sh):
cmd_list="$Output/2.1_commands_simphy/simphy_command_list.txt"

# Output subdirectory for the simulations:
out_dir="$Output/2.2_simulated_loci"
mkdir -p "$out_dir"

# Output: final combined gene tree file
master_trees="$out_dir/gene_trees.tre"
# Initialize/clear the gene tree log file
> "$master_trees"

# Log: Tracks which simulations succeeded
job_log="$out_dir/2.2_parallel_joblog.log"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load parallel/20240822

# --- 1. Initial Parallel Execution ---
echo "Starting initial SimPhy simulations..."

parallel --jobs $SLURM_CPUS_PER_TASK --joblog "$job_log" --progress < "$cmd_list"
# --jobs $SLURM_CPUS_PER_TASK: Uses the cores requested above
# --joblog: Records completion; if job fails, re-running will skip finished tasks
# --progress: Prints a progress bar and ETA to the slurm log

# --- 2. Validation & Auto-Recovery Loop ---
echo "Verifying simulation integrity..."

# We will loop until all files exist, or set a max number of retry attempts
MAX_RETRIES=2
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    FAILED_LOCI=()

    # Check each of the 2000 expected tree files
    for i in {1..2000}; do
        tree_file="$out_dir/loc_$i/1/g_trees1.trees"

        # If file doesn't exist (-f) OR is empty (! -s)
        if [[ ! -f "$tree_file" || ! -s "$tree_file" ]]; then
            FAILED_LOCI+=($i)
        fi
    done

    # If no failures, break the loop
    if [ ${#FAILED_LOCI[@]} -eq 0 ]; then
        echo "All 2,000 simulations verified successfully."
        break
    else
        RETRY_COUNT=$((RETRY_COUNT + 1))
        echo "WARNING: ${#FAILED_LOCI[@]} simulations failed. Starting Retry #$RETRY_COUNT..."

        # Log the failures
        echo "$(date): Attempt $RETRY_COUNT - Failed Loci: ${FAILED_LOCI[*]}" >> "$out_dir/retry_log.txt"

        # Re-run only the failed commands
        for locus_id in "${FAILED_LOCI[@]}"; do
            # Extract the specific line from the original command list
            # Since loci are loc_1 to loc_2000, we pull line $locus_id
            cmd=$(sed -n "${locus_id}p" "$cmd_list")

            echo "Re-running Locus $locus_id..."
            eval "$cmd"
        done
    fi
done

echo "Final check after retries..."
if [ ${#FAILED_LOCI[@]} -gt 0 ]; then
    echo "CRITICAL: ${#FAILED_LOCI[@]} loci still failing after $MAX_RETRIES retries. Check retry_log.txt"
    exit 1
fi

# --- 3. Sequential Harvesting ---
echo "All simulations verified. Starting sequential harvest into $master_trees..."

# Initialize/clear the master file
> "$master_trees"

# Loop from 1 to 2000 to ensure perfect numerical order
# Line 1 in the tree file will correspond to Locus 1 (Row 1 in df.csv)
for i in {1..2000}
do
    # Define the path to the replicate 1 tree file
    tree_file="$out_dir/loc_$i/1/g_trees1.trees"

    # Append the tree to the master file
    cat "$tree_file" >> "$master_trees"
done

echo "Harvesting complete at $(date)"

# --- 4. Final Verification ---
echo "Performing final audit of $master_trees..."

# Count the number of Newick trees (lines starting with '(')
actual_count=$(grep -c "(" "$master_trees")
expected_count=2000

if [ "$actual_count" -eq "$expected_count" ]; then
    echo "-------------------------------------------------------"
    echo " SUCCESS: 2.2_run_simulations.sh finished successfully."
    echo " Total Trees: $actual_count"
    echo " Order: Sequential (Locus 1 to 2000)"
    echo " Output file: $master_trees"
    echo "-------------------------------------------------------"
else
    echo "-------------------------------------------------------"
    echo " ERROR: Final tree count ($actual_count) does not match"
    echo " expected count ($expected_count)."
    echo " Please check $out_dir/retry_log.txt for clues."
    echo "-------------------------------------------------------"
    exit 1
fi

date
---------------------------------
sbatch 2.2_run_simulations.sh
Submitted batch job 50824418
sacct -j 50824418 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824418          0:0   00:00:29
50824418.ba+      0:0   00:00:29    189884K
50824418.ex+      0:0   00:00:29

Replicate 1 of 1: Simulating 1 gene trees from 1 locus trees... Done
All 2,000 simulations verified successfully.
Final check after retries...
All simulations verified. Starting sequential harvest into /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre...
Harvesting complete at Mon Dec 29 03:39:19 AM UTC 2025
Performing final audit of /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre...
-------------------------------------------------------
 SUCCESS: 2.2_run_simulations.sh finished successfully.
 Total Trees: 2000
 Order: Sequential (Locus 1 to 2000)
 Output file: /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre
-------------------------------------------------------
Mon Dec 29 03:39:19 AM UTC 2025

Git commit message:
feat: implement auto-recovery and sequential harvesting in 2.2_run_simulations.sh

- Add --constraint=avx512 and specific parallel/20240822 module for cluster compatibility.
- Implement a while-loop for auto-recovery that detects and re-runs failed/empty simulations.
- Replace wildcard/find harvesting with a sequential for-loop to preserve locus order (1-2000).
- Add a final grep-based audit to verify the master tree file contains exactly 2000 Newick strings.
- Integrate --joblog and --progress flags for better job monitoring and crash resiliency.

Workflow Check

2.2 creates the trees.
2.3 (Bash) sets up the environment and runs Parallel.
2.3 (Prep R) creates 2,000 control files.
2.3 (Parallel) runs INDELible and renames outputs to locus_i.phy.
2.3 (Post R) reads those .phy files, adds contamination/gaps, and saves as .fas.

Summary of Changes to INDELible scripts:
The simulation pipeline was overhauled to transition from a serialized, directory-dependent structure to a parallelized Control-File Architecture.

2.3_INDELible.sh: I implemented a "Thread-Safe" execution model using GNU Parallel.
By creating unique temporary working directories (tmp_$i) for each of the 2,000 tasks, I bypassed INDELible’s limitation of requiring a fixed filename (control.txt).
This allows 32 simulations to run concurrently without file-access collisions, reducing runtime.

prep_INDELible.R: The script was refactored to be fully modular and portable.
It now accepts dynamic paths for helper scripts and output directories as arguments.
The core logic was modified to output 2,000 individual control files, and the Nucleotide Substitution Model logic was restored to ensure that the mathematical parameters (GTR, HKY, etc.) are correctly mapped to INDELible’s internal commands.

post_INDELible.R: This script was synchronized with the new naming convention (locus_f.phy) and updated to handle the massive output in a single pass.
I updated the column-removal logic and ensured the final output is saved in a standardized FASTA format within a dedicated alignments_final directory.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano prep_INDELible.R
---------------------------------
library(ape)
library(geiger)
library(extraDistr)
library(MultiRNG)
library(EnvStats)
library(castor)
library(phangorn)
library(tidyverse)

# --- 1. Capture Arguments ---
args = commandArgs(trailingOnly=TRUE)
gene_trees_path <- args[1]
df_path         <- args[2]
out_dir         <- args[3]
script_dir      <- args[4] # This is the $Scripts/2_simulation_scripts path

# --- 2. Dynamic Sourcing ---
# Use file.path for cross-platform compatibility
source(file.path(script_dir, "modified.write.tree2.R"))
source(file.path(script_dir, "modify_gene_tree.R"))

# Apply the custom tree-writing function to the ape namespace
assignInNamespace(".write.tree2", .write.tree2, "ape")
options(scipen = 999)

# --- 3. Data Loading ---
gene_trees <- read.tree(gene_trees_path)
df <- read.csv(df_path)
nloci <- length(df[,1])

# --- 4. Main Processing Loop ---
# We loop through each locus and create a UNIQUE control file for it
df2_list <- list()

for (f in 1:nloci) {
  # Define the specific control file for this locus
  locus_control <- file.path(out_dir, paste0("control_", f, ".txt"))

  # Set seed for model parameters
  set.seed(df$modelseed[f])

  # --- Model Block Initialization ---
  if (df$proteinCoding[f] == "TRUE") {
    write("[TYPE] CODON 1", file=locus_control)
    write("[SETTINGS]", file=locus_control, append=T)
    write(paste("\t[randomseed]", df$seed2[f]), file=locus_control, append=T)
  } else {
    write("[TYPE] NUCLEOTIDE 1", file=locus_control)
    write("[SETTINGS]", file=locus_control, append=T)
    write(paste("\t[randomseed]", df$seed2[f]), file=locus_control, append=T)
  }

  # --- Tree Modification Logic (Paralogs/Shifts) ---
  # (Keeping your original logic for modifying the tree structure)
  new_tree <- modify_tree(gene_trees[[f]], df$lambdaPS[f], df$paralog_taxa[f], df$paralog_branch_mod[f])
  new_tree2 <- new_tree
  new_tree2$node.label <- rep("", new_tree2$Nnode)

  # Model shift logic...
  edges0 <- new_tree$edge.length
  names(edges0) <- 1:length(edges0)
  edges1 <- sort(edges0, decreasing = T)
  edges2 <- edges1[which(cumsum(edges1)<sum(edges1)/4)]
  edges3 <- as.numeric(names(edges2))
  nEdges <- rtpois(1, 0.5, -1,length(edges3))
  edges4 <- sample(edges3,nEdges)

  modelnames <- c()
  traversal <- get_tree_traversal_root_to_tips(new_tree2, T)

  for (x in traversal$queue){
    if (Ancestors(new_tree2,x,'parent') == 0) {
      current_model <- paste0("#mRoot")
      modelnames <- c(modelnames, current_model)
      new_tree2$node.label[x-length(new_tree2$tip.label)] <- current_model
    } else {
      current_branch <- which(new_tree2$edge[,2]==x)
      parent_model <- new_tree2$node.label[Ancestors(new_tree2,x,'parent')-length(new_tree2$tip.label)]
      if (current_branch %in% edges4) {
        current_model <- paste0("#m", x)
        modelnames <- c(modelnames, current_model)
      } else {
        if (current_model != parent_model) { current_model <- parent_model }
      }
      if (x <= length(new_tree2$tip.label)) {
        new_tree2$tip.label[x] <- paste0(new_tree2$tip.label[x], current_model)
      } else {
        new_tree2$node.label[x-length(new_tree2$tip.label)] <- current_model
      }
    }
  }

  # --- Write MODEL definitions to the file ---
  clean_modelnames <- gsub('#','', modelnames)

  if (df$proteinCoding[f] == "TRUE") {
    # Codon Model Parameters...
    pInv <- round(runif(1,0,0.25),3); pNeutral <- round(runif(1,0,1-pInv),3)
    for (m in clean_modelnames) {
      basefreqs <- draw.dirichlet(1,61,rep(10,61),1)[1,]
      basefreqs <- c(basefreqs[1:10], 0, 0, basefreqs[11:12], 0, basefreqs[13:61])
      kappa <- round(rlnormTrunc(1,log(4), log(2.5),max=14),3)
      omegaSelect <- round(runif(1,0,3),3)

      write(paste("[MODEL]", m), file=locus_control, append=T)
      write(paste("\t[statefreq]", paste(basefreqs, collapse=" ")), file=locus_control, append=T)
      write(paste("\t[submodel]", paste(kappa, pInv, pNeutral, 0, 1, omegaSelect, collapse=" ")), file=locus_control, append=T)
      write(paste("\t[indelmodel] POW", round(runif(1,1.5,2),3), "10"), file=locus_control, append=T)
      write(paste("\t[indelrate]", round(runif(1,0.001,0.002),5)), file=locus_control, append=T)
    }
    } else {
      # Nucleotide Model Parameters...
      pInv <- round(runif(1,0,0.25),5)
      ngamcat <- sample(c(0,1),1)
      alpha <- if(ngamcat==0) round(rlnormTrunc(1,log(0.3), log(2.5),max=1.4),5) else 0

      for (m in clean_modelnames) {
        modelType <- sample(c("GTR", "SYM", "TVM", "TVMef", "TIM", "TIMef", "K81uf", "K81", "TrN", "TrNef", "HKY", "K80", "F81", "JC"),1)
        paramvector <- get_param_vector(modelType)

        # Determine base frequencies for specific models
        basefreqs <- NA
        if (modelType %in% c("GTR", "TVM", "TIM", "K81uf", "TrN", "HKY", "F81")) {
          basefreqs <- draw.dirichlet(1,4,c(10,10,10,10),1)[1,]
        }

        # --- BUILD THE SUBMODEL STRING ---
        # This part is mandatory for INDELible to understand the modelType
        if (modelType %in% c("GTR", "SYM")) {
          modelstring <- paste(modelType, paste(paramvector[1:5], collapse=" "))
        } else if (modelType %in% c("TVM", "TVMef")) {
          modelstring <- paste(modelType, paste(paramvector[2:5], collapse=" "))
        } else if (modelType %in% c("TIM", "TIMef")) {
          modelstring <- paste(modelType, paste(paramvector[1:3], collapse=" "))
        } else if (modelType %in% c("K81uf", "K81")) {
          modelstring <- paste(modelType, paste(paramvector[2:3], collapse=" "))
        } else if (modelType %in% c("TrN", "TrNef")) {
          modelstring <- paste(modelType, paste(paramvector[c(1,6)], collapse=" "))
        } else if (modelType %in% c("HKY", "K80")) {
          modelstring <- paste(modelType, paramvector[1])
        } else {
          modelstring <- modelType # Covers JC and F81
        }

        # --- WRITE TO CONTROL FILE ---
        write(paste("[MODEL]", m), file=locus_control, append=T)
        write(paste("\t[submodel]", modelstring), file=locus_control, append=T)

        if (!all(is.na(basefreqs))) {
          write(paste("\t[statefreq]", paste(basefreqs, collapse=" ")), file=locus_control, append=T)
        }

        write(paste("\t[rates]", pInv, alpha, ngamcat), file=locus_control, append=T)
        write(paste("\t[indelmodel] POW", paramvector[7], "10"), file=locus_control, append=T)
        write(paste("\t[indelrate]", paramvector[8]), file=locus_control, append=T)
      }
    }

  # --- Write TREE, BRANCHES, and PARTITIONS to the file ---
  write(paste0("[TREE] t1 ", write.tree(new_tree, file="")), file=locus_control, append=T)

  new_tree2$edge.length <- NULL
  write(paste0("[BRANCHES] b1 ", write.tree(new_tree2, file="")), file=locus_control, append=T)

  seq_len <- if(df$proteinCoding[f]=="TRUE") round(df$loclen[f]/3) else df$loclen[f]
  write(paste0("[PARTITIONS] p1 [t1 b1 ", seq_len, "]"), file=locus_control, append=T)

  # --- Final EVOLVE block ---
  write("[EVOLVE] p1 1 output", file=locus_control, append = T)
}

# (Optional: df2.csv logging logic remains here) # logging simulated model params: the original script, was tracking things like modelkappasd and modelratesd.
# Since we are now using a loop, keeping that log would require initialize a data frame before the loop and fill it as you go.
# Not sure yet if we strictly need those specific standard deviation metrics for downstream analysis, so leaving it out for now to keep the script faster.
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano post_INDELible.R
---------------------------------
library(ape)

# --- 1. Capture Arguments ---
args = commandArgs(trailingOnly=TRUE)
alignment_folder_path <- args[1] # This is $indel_dir from the .sh
df_path <- args[2]
df <- read.csv(df_path)

nloci <- length(df[,1])

# Create output directory for modified fasta files
# We'll use file.path for safety
final_aln_dir <- file.path(alignment_folder_path, "alignments_final")
if (!dir.exists(final_aln_dir)) dir.create(final_aln_dir)

# --- 2. Modification Loop ---
for (f in 1:nloci){
    # Identify the simulated file from INDELible
    # Matches the 'mv' command in our bash script: locus_i.phy
    seqpath <- file.path(alignment_folder_path, paste0("locus_", f, ".phy"))

    if (!file.exists(seqpath)) {
        message(paste("Warning: File missing for Locus", f))
        next
    }

    # Read DNA (Sequential Phylip format)
    locus <- read.dna(seqpath, format="sequential", as.character = T)

    # Filter for remaining taxa
    remaining_taxa <- as.character(eval(parse(text=df$remaining_taxa[f])))
    locus <- locus[rownames(locus) %in% remaining_taxa,]

    # --- Contamination Processing ---
    # Swaps sequence data between pairs to simulate lab contamination
    contaminant_tips <- eval(parse(text=df$cont_pairs[f]))
    lencon <- length(contaminant_tips)
    if (lencon > 0) {
        for (t in seq(2, lencon, 2)) {
            # Taxon t gets the sequence of Taxon t-1
            locus[which(rownames(locus) == contaminant_tips[t]), ] <-
                locus[which(rownames(locus) == contaminant_tips[t-1]), ]
        }
    }

    # --- Missing Data Simulation ---
    # Adds gaps (-) to the 5' or 3' ends based on prop and bias in df.csv
    loclen <- ncol(locus)
    taxa_vector <- as.character(eval(parse(text=df$taxa_missing_segments[f])))
    missing_segments_prop_vector <- eval(parse(text=df$missing_segments_prop[f]))
    missing_segments_bias_vector <- eval(parse(text=df$missing_segments_bias[f]))

    if (length(taxa_vector) > 0) {
        for (t in 1:length(taxa_vector)) {
            taxon_name <- taxa_vector[t]
            taxon_index <- which(rownames(locus) == taxon_name)

            if (length(taxon_index) > 0) {
                gapLen <- loclen * missing_segments_prop_vector[t]
                gapLen5 <- round(gapLen * missing_segments_bias_vector[t])
                gapLen3 <- round(gapLen - gapLen5)

                if (gapLen5 > 0) locus[taxon_index, 1:gapLen5] <- "-"
                if (gapLen3 > 0) locus[taxon_index, (loclen - gapLen3 + 1):loclen] <- "-"
            }
        }
    }

    # --- Cleanup: Remove empty columns ---
    # If all taxa have a gap at a site, delete the site
    badpos <- which(apply(locus, 2, function(col) all(col == "-")))
    if (length(badpos) > 0) {
        locus <- locus[, -badpos]
    }

    # --- Write Result ---
    # Save as FASTA in the new alignments_final folder
    locus_bin <- as.DNAbin(locus)
    write.FASTA(locus_bin, file.path(final_aln_dir, paste0("loc_", f, ".fas")))
}

message("Post-processing complete. Modified alignments are in: ", final_aln_dir)
---------------------------------

Install INDELibleV1.03:

cd /project/pi_rsschwartz_uri_edu/Biancani/Software/
git clone https://github.com/kloetzl/indelible.git

/indelible.git
Cloning into 'indelible'...
remote: Enumerating objects: 91, done.
remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91 (from 1)
Receiving objects: 100% (91/91), 1.19 MiB | 14.72 MiB/s, done.
Resolving deltas: 100% (27/27), done.

cd /project/pi_rsschwartz_uri_edu/Biancani/Software/indelible/src
g++ -o indelible -O3 -m64 -std=gnu++98 -include unistd.h indelible.cpp -lm

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.3_INDELible.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=indelible_para
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # Number of INDELible instances to run at once
#SBATCH --mem=4G
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512
#SBATCH --time=00:20:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"
# Path to INDELible executable
indel_exe="/project/pi_rsschwartz_uri_edu/Biancani/Software/indelible/src/indelible"

# Input from 2.0 and 2.2
tree_file="$Output/2.2_simulated_loci/gene_trees.tre"
df_file="$Output/2.0_simphy_prep/df.csv"

# Output subdirectory:
indel_dir="$Output/2.3_indelible"
mkdir -p "$indel_dir"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
module load parallel/20240822
# Fix for the C++ library (GLIBCXX) errors:
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
# Point R to your custom package library
export R_LIBS=~/R-packages

# --- 1. Prep Control Files ---
# Ensure your R script creates 'control_1.txt' through 'control_2000.txt' in $indel_dir
echo "Generating 2,000 INDELible control files..."
# Passing: [1]tree_file [2]df_file [3]output_dir [4]sim_script_dir
Rscript "$Scripts/2_simulation_scripts/prep_INDELible.R" \
    "$tree_file" \
    "$df_file" \
    "$indel_dir" \
    "$Scripts/2_simulation_scripts"

# --- 2. Parallel Execution ---
echo "Running INDELible in parallel across $SLURM_CPUS_PER_TASK cores..."
cd "$indel_dir"

# Generate a list of tasks for parallel
# We create a unique temporary folder for each locus to prevent control.txt collisions
for i in {1..2000}; do
    echo "mkdir -p tmp_$i && cp control_$i.txt tmp_$i/control.txt && cd tmp_$i && $indel_exe > /dev/null && mv output_TRUE.phy ../locus_$i.phy && cd .. && rm -rf tmp_$i"
done > indelible_tasks.txt

# Run the tasks
parallel --jobs $SLURM_CPUS_PER_TASK < indelible_tasks.txt

# --- 3. Post-Processing ---
echo "Running post-simulation sequence modification..."
Rscript "$Scripts/2_simulation_scripts/post_INDELible.R" "$indel_dir" "$df_file"

date
---------------------------------
sbatch 2.3_INDELible.sh
Submitted batch job 50824809
sacct -j 50824809 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824809          0:0   00:01:58
50824809.ba+      0:0   00:01:58    412352K
50824809.ex+      0:0   00:01:58

Running INDELible in parallel across 32 cores...
Running post-simulation sequence modification...
Post-processing complete. Modified alignments are in: /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final
Mon Dec 29 06:14:50 AM UTC 2025

Checking...
# Count raw Phylip files from INDELible
ls /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/locus_*.phy | wc -l
2000
# Count final FASTA files from Post-Processing
ls /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/*.fas | wc -l
2000
# visually inspect a random file:
head -n 20 /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/loc_500.fas
(looks good)
# Sometimes simulations fail silently for specific loci. This command searches for files that are too small (e.g., under 100 bytes) which might indicate a failed simulation:
find /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/ -name "*.fas" -size -100c
#If this returns nothing, all alignments have substantial data.

INDELible summary:
Paralogs: Subtrees were swapped as per df.csv.
Contamination: Taxon sequences were duplicated/overwritten to simulate lab error.
Trimming: Sequence ends were masked with "-" to simulate degraded or poorly sequenced DNA.

Git Commit Message:
feat: parallelize sequence simulation and modernize INDELible workflow

- Refactor 2.3_INDELible.sh to use GNU Parallel with thread-safe temporary directories.
- Update prep_INDELible.R to generate 2,000 individual control files and support dynamic script sourcing.
- Restore Nucleotide Substitution Model switch-case logic to ensure correct INDELible [MODEL] definitions.
- Update post_INDELible.R to handle the new locus naming convention and output processed FASTA alignments.
- Optimize I/O by implementing a centralized alignments_final directory for downstream analysis.

2025.12.29

Step 2.4: Taxon Name Restoration

The final step in the Phase 2 pipeline, Step 2.4, serves as the bridge between the simplified numerical space required by simulation engines and the human-readable requirements of downstream phylogenomic analysis.
Because SimPhy and INDELible are optimized to process taxa as integer-based indices (1, 2, 3, etc.), the biological identity of the sequences is temporarily obscured during the simulation.
Step 2.4 uses the taxon_map.csv—originally captured during the tree-formatting stage (Step 1.1)—to systematically map these integers back to their original empirical species names.
By iterating through the 2,000 final alignments and performing a 1:1 string replacement of the FASTA headers, this script ensures that the "ground truth" labels are restored with integrity, producing a final dataset that is ready for phylogenetic inference tools.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano restore_names.R
---------------------------------
library(ape)

# --- 1. Capture Arguments ---
args <- commandArgs(trailingOnly=TRUE)
input_dir  <- args[1]
output_dir <- args[2]
map_path   <- args[3]

# --- 2. Load Taxon Map ---
taxon_map <- read.csv(map_path)
# Ensure columns are treated as strings to avoid matching errors
taxon_map$number <- as.character(taxon_map$number)

# --- 3. Renaming Loop ---
fas_files <- list.files(input_dir, pattern = "\\.fas$", full.names = TRUE)
message(paste("Renaming taxa in", length(fas_files), "alignments..."))

for (f in fas_files) {
  # Read alignment (as character matrix for easy renaming)
  aln <- read.dna(f, format = "fasta", as.character = TRUE)

  # Map numerical rownames back to names using the CSV
  # match() finds the row index in taxon_map where 'number' matches the rowname
  new_names <- taxon_map$name[match(rownames(aln), taxon_map$number)]

  # Apply new names
  rownames(aln) <- new_names

  # Save output
  out_name <- file.path(output_dir, basename(f))
  write.FASTA(as.DNAbin(aln), out_name)
}

message("All names restored successfully.")
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.4_restore_names.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=restore_names
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH -p uri-cpu
#SBATCH --time=00:20:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"

# Input: final alignments with numbered taxa (generated by 2.3_INDELible.sh)
Input_Aln="$Output/2.3_indelible/alignments_final"
# Output path for final alignments with restored names (will be created if needed)
Final_Aln="$Output/2.4_final_named_alignments"
# Taxon map (generated by 1.1_format_tree.sh)
Taxon_Map="$Output/1.1_formatted_empirical_tree/taxon_map.csv"

mkdir -p "$Final_Aln"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Execute Renaming ---
echo "Restoring empirical taxon names using taxon_map.csv..."
Rscript "$Scripts/2_simulation_scripts/restore_names.R" \
    "$Input_Aln" \
    "$Final_Aln" \
    "$Taxon_Map"

echo "Process complete. Final alignments located in: $Final_Aln"
date
---------------------------------
sbatch 2.4_restore_names.sh
Submitted batch job 50825498
sacct -j 50825498 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50825498          0:0   00:00:29
50825498.ba+      0:0   00:00:29    210536K
50825498.ex+      0:0   00:00:29

Git Commit Message:
feat: add Step 2.4 taxon name restoration script

- Adds 2.4_restore_names.sh and restore_names.R to the pipeline.
- Implements 1:1 mapping of numerical labels back to empirical species names.
- Uses taxon_map.csv from Step 1.1 to ensure naming integrity.

Phase 3: Filter_by_Known_Clades
Filter all loci for support for Established Clades

Clone the 01_iqtree directory while preserving specific commit history (via git subtree):
# 1. Navigate to root of current repository:
cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
# 2. Add the external repo as a "Remote"
# This tells your local Git where to look for the source code without actually downloading everything yet.
git remote add filter_repo https://github.com/LMBiancani/FilterByKnownClades.git
git fetch filter_repo
# 3. Pull the directory in as a Subtree
# This command "slices" the 01_iqtree folder out of the external history and grafts it into a new folder in your repo.
# --prefix Scripts/3_filter_by_known_clades: This names the new folder in your current repo
# filter_repo main: This specifies the remote name and the branch you are pulling from.
git subtree add --prefix Scripts/3_filter_by_known_clades filter_repo main


The requires a taxon-to-group correspondence table, groups.csv, is a csv file with the following format:
Group,Taxa
group1,taxonName1
group1,taxonName2
group2,taxonName3
group2,taxonName4

copy that file from my PlacentalPolytomy directory:
cp /project/pi_rsschwartz_uri_edu/Biancani/PlacentalPolytomy/01_FilterByTaxa/groups.csv /scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/

less /scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/groups.csv

Group,Taxa
Afrotheria,Amblysomus_hottentotus_longiceps
Afrotheria,Chrysochloris_asiatica
Afrotheria,Echinops_telfairi
Afrotheria,Elephantulus_edwardii
Afrotheria,Hydrodamalis_gigas
Afrotheria,Loxodonta_africana
Afrotheria,Microgale_talazaci
Afrotheria,Orycteropus_afer
Afrotheria,Procavia_capensis
Afrotheria,Trichechus_manatus_latirostris
Boreoeutheria,Ceratotherium_simum
Boreoeutheria,Condylura_cristata
Boreoeutheria,Galeopterus_variegatus
Boreoeutheria,Hippopotamus_amphibius
Boreoeutheria,Manis_javanica
Boreoeutheria,Odobenus_rosmarus_divergens
Boreoeutheria,Pan_troglodytes
Boreoeutheria,Pteropus_vampyrus
Boreoeutheria,Rattus_norvegicus
Boreoeutheria,Tupaia_tana
Metatheria,Didelphis_virginiana
Metatheria,Gymnobelideus_leadbeateri
Metatheria,Phalanger_gymnotis
Metatheria,Phascolarctos_cinereus
Metatheria,Potorous_gilbertii
Metatheria,Pseudochirops_corinnae
Metatheria,Sarcophilus_harrisii
Metatheria,Thylacinus_cynocephalus
Metatheria,Vombatus_ursinus
Metatheria,Wallabia_bicolor
Xenarthra,Chaetophractus_vellerosus
Xenarthra,Choloepus_didactylus
Xenarthra,Dasypus_novemcinctus
Xenarthra,Mylodon_darwinii
Xenarthra,Myrmecophaga_tridactyla
Xenarthra,Tamandua_tetradactyla
Xenarthra,Tolypeutes_matacus

In Step 3.0, we developed a portable framework to generate topological constraints for the mammal simulation dataset.
We created a generic R engine, generate_constraints.R, which utilizes the ape library to ingest a standardized clade definition file (groups.csv) and a directory of alignments.
For each of the 2,000 loci, the script dynamically identifies which taxa are present and prunes the "established clades" accordingly, ensuring that a constraint is only generated if a clade contains at least two representative taxa.
These pruned clades are then nested into a multifurcating Newick string—representing our "known" mammalian backbone—and saved as individual .newick files.
This step is critical for our experimental validation, as it provides the necessary "Skeleton" trees for the subsequent constrained IQ-TREE inferences, ultimately allowing us to filter loci based on their statistical support for biologically certain nodes.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano generate_constraints.R
---------------------------------
library(ape)

# Capture arguments from shell
args <- commandArgs(trailingOnly = TRUE)
ALN_DIR     <- args[1]
OUTPUT_DIR  <- args[2]
CLADE_FILE  <- args[3]

dir.create(OUTPUT_DIR, recursive = TRUE, showWarnings = FALSE)

# Load Clade Definitions (Group, Taxa)
clade_df <- read.csv(CLADE_FILE, header = TRUE)
all_clades <- split(clade_df$Taxa, clade_df$Group)

# Process all alignments
loci_files <- list.files(ALN_DIR, pattern = "\\.fas$")

for (f in loci_files) {
  locus_path <- file.path(ALN_DIR, f)
  fasta <- read.FASTA(locus_path)
  present_taxa <- names(fasta)

  # Intersect defined clades with what is actually in this locus
  pruned_clades <- lapply(all_clades, function(x) intersect(x, present_taxa))
  valid_clades <- pruned_clades[sapply(pruned_clades, length) > 1]

  if (length(valid_clades) > 0) {
    # Create the constraint string
    clade_strings <- sapply(valid_clades, function(x) paste0("(", paste(x, collapse = ","), ")"))
    final_constraint <- paste0("(", paste(clade_strings, collapse = ","), ");")

    # Save as locus_i_constraint.newick
    out_name <- gsub("\\.fas$", "_constraint.newick", f)
    write(final_constraint, file = file.path(OUTPUT_DIR, out_name))
  }
}
cat(paste("Generated", length(list.files(OUTPUT_DIR)), "constraint files.\n"))
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano 3.0_generate_constraints.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=constraints
#SBATCH --partition=uri-cpu
#SBATCH --time=00:15:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --constraint=avx512

# --- Variables ---
# Paths to project directories:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"

# Input alignment (generated by 2.4_restore_names.sh):
Alignments=$Output/2.4_final_named_alignments

# Input clades:
clades=/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/groups.csv
# a taxon-to-group correspondence CSV table with the following format:
# Group,Taxa
# group1,taxonName1
# group1,taxonName2
# group2,taxonName3
# group2,taxonName4

# Output subdirectory for the constraint tree:
out_dir="$Output/3.0_constraint_trees"
mkdir -p "$out_dir"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Execute ---
Rscript $Scripts/3_filter_by_known_clades/generate_constraints.R "$Alignments" "$out_dir" "$clades"

# --- Audit & Confirmation ---
echo "------------------------------------------------"
echo "Post-Processing Audit:"
expected_count=$(ls -1 "$Alignments"/*.fas | wc -l)
actual_count=$(ls -1 "$out_dir"/*_constraint.newick 2>/dev/null | wc -l)

echo "Expected constraints: $expected_count"
echo "Generated constraints: $actual_count"

if [ "$expected_count" -eq "$actual_count" ]; then
    echo "SUCCESS: All constraint files were generated correctly."
else
    echo "WARNING: Count mismatch! Some loci may not have enough taxa to form a constraint."
fi
echo "------------------------------------------------"
---------------------------------
sbatch 3.0_generate_constraints.sh
Submitted batch job 50827749
sacct -j 50827749 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50827749          0:0   00:00:13
50827749.ba+      0:0   00:00:13    195460K
50827749.ex+      0:0   00:00:13

git commit message:
feat: implement automated topological constraint generation for LRT filtering

- Added generate_constraints.R to dynamically build Newick constraints.
- Integrated groups.csv to define established mammalian clades (Afrotheria, etc.).
- Added 3.0_generate_constraints.sh
- Logic automatically prunes constraints for loci with missing taxa.
- Added post-processing audit to verify 1:1 alignment-to-constraint ratio.

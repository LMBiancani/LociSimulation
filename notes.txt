NOTES - LociSimulation
github: https://github.com/LMBiancani/LociSimulation.git
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"

2025.10.09

ws_allocate -m biancani@uri.edu -r 5 -G pi_rsschwartz_uri_edu LociSimulation 30

Info: creating workspace.
/scratch4/workspace/biancani_uri_edu-LociSimulation
remaining extensions  : 5
remaining time in days: 30

Starting with mammal loci from https://github.com/LMBiancani/PlacentalPolytomy
Original Path to aligned loci:
"/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"

2025.10.09

cd /scratch4/workspace/biancani_uri_edu-LociSimulation

copy mammal loci to scratch space:

transfer.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="tar_transfer"
#SBATCH --time=48:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -p uri-cpu
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=6G

# Define source and destination directories
SOURCE_DIR="/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"
DEST_DIR="/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci"

# Make sure the destination exists
mkdir -p "$DEST_DIR"
echo "Started at $(date)"

# Stream the directory (preserving its name) to scratch
tar -cf - -C "$(dirname "$SOURCE_DIR")" "$(basename "$SOURCE_DIR")" | pv | tar -xf - -C "$DEST_DIR"

echo "Finished at $(date)"
---------------------------------
sbatch transfer.sh
Submitted batch job 45655769

sacct -j 45655769 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
45655769          0:0   00:01:48
45655769.ba+      0:0   00:01:48   2114772K
45655769.ex+      0:0   00:01:48          0

mkdir /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation

github: https://github.com/LMBiancani/LociSimulation.git

2025.10.20

Use git subtree to copy Molly's updated simulation/machine learning scripts (and preserve commit history)
Molly's repo: https://github.com/mollyodonnellan/PML.git

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
git subtree add --prefix=Scripts https://github.com/mollyodonnellan/PML.git main

Mammal data is too large for AMAS so script needs updating to run through loci in batches.

run_amas.py is a custom batch concatenation wrapper around AMAS.
It processes FASTA alignments in chunks of 1000 files at a time (to avoid overloading AMAS input limitations), concatenates them, and appends the results into cumulative files.
Outputs:
concatenated.fasta
partitions.txt

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano run_amas.py
---------------------------------
#!/usr/bin/env python3
"""
run_amas.py
------------
Concatenates FASTA alignments in batches using AMAS (to avoid overloading AMAS input limitations).
Creates a concatenated alignment file and a corresponding partition file:
concatenated.fasta
partitions.txt

Usage:
    python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>
"""

import sys
import glob
import subprocess
import os

# --- Input arguments ---
fasta_folder = sys.argv[1]
total_cores = sys.argv[2]
amas = sys.argv[3]

# --- Collect all fasta files ---
files = glob.glob(os.path.join(fasta_folder, "*.fasta"))
files.sort()  # ensure consistent, reproducible order

batch_size = 1000
batch_outputs = []
batch_parts = []

count = 0
fileList = []
batch_num = 1

# --- Process loci in batches ---
for f in files:
    fileList.append(f)
    count += 1

    if count == batch_size:
        batch_fasta = f"amas_batch_{batch_num}.fasta"
        batch_part = f"partitions_batch_{batch_num}.txt"

        cmd = (
            f"python3 {amas} concat "
            f"-f fasta -d dna --out-format fasta --part-format raxml "
            f"-i {' '.join(fileList)} "
            f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
        )
        print(f"\n=== Running AMAS on batch {batch_num} ({len(fileList)} files) ===")
        subprocess.call(cmd, shell=True)

        batch_outputs.append(batch_fasta)
        batch_parts.append(batch_part)
        fileList = []
        count = 0
        batch_num += 1

# --- Final partial batch (if any) ---
if len(fileList) > 0:
    batch_fasta = f"amas_batch_{batch_num}.fasta"
    batch_part = f"partitions_batch_{batch_num}.txt"

    cmd = (
        f"python3 {amas} concat "
        f"-f fasta -d dna --out-format fasta --part-format raxml "
        f"-i {' '.join(fileList)} "
        f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
    )
    print(f"\n=== Running AMAS on final batch {batch_num} ({len(fileList)} files) ===")
    subprocess.call(cmd, shell=True)

    batch_outputs.append(batch_fasta)
    batch_parts.append(batch_part)

# --- Final concatenation across all batches ---
print("\n=== Performing final concatenation across batches ===")

cmd_final = (
    f"python3 {amas} concat "
    f"-f fasta -d dna --out-format fasta --part-format raxml "
    f"-i {' '.join(batch_outputs)} "
    f"-c {total_cores} -t concatenated.fasta -p partitions.txt"
)
subprocess.call(cmd_final, shell=True)

# --- Cleanup temporary batch files ---
print("\n=== Cleaning up intermediate files ===")
for f in batch_outputs + batch_parts:
    try:
        os.remove(f)
    except OSError:
        print(f"Warning: could not remove {f}")

print("\n=== run_amas.py execution completed. ===")
print("Output files:")
print(" - concatenated.fasta")
print(" - partitions.txt\n")
---------------------------------

0.0_amas_concat.sh runs AMAS on an empirical dataset to concatenate input fasta files and prepare partitions ahead of IQTree run.
Uses a helper Python script (run_amas.py), which wraps around the AMAS.py concat command.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano 0.0_amas_concat.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="AMAS"
#SBATCH --time=2:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory (will be created if necessary)
Output="$Project/output/mammals"
# Path to aligned loci in fasta format:
Data="$Project/mammal_loci/01_SISRS_loci_filtered"
# Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"
# Number of processor cores per node:
Cores=$(echo $SLURM_TASKS_PER_NODE | sed 's/(x.*)//')

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

date
mkdir -p ${Output}/0.0_concatenated
cd ${Output}/0.0_concatenated

#Concatenate input fasta files and prepare partitions ahead of IQTree run
python3 ${Scripts}/0_data_prep/run_amas.py ${Data} ${Cores} ${AMAS}

date
---------------------------------
sbatch 0.0_amas_concat.sh
Submitted batch job 49112018
sacct -j 49112018 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
49112018          0:0   00:01:39
49112018.ba+      0:0   00:01:39   5584204K
49112018.ex+      0:0   00:01:40          0

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

0.1_iqtree_empirical.sh will use iqtree to infer an empirical tree.

nano 0.1_iqtree_empirical.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="IQTREE"
#SBATCH --time=96:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=24
#SBATCH --mem=250G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to output directory
Output="$Project/output/mammals"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"
# Path to output files from 0.0_amas_concat.sh
Input="$Output/0.0_concatenated"
# Number of cpus per task:
Threads=${SLURM_CPUS_PER_TASK}

module purge

date
mkdir -p ${Output}/0.1_empirical_tree
cd ${Output}/0.1_empirical_tree

# --- Check for input files produced by 0.0_amas_concat.sh---

if [[ ! -f "$Input/concatenated.fasta" || ! -f "$Input/partitions.txt" ]]; then
    echo "Error: concatenated.fasta or partitions.txt not found in ${Input}"
    exit 1
fi

# --- Run IQ-TREE ---
# Flags:
#   -nt: number of CPU threads
#   -spp: partition file allowing different evolutionary rates per partition
#   -pre: prefix for output files
#   -m MFP: ModelFinder Plus for best-fit model selection
#   -bb: ultrafast bootstrap replicates
#   -alrt: SH-like approximate likelihood test replicates

${IQTREE} -nt ${Threads} \
    -s $Input/concatenated.fasta \
    -spp $Input/partitions.txt \
    -pre inferenceEmpirical \
    -m MFP -bb 1000 -alrt 1000

date
---------------------------------
sbatch 0.1_iqtree_empirical.sh
Submitted batch job 47065851
sacct -j 47065851 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
47065851          0:0 2-17:22:51
47065851.ba+      0:0 2-17:22:51 149134624K
47065851.ex+      0:0 2-17:22:51       256K

2025.11.21

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano install.packages.R
---------------------------------
# Capture command-line arguments (package names to check and install)
args <- commandArgs(trailingOnly = TRUE)

if (length(args) == 0) {
  stop("Error: No package names provided as arguments. Exiting.", call. = FALSE)
}

# The packages to install are now in the 'args' vector
packages_to_install <- args

# Function to check, install, and load packages
install_and_load <- function(pkg) {
  # Check if the package is installed
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Package", pkg, "not found. Installing..."))
    # Install the package from CRAN
    install.packages(pkg, dependencies = TRUE, repos = "https://cloud.r-project.org")
  } else {
    message(paste("Package", pkg, "is already installed."))
  }

  # Load the package (library function)
  library(pkg, character.only = TRUE)
  message(paste("Package", pkg, "loaded successfully."))
}

# Apply the function to the list of packages
invisible(sapply(packages_to_install, install_and_load))
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano empirical_tree_processor.R
### ---------------------------------
### ## Capture Command Line Arguments
###
### # Arguments must be passed in the following order:
### # [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
### # [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
### # [3] format_tree_out     (Base directory for output files)
### # [4] out_tip             (The tip name to use as the outgroup for rooting)
### # [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
### # [6] gen_time            (Estimated generation time in years for all species in the tree)
### # [7] simphy_seed1        (First random number seed for generate_params.txt)
### # [8] simphy_seed2        (Second random number seed for generate_params.txt)
###
### args <- commandArgs(trailingOnly = TRUE)
###
### # Check if the correct number of arguments was provided
### if (length(args) < 8) {
###   stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
### }
###
### # Assign arguments to variables
### mod_write_tree2 <- args[1]
### treefile <- args[2]
### format_tree_out <- args[3]
### out_tip <- args[4]
### simphy_seed1 <- args[7]
### simphy_seed2 <- args[8]
###
### # Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
### scale_factor <- as.numeric(args[5])/as.numeric(args[6])
###
### # --- 2. Load Libraries and Setup ---
###
### library(ape)
### library(ggplot2)
### library(geiger)
### library(ggtree)
###
### ## Force a headless friendly bitmap device for the session
### options(bitmapType = "cairo")
###
### # Adjust path to modified.write.tree2.R
### source(mod_write_tree2)
### assignInNamespace(".write.tree2", .write.tree2, "ape")
###
### # --- 3. Process and Save Empirical Tree ---
###
### # Empirical tree:
### tree <- read.tree(treefile)
###
### # Root the tree and save the check image
### tree <- root(tree, outgroup = out_tip)
### p <- ggtree(tree) + theme_tree2() + geom_tiplab()
### ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))
###
### # Check the tree rooted correctly. Then transform to ultrametric and rescale
###
### tree_um <- chronos(tree)
### class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
### tree_um <- rescale(tree_um, model = "depth", scale_factor)
###
### # Save the ultrametric tree check image
### q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
### ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))
###
### # --- 4. Prepare for SimPhy and Write Outputs ---
###
### # Replace labels with numbers, strip node labels, and write out the tree
### tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
### tree_um$node.label <- NULL
### write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)
###
### # Write the seeds for subsequent dataset parameter simulations
### write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))
###
### print("Tree processing complete and output files saved.")
### ---------------------------------

## INPUTS for empirical_tree_processor.R

# generation scaling factor: Number of generations in the tree: absolute tree age in years divided by average generation time of species in the tree.
total tree depth (years) / average generation time (years)

# total tree depth: age of most recent common ancestor of all species in the tree.
For this mammal data, this would be the age of Theria or age of the common ancestor of placental mammals (ingroup) and marsupials (outgroup)
total tree depth (years) is estimated to be 168 mya
Citations:
2019 - Upham NS, Esselstyn JA, Jetz W (2019) Inferring the mammal tree: Species-level sets of phylogenies for questions in ecology, evolution, and conservation. PLoS Biol 17(12): e3000494.https://doi.org/10.1371/journal.pbio.3000494
168 Fig 1
2017 - Thomas L. Dunwell, Jordi Paps, Peter W. H. Holland; Novel and divergent genes in the evolution of placental mammals. Proc Biol Sci 1 October 2017; 284 (1864): 20171357. https://doi.org/10.1098/rspb.2017.1357
140-191 "The common ancestor of placentals and marsupials dates to approximately 140 – 191 million years ago (Ma)"

tree_depth=168000000

# average generation time (years)
We used a estimated generation time of 4.5 years for Theria. This value is chosen not as the arithmetic mean or median of all extant species (which is skewed by small-bodied rodents and bats), but as a phylogenetically-informed effective generation time that is consistent with the estimated rate of mutation at the deepest nodes of the mammalian tree. This approach accounts for the allometric relationship between body mass and generation length, a relationship thoroughly characterized across Mammalia [cite Pacifici et al., 2013]. Our chosen rate falls within the range utilized by major phylogenomic studies (e.g., Meredith et al., 2011), which established evolutionary rates for ancestral nodes that are consistent with a moderately sized stem mammal ancestor and a generation time in the 4–5 year range."
Citation:
Pacifici, Michela, et al. "Generation length for mammals." Nature Conservation 5 (2013): 89-94.
Meredith, Robert W., et al. "Impacts of the Cretaceous Terrestrial Revolution and KPg extinction on mammal diversification." science 334.6055 (2011): 521-524.

gen_time=4.5

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano 1.0_format_tree.sh - OLD VERSION
### ---------------------------------
### #!/bin/bash
### #SBATCH --job-name="process_tree"
### #SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
### #SBATCH --nodes=1                      # Number of nodes
### #SBATCH --ntasks=1                     # Total number of tasks (processes)
### #SBATCH --cpus-per-task=2              # Number of CPU cores per task
### #SBATCH --mem-per-cpu=6G               # Memory per cpu
### #SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
### #SBATCH --mail-type=ALL
### #SBATCH -p uri-cpu                     # Partition/queue to submit job to
###
### # --- Variables ---
### # Path to project directory:
### Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
### # Path to scripts directory:
### Scripts="$Project/LociSimulation/Scripts"
### # Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
### mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
### # Path to output directory
### Output="$Project/output/mammals"
### # Path to treefile generated by 0.1_iqtree_empirical.sh
### treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
### # List of necessary R packages (separated by spaces)
### R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger"
### # Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
### tree_depth=168000000
### # Generation time: Estimated generation time (in years) of all species in the tree:
### gen_time=4.5
### # Taxon (tip name) to use as the outgroup for rooting
### out_tip="Didelphis_virginiana"
### # Random number seeds for generate_params.txt
### simphy_seed1=12345
### simphy_seed2=67890
###
### # create output subdirectory:
### format_tree_out=$Output/1.0_formatted_empirical_tree
### mkdir -p $format_tree_out
### cd $format_tree_out
###
### module purge
### module load uri/main
### module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
### module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
### module load R/4.3.2-gfbf-2023a # Loads updated R version
### # This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
### # This must be done AFTER module loading to override potential downgrades.
### export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
### export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
###
### ## Install R packages
###
### # add local space for R packages (won't ask about install location):
### mkdir -p ~/R-packages
### export R_LIBS=~/R-packages
###
### # install R packages
### Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages
###
###
### ## Process Empirical tree
### Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $out_tip $tree_depth $gen_time $simphy_seed1 $simphy_seed2
###
### date
### ---------------------------------
### sbatch 1.0_format_tree.sh

Submitted batch job 50314567 - module Error
Error: failing to load the Rcpp.so library due to a missing GLIBCXX_3.4.32 symbol, was caused by a compiler mismatch on the HPC cluster. The R package Rcpp (required by ape) had been compiled using a newer version of the GNU Compiler Collection (GCC) than the one available to R at runtime (GCCcore/11.2.0). This was resolved by updating the Slurm script's module load strategy: we first load the newest available toolchain (foss/2024a) to provide the necessary, updated C++ shared libraries (libstdc++.so.6), and then load a compatible R module (R/4.3.2-gfbf-2023a). Crucially, we also forced the deletion and reinstallation of all local R packages (rm -rf ~/R-packages/*) to ensure they were compiled cleanly using the newly loaded, compatible foss/2024a toolchain.
FIX: Resolve GLIBCXX compiler mismatch in R environment - Updates 1.0_format_tree.sh to load the latest foss toolchain and forces R package reinstallation to fix dependency loading error.

sbatch 1.0_format_tree.sh
Submitted batch job 50314943 - module error
Error: The R package installation failed for geiger and phytools because two of their indirect dependencies, MASS and clusterGeneration, were not found or did not install correctly during the automated dependency resolution. This kind of failure is common on HPC systems when packages have many dependencies. The fix was to explicitly include the missing top-level dependency packages (MASS, clusterGeneration, and phytools) alongside the target packages (ape, ggplot2, geiger, ggtree) in the $R\_packages variable in the Slurm script, forcing the clean reinstallation of all necessary components.
FIX: Explicitly list R package dependencies - Adds MASS, clusterGeneration, and phytools to R_packages variable in 1.0_format_tree.sh to resolve dependency failures for geiger during installation.

sbatch 1.0_format_tree.sh
Submitted batch job 50315543 - module Error
Error: The R package installation failed because the latest version of the MASS package requires R version ≥4.4.0, which is incompatible with the cluster's available R 4.3.2 module. The previous attempt to pin the version failed because install.packages() ignores the version argument when using the main CRAN mirror.
The definitive fix required opening an interactive session (see below) to install MASS version 7.3-60 (a compatible version) by pointing directly to its source file on the CRAN Archive URL (repos = NULL). This forces the installation of the correct package version, resolving the R version compatibility conflict and allowing dependent packages like geiger to install successfully.

# --- Install MASS separately with a pinned, compatible version for R 4.3.2 ---
# Version 7.3-60 is compatible with R 4.3.x
salloc # open interactive session
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export R_LIBS=~/R-packages
R
# --- Install MASS using the direct archive URL (CRITICAL FIX) ---
# This forces the download of the compatible version (7.3-60) for R 4.3.2.
install.packages("https://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.tar.gz",
                 repos = NULL, # MUST be NULL when installing from a URL
                 type = "source",
                 dependencies = TRUE)

sbatch 1.0_format_tree.sh
Submitted batch job 50315977 - Error
The installation of the R package magick failed because the compiler could not find the external system development files (Magick++.h and the corresponding libraries).
Error: <stdin>:1:10: fatal error: Magick++.h: No such file or directory
Cause: The ImageMagick system library and its development headers are not in your environment's default search paths (neither via a simple module nor the base system).
The R package installation encountered two simultaneous failures:
System Dependency Failure: The R package magick failed to compile because its external system dependency, the ImageMagick development library (Magick++.h), was not found in the environment's default search path.
Recurring Compiler Mismatch: The GLIBCXX_3.4.32 error reappeared for phytools (via igraph) because the conflicting uri/main module was loaded before the necessary foss/2024a toolchain, resulting in a library downgrade.
The fix required a final comprehensive adjustment to the module load order in the Slurm script: explicitly loading the ImageMagick/7.1.1-15-GCCcore-12.3.0 module to provide the missing system headers, and ensuring the foss/2024a toolchain is loaded last to correctly define the newest C++ library environment. This ensures all low-level, system, and R-version-specific dependencies are correctly satisfied.
FIX: Resolve final compilation/dependency errors - Loads ImageMagick module to provide system headers for R 'magick' package, and finalizes module load order (foss/2024a last) to fix recurring GLIBCXX compiler downgrade error

sbatch 1.0_format_tree.sh
Submitted batch job 50316544 - Error
More dependency issues. Try adding igraph to list of R packages

sbatch 1.0_format_tree.sh
Submitted batch job 50316710 - Error
The core problem was a persistent compiler linking error (GLIBCXX_3.4.32 not found), which manifested across multiple R package installations (igraph, phangorn, phytools). Although the correct, newest compiler toolchain (foss/2024a) was loaded, the conflicting uri/main system module or standard library search paths kept defaulting to an older, incompatible library (GCCcore/12.3.0). The definitive fix was to use the path retrieved via module show to manually update the linker path. The Slurm script now explicitly sets LD\_LIBRARY\_PATH to prioritize the directory containing the newest C++ libraries (/modules/uri\_apps/software/GCCcore/13.3.0/lib64), forcing the system to link all new R packages correctly and resolving the chronic loading error.
FIX: Resolve chronic GLIBCXX linking error - Forces the LD_LIBRARY_PATH in 1.0_format_tree.sh to the newest GCCcore path (/modules/uri_apps/software/GCCcore/13.3.0/lib64). This overrides system defaults and fixes the recurring GLIBCXX_3.4.32 linker failure across all compiled R packages.

sbatch 1.0_format_tree.sh
Submitted batch job 50316861 - Error

# --- Install devtools and ggtree (via interactive session) ---
salloc
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages
R
# 1. Install devtools (needed for install_github)
if (!requireNamespace("devtools", quietly = TRUE)) {
    install.packages("devtools", dependencies = TRUE, repos = "https://cloud.r-project.org")
}
# 2. Load devtools
library(devtools)
# 3. Install ggtree from GitHub (this often bypasses strict CRAN/Bioconductor version checks)
# Note: You may also need to install the 'BiocManager' package first if this fails.
install_github("YuLab-SMU/ggtree")

sbatch 1.0_format_tree.sh
Submitted batch job 50317295 - worked!!

sacct -j 50317295 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50317295          0:0   00:00:24
50317295.ba+      0:0   00:00:24    428496K
50317295.ex+      0:0   00:00:24          0

* Check re-rooted and ultrametric trees! - Issue. only rooting with one taxa instead of entire outgroup.
Thoughts:
As of now the tree is being rooted by specifying out_tip (tip name to use as the outgroup for rooting). However, many trees have a clade as an outgroup, not a single taxa. I know ggtree can root using a node instead of a taxon:
rooted_tree <- root(tree, node=109, resolve.root = TRUE, edgelabel=TRUE)
However, I don't know the node number for my outgroup.
The following code would produce a tree where the nodes are labeled with their node numbers, but I'd need to manually look at the tree and see what the outgroup node is labeled.
t <- ggtree(tree, layout="rectangular", size=1, branch.length="none") + geom_text(aes(label=node)) + geom_tiplab(align=TRUE, hjust=-.15)
What I would like to do, is write an R ggtree function or similar that takes a list of all the outgroup taxa and returns the number of the node the represents the common ancestor for those taxa (which would subsequently be used for rooting the tree)
To root a phylogenetic tree using an entire outgroup clade instead of a single taxon, the manual lookup of node numbers in a ggtree plot is unnecessary. The required functionality is available in the ape package, which is already loaded in the script. The function to use is getMRCA() (Most Recent Common Ancestor). This function efficiently takes two arguments: the phylogenetic tree object and a vector containing the names of all the outgroup tip labels. It then automatically returns the specific node index corresponding to the common ancestor of that entire outgroup clade. This node index can then be passed directly to the root() function for accurate clade-based rooting.

To transition from rooting the phylogenetic tree using a single taxon to using an entire outgroup clade, the script was updated to leverage the ape::getMRCA() function. This involved two main changes:
Input Change: The command-line argument $out_tip was replaced with a string of comma-separated outgroup taxa (e.g., Didelphis_virginiana,Monodelphis_domestica).
R Logic: The empirical_tree_processor.R script was modified to:
Parse the input string into an R vector of tip labels using strsplit().
Use getMRCA(tree, tip = outgroup_taxa) to programmatically find the integer node index corresponding to the most recent common ancestor of all specified outgroup tips.
Replace the single-taxon root(..., outgroup=...) call with the root(..., node=outgroup_node) call, ensuring the tree is rooted correctly on the ancestral node of the entire outgroup clade.

Making updates scripts...

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.0_prep_R_env.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="R_pkgs"
#SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=1              # Number of CPU cores per task
#SBATCH --mem-per-cpu=6G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# List of necessary R packages (separated by spaces)
R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger MultiRNG EnvStats extraDistr tidyverse"

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

## Install R packages

# add local space for R packages (won't ask about install location):
mkdir -p ~/R-packages
export R_LIBS=~/R-packages

# install R packages
Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages

date
---------------------------------
sbatch 1.0_prep_R_env.sh
Submitted batch job 50606562
sacct -j 50606562 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50606562          0:0   00:17:31
50606562.ba+      0:0   00:17:31   1498740K
50606562.ex+      0:0   00:17:31       256K

FEAT: Implement clade-based tree rooting via MRCA
Replaces single-taxon rooting with support for multi-taxa outgroups. Uses ape::getMRCA() to find the common ancestor node index for a comma-separated list of outgroup tips, ensuring accurate clade-based tree rooting.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano empirical_tree_processor.R
---------------------------------
## Capture Command Line Arguments

# Arguments must be passed in the following order:
# [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
# [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
# [3] format_tree_out     (Base directory for output files)
# [4] outgroup            (List of comma separated outgroup taxa for tree rooting)
# [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
# [6] gen_time            (Estimated generation time in years for all species in the tree)
# [7] simphy_seed1        (First random number seed for generate_params.txt)
# [8] simphy_seed2        (Second random number seed for generate_params.txt)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 8) {
  stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
}

# Assign arguments to variables
mod_write_tree2 <- args[1]
treefile <- args[2]
format_tree_out <- args[3]
outgroup <- args[4]
simphy_seed1 <- args[7]
simphy_seed2 <- args[8]

# Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
scale_factor <- as.numeric(args[5])/as.numeric(args[6])

# --- 2. Load Libraries and Setup ---

library(ape)
library(ggplot2)
library(geiger)
library(ggtree)

## Force a headless friendly bitmap device for the session
options(bitmapType = "cairo")

# Adjust path to modified.write.tree2.R
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# --- 3. Process and Save Empirical Tree ---

# Empirical tree:
tree <- read.tree(treefile)

# 1. Convert the comma-separated string into a vector of tip labels
outgroup_taxa <- unlist(strsplit(outgroup, ","))

# 2. Check that the taxa are actually in the tree
if (!all(outgroup_taxa %in% tree$tip.label)) {
    # Check if any taxa in the list are missing from the tree
    missing_taxa <- setdiff(outgroup_taxa, tree$tip.label)
    stop(paste("Error: The following outgroup taxa were not found in the tree:", paste(missing_taxa, collapse=", ")), call.=FALSE)
}

# 3. Find the Most Recent Common Ancestor (MRCA) node index
# getMRCA returns the node index that is ancestral to all tips in the vector
outgroup_node <- getMRCA(phy = tree, tip = outgroup_taxa)

# 4. Root the tree using the identified MRCA node index
# Use the node= argument instead of outgroup=
tree <- root(tree, node = outgroup_node, resolve.root = TRUE)

# Save the check image
p <- ggtree(tree) + theme_tree2() + geom_tiplab()
ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))

# Check the tree rooted correctly. Then transform to ultrametric and rescale

tree_um <- chronos(tree)
class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
tree_um <- rescale(tree_um, model = "depth", scale_factor)

# Save the ultrametric tree check image
q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))

# --- 4. Prepare for SimPhy and Write Outputs ---

# Replace labels with numbers, strip node labels, and write out the tree
tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
tree_um$node.label <- NULL
write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)

# Write the seeds for subsequent dataset parameter simulations
write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))

print("Tree processing complete and output files saved.")
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.1_format_tree.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="process_tree"
#SBATCH --time=12:00:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=2              # Number of CPU cores per task
#SBATCH --mem-per-cpu=6G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to treefile generated by 0.1_iqtree_empirical.sh
treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
# List of outgroup taxa for tree rooting (comma separated)
outgroup="Wallabia_bicolor,Potorous_gilbertii,Pseudochirops_corinnae,Gymnobelideus_leadbeateri,Phalanger_gymnotis,Vombatus_ursinus,Phascolarctos_cinereus,Thylacinus_cynocephalus,Sarcophilus_harrisii,Didelphis_virginiana"
# Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
tree_depth=168000000
# Generation time: Estimated generation time (in years) of all species in the tree:
gen_time=4.5
# Random number seeds for generate_params.txt
simphy_seed1=12345
simphy_seed2=67890

# create output subdirectory:
format_tree_out=$Output/1.1_formatted_empirical_tree
mkdir -p $format_tree_out
cd $format_tree_out

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

## Process Empirical tree
Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $outgroup $tree_depth $gen_time $simphy_seed1 $simphy_seed2

date
---------------------------------
sbatch 1.1_format_tree.sh
Submitted batch job 50606632
sacct -j 50606632 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50606632          0:0   00:00:27
50606632.ba+      0:0   00:00:27    431760K
50606632.ex+      0:0   00:00:27       256K

2025.12.11

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

Skipping simulation_prep.sh and run_sptree_SimPhy.R because they generate simulated species trees which I don’t need for now. I just need loci simulated from the empirical tree above.

2025.12.18

I streamlined the 2.0_prep_simphy.sh bash script by replacing nested loops and relative pathing with a variable-driven structure that utilizes absolute paths and accepts three specific command-line arguments: the rescaled treefile, the parameter file, and the ape patch script.
I restructured the generate_sim_properties.R script to capture these arguments via commandArgs(). I updated the average branch length (abl) sampling range to a natural log scale of -20 to -18, which aligns with estimated mammalian substitution rates.

FEAT: Refactor SimPhy prep workflow for single-tree execution
Streamlined the `2.0_prep_simphy.sh` bash script and `generate_sim_properties.R` to remove nested loops and relative path dependencies. Key updates include:
- Implemented `commandArgs()` in R to ingest tree, parameter, and patch files directly from Bash variables.
- Updated substitution rate (ABL) sampling range to -20 to -18 (natural log) to match mammalian empirical data.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano generate_sim_properties.R
---------------------------------
# generate_sim_properties.R
# Purpose: Generate stochastic evolutionary parameters for 2000 simulated loci
# based on an empirical species tree and mammalian rate ranges.
# The resulting df.csv will serve as the master "blueprint" for Run_SimPhy.R
# df.csv contains every stochastic variable needed to generate gene trees

# --- 1. Capture Command Line Arguments ---
# Arguments must be passed in the following order:
# [1] treefile         (Path to ultrametric species tree rescaled in generations)
# [2] params           (Path to generate_params.txt containing reproducibility seed and Ne)
# [3] mod_write_tree2  (Path to modified.write.tree2.R - custom ape patch for NEXUS output)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 3) {
  stop("Error: Missing arguments. Expected: [1] treefile [2] params [3] mod_write_tree2", call. = FALSE)
}

# Assign arguments to variables
tree_path       <- args[1]
params_path     <- args[2]
mod_write_tree2 <- args[3]

# --- 2. Load Simulation Libraries ---
library(ape)        # Phylogenetics core
library(geiger)     # Tree transformation
library(MultiRNG)   # Multivariate random number generation
library(EnvStats)   # Statistical distributions
library(extraDistr) # Zero-inflated Poisson (rzip) for paralogs/contaminants

# --- 3. Setup and Data Loading ---

# Load the scaled, ultrametric species tree
sptree <- read.tree(tree_path)
ntaxa <- length(sptree$tip.label)

# Apply ape patch to ensure SimPhy-compatible NEXUS formatting
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# Extract simulation-wide constants from parameter input file (Seed and Population Size)
params_vals <- unlist(strsplit(readLines(params_path), " "))
random_seed <- as.numeric(params_vals[1])
Ne          <- as.numeric(params_vals[2])

# Set global seed for exact replication of the 2000-locus parameter set
set.seed(random_seed)

# --- 4. Parameter Generation ---
## Parameters:              Value/Range:     Scientific Context:
## Ave Branch Length (abl)  -20 to -18       natural log range estimated for mammals
## Loci Count (nloci)       2000             Standard for the PML training/testing datasets.
## Locus Length	            200–2000 bp      Longer loci are identified as a top feature for higher RF similarity.
## Heterotachy (vbl)        0.5–2.5          Introduces substitution rate variance, a key interaction factor in wRF models.
## ILS Level (Ne)           in params file   Scales discordance based on the effective population size of the empirical group.
## random seed              in params file   For reproducibility

nloci <- 2000
df <- data.frame(loci = paste0("loc_", as.character(1:nloci)))

# [ABL] Average Branch Length (Substitution Rate)
# Using natural log range -20 to -18 as identified for mammals
abl <- round(runif(nloci, min = -20, max = -18), 3)
df$abl <- abl

# Write Species Tree to NEXUS with the [&R] Rooted tag required by SimPhy
write("#NEXUS", file="sptree.nex")
write("begin trees;", file="sptree.nex", append=T)
write(paste0("\ttree tree_1 = [&R] ", write.tree(sptree, digits=8, file="")), file="sptree.nex", append=T)
write("end;", file="sptree.nex", append=T)

# [VBL] Variance in branch length: Heterotachy (Rate Variance)
# Models rate variation across the tree
vbl <- round(runif(nloci,min=0.5,max=2.5),3)
df <- cbind(df, vbl)

# [Coding Status] CDS? Determines if locus evolves under NUCLEOTIDE or CODON models
proteinCoding <- sample(c(TRUE,FALSE), nloci, TRUE)
df <- cbind(df, proteinCoding)

# [Model Seed] Unique seed for individual locus evolution models
modelseed <- sample(10000:99999,nloci, replace=F)
df <- cbind(df, modelseed)

# [Locus Length] 200-2000 bp.
# Longer loci strongly correlate with improved RF similarity/utility
loclen <- sample(200:2000,nloci, replace=T)
df <- cbind(df, loclen)

# [LambdaPS] Pagel's Lambda
# Scales proportion of phylogenetic signal on internal branches
lambdaPS <- round(runif(nloci,min=0.75,max=1.0),5)
df <- cbind(df, lambdaPS)

# [ILS] Proportional to Effective Population Size (Ne)
# Dictates the level of Incomplete Lineage Sorting
Ne <- rep(Ne, nloci)
df <- cbind(df, Ne)

# [Seeds] Specific seeds for downstream software execution
df$seed1 <- sample(10000:99999, nloci, replace = FALSE) # SimPhy seed
df$seed2 <- ifelse(df$proteinCoding, 54321, 12345)      # INDELible seed

# --- 5. Missing Data Simulation ---
# Simulates stochastic and systematic data loss

# Entirely missing taxa (up to 50% of total taxa)
ntaxa_missing <- sample(0:round(ntaxa/2), nloci, replace = TRUE)
taxa_missing <- list()
remaining_taxa <- list()

for (f in ntaxa_missing){
  txm <- sample(c(1:ntaxa), f, replace = FALSE)
	taxa_missing <- c(taxa_missing, list(txm))
	remaining_taxa <- c(remaining_taxa, list(setdiff(c(1:ntaxa), txm)))
}
df$remaining_taxa <- remaining_taxa
df$taxa_missing <- taxa_missing

# Partially missing segments within remaining taxa
taxa_missing_segments <- lapply(remaining_taxa, function(x) sample(x, round(length(x)/2)))
df$taxa_missing_segments <- taxa_missing_segments
df$missing_segments_prop <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0.2, 0.6), 3))
df$missing_segments_bias <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0, 1), 2))

# --- 6. Paralogy and Contamination ---
# Simulates non-orthologous signal that can confound species tree error[cite: 35, 36, 556].

# Deep paralogs via Zero-Inflated Poisson distribution
nremaining_taxa <- lapply(remaining_taxa, length)
df$paralog_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$paralog_branch_mod <- round(runif(nloci, 1.0, 10.0), 2)
df$paralog_taxa <- apply(df, 1, function(x) sample(x$remaining_taxa, x$paralog_cont))

# Contaminant groups
df$cont_pair_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$cont_pairs <- apply(df, 1, function(x) sample(x$remaining_taxa, x$cont_pair_cont * 2))

# --- 7. Save Blueprint ---
# Convert lists to character strings to prevent CSV formatting errors
df <- as.data.frame(df)
df$remaining_taxa <- gsub("\n", " ", as.character(df$remaining_taxa))
df_out <- apply(df, 2, as.character)

write.csv(df_out, "df.csv", row.names = FALSE)
---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.0_prep_simphy.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="prep_simphy"
#SBATCH --time=6:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu
#SBATCH -c 2
#SBATCH --mem-per-cpu=6G

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to reformatted ultrametric treefile and parameter file generated by 1.1_format_tree.sh
treefile=$Output/1.1_formatted_empirical_tree/s_tree.trees
params=$Output/1.1_formatted_empirical_tree/generate_params.txt

# create output subdirectory:
out_dir=$Output/2.0_simphy_prep
mkdir -p $out_dir
cd $out_dir

# --- Module Management ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

# --- Critical Environment Fixes ---
# 1. Fix for the C++ library (GLIBCXX) errors
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

# 2. Point R to your custom package library
export R_LIBS=~/R-packages

# --- generate_sim_properties.R ---

Rscript $Scripts/2_simulation_scripts/generate_sim_properties.R $treefile $params $mod_write_tree2

date
---------------------------------
sbatch 2.0_prep_simphy.sh
Submitted batch job 50617731
sacct -j 50617731 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50617731          0:0   00:00:13
50617731.ba+      0:0   00:00:13    259484K
50617731.ex+      0:0   00:00:13          0

2025.12.19

Install SimPhy:
Installation instructions for SimPhy can be found at https://github.com/adamallo/SimPhy/wiki/Manual

# 1. Create the directory:
mkdir -p /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy
cd /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy

# 2. Download the 64-bit Linux binary directly from the official release
wget https://github.com/adamallo/SimPhy/releases/download/v1.0.2/SimPhy_1.0.2.tar.gz

# 3. Extract the package
tar -xzvf SimPhy_1.0.2.tar.gz

# 4.
cd SimPhy_1.0.2/bin/

# 5. Make the 64-bit Linux binary executable
chmod +x simphy_lnx64

# 2. Test it
./simphy_lnx64 -h

Path:
/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

2025.12.23

Summary of Changes to run_SimPhy.R:
The refactored run_SimPhy.R has been transformed from an execution script into a dedicated Command Factory.
I removed the internal system() calls and the file-cleaning logic to focus strictly on generating simulation parameters.
The script now accepts exactly five command-line arguments, allowing it to be fully controlled by a parent Bash script.
By utilizing a tidyverse approach with mutate and paste0, it maps the variables from the parameter data frame (df.csv) directly into raw SimPhy shell commands.
This output is written to a plain-text file, which serves as a "blueprint" for parallel execution, increasing the flexibility and scalability of the pipeline.

nano run_SimPhy.R
---------------------------------
# ==============================================================================
# Script: Run_SimPhy.R
# Purpose: Generate a list of SimPhy commands based on generated parameters.
# ==============================================================================

library(tidyverse)

args <- commandArgs(trailingOnly = TRUE)

# Check for the 5 arguments passed from the shell script
if (length(args) < 5) {
  stop("Usage: Rscript Run_SimPhy.R <sptree_path> <df_path> <output_list> <loci_dir> <simphy_path>", call. = FALSE)
}

species_tree_path <- args[1]
df_path           <- args[2]
output_list_path  <- args[3]
loci_dir          <- args[4]
simphy            <- args[5]


# Load the parameter blueprint generated in the previous step
df <- read.csv(df_path)
nloci <- nrow(df)

# Generate the command strings
# Using mutate and paste0 for a clean 'tidyverse' approach
df_cmds <- df %>%
  mutate(command = paste0(
    simphy,
    " -rl f:1",                          # Replicates per locus
    " -sr ", species_tree_path,          # Input species tree
    " -sp f:", Ne,                       # Population size for ILS
    " -su ln:", abl, ",0.1",             # Subst. rate (ln scale)
    " -hs ln:", vbl, ",1",               # Heterotachy (vbl)
    " -cs ", seed1,                      # Random seed
    " -o ", loci_dir, loci               # Locus-specific output folder
  ))

# Write the command list to the text file
writeLines(df_cmds$command, con = output_list_path)

cat(paste("Successfully generated  ", nloci, "SimPhy commands in:", output_list_path, "\n"))
---------------------------------

Summary of Changes to 2.1_simphy_commands.sh:
The 2.1_simphy_commands.sh script was redesigned to serve as the Bridge between parameter generation and execution.
I cleaned up the environment setup by adding cluster-specific fixes, such as the GLIBCXX library path export and the R_LIBS pointer.
I replaced the complex "grep-and-split" logic used in older versions with a clean hand-off to the R generator.
The script now handles directory infrastructure (creating the 2.1 and 2.2 output folders) and passes absolute paths to the R script to avoid any ambiguity.
This separation ensures that the 2,000 commands are validated and stored safely before a single simulation core is even engaged.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.1_simphy_commands.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_cmd
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH -p uri-cpu
#SBATCH --time=1:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory
Output="$Project/output/mammals"
# Path to SimPhy executable:
simphy_exe=/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

# Input files (Generated by 2.0_prep_simphy.sh)
tree_file=$Output/2.0_simphy_prep/sptree.nex
df_file=$Output/2.0_simphy_prep/df.csv

# create output subdirectory:
out_dir=$Output/2.1_commands_simphy
mkdir -p $out_dir
cd $out_dir

# Output files (To be generated)
cmd_list=$out_dir/simphy_command_list.txt
loci_out_dir=$Output/2.2_simulated_loci/ ## note: terminal "/" is necessary! ##

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
# Fix for the C++ library (GLIBCXX) errors:
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
# Point R to your custom package library
export R_LIBS=~/R-packages

# --- run_SimPhy.R ---

Rscript "$Scripts/2_simulation_scripts/run_SimPhy.R" \
    "$tree_file" \
    "$df_file" \
    "$cmd_list" \
    "$loci_out_dir" \
    "$simphy_exe"

date
---------------------------------
sbatch 2.1_simphy_commands.sh
Submitted batch job 50809439
sacct -j 50809439 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50809439          0:0   00:00:17
50809439.ba+      0:0   00:00:17    199912K
50809439.ex+      0:0   00:00:17          0

Git commit message:
feat: refactor SimPhy pipeline for modular parallel execution

- Refactor Run_SimPhy.R to act as a command generator instead of an executor.
- Update Run_SimPhy.R to accept 5 CLI arguments for better path management.
- Standardize 2.1_simphy_commands.sh to utilize absolute paths and HPC environment fixes.
- Remove legacy grep/split logic in favor of a unified command list for GNU Parallel.
- Align Bash-to-R variable hand-off for Ne, seed, and branch length parameters.

Summary of Changes to 2.2_run_simulations.sh:
The 2.2_run_simulations.sh script has been transformed from an execution script into a self-correcting pipeline.
Key changes include the integration of GNU Parallel with a --joblog and --progress monitor to maximize CPU efficiency.
I also implemented a Validation and Auto-Recovery Loop that audits each of the 2,000 expected outputs to ensure files are present and non-zero in size; any failures are automatically extracted from the master command list and re-executed.
Finally, I replaced the find harvesting method with a Sequential Harvesting Loop.
This ensures that the final gene_trees.tre file is perfectly ordered from Locus 1 to 2000, creating a reliable 1:1 mapping between the tree data and the simulation metadata (seeds and Ne)


cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.2_run_simulations.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_run
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # Number of simulations to run in parallel
#SBATCH --mem=16G                 # Adjust based on node availability
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512       # Required for foss/2024a on this cluster
#SBATCH --time=12:00:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to project output directory
Output="$Project/output/mammals"

# Input file (generated by 2.1_simphy_commands.sh):
cmd_list="$Output/2.1_commands_simphy/simphy_command_list.txt"

# Output subdirectory for the simulations:
out_dir="$Output/2.2_simulated_loci"
mkdir -p "$out_dir"

# Output: final combined gene tree file
master_trees="$out_dir/gene_trees.tre"
# Initialize/clear the gene tree log file
> "$master_trees"

# Log: Tracks which simulations succeeded
job_log="$out_dir/2.2_parallel_joblog.log"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load parallel/20240822

# --- 1. Initial Parallel Execution ---
echo "Starting initial SimPhy simulations..."

parallel --jobs $SLURM_CPUS_PER_TASK --joblog "$job_log" --progress < "$cmd_list"
# --jobs $SLURM_CPUS_PER_TASK: Uses the cores requested above
# --joblog: Records completion; if job fails, re-running will skip finished tasks
# --progress: Prints a progress bar and ETA to the slurm log

# --- 2. Validation & Auto-Recovery Loop ---
echo "Verifying simulation integrity..."

# We will loop until all files exist, or set a max number of retry attempts
MAX_RETRIES=2
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    FAILED_LOCI=()

    # Check each of the 2000 expected tree files
    for i in {1..2000}; do
        tree_file="$out_dir/loc_$i/1/g_trees1.trees"

        # If file doesn't exist (-f) OR is empty (! -s)
        if [[ ! -f "$tree_file" || ! -s "$tree_file" ]]; then
            FAILED_LOCI+=($i)
        fi
    done

    # If no failures, break the loop
    if [ ${#FAILED_LOCI[@]} -eq 0 ]; then
        echo "All 2,000 simulations verified successfully."
        break
    else
        RETRY_COUNT=$((RETRY_COUNT + 1))
        echo "WARNING: ${#FAILED_LOCI[@]} simulations failed. Starting Retry #$RETRY_COUNT..."

        # Log the failures
        echo "$(date): Attempt $RETRY_COUNT - Failed Loci: ${FAILED_LOCI[*]}" >> "$out_dir/retry_log.txt"

        # Re-run only the failed commands
        for locus_id in "${FAILED_LOCI[@]}"; do
            # Extract the specific line from the original command list
            # Since loci are loc_1 to loc_2000, we pull line $locus_id
            cmd=$(sed -n "${locus_id}p" "$cmd_list")

            echo "Re-running Locus $locus_id..."
            eval "$cmd"
        done
    fi
done

echo "Final check after retries..."
if [ ${#FAILED_LOCI[@]} -gt 0 ]; then
    echo "CRITICAL: ${#FAILED_LOCI[@]} loci still failing after $MAX_RETRIES retries. Check retry_log.txt"
    exit 1
fi

# --- 3. Sequential Harvesting ---
echo "All simulations verified. Starting sequential harvest into $master_trees..."

# Initialize/clear the master file
> "$master_trees"

# Loop from 1 to 2000 to ensure perfect numerical order
# Line 1 in the tree file will correspond to Locus 1 (Row 1 in df.csv)
for i in {1..2000}
do
    # Define the path to the replicate 1 tree file
    tree_file="$out_dir/loc_$i/1/g_trees1.trees"

    # Append the tree to the master file
    cat "$tree_file" >> "$master_trees"
done

echo "Harvesting complete at $(date)"

# --- 4. Final Verification ---
echo "Performing final audit of $master_trees..."

# Count the number of Newick trees (lines starting with '(')
actual_count=$(grep -c "(" "$master_trees")
expected_count=2000

if [ "$actual_count" -eq "$expected_count" ]; then
    echo "-------------------------------------------------------"
    echo " SUCCESS: 2.2_run_simulations.sh finished successfully."
    echo " Total Trees: $actual_count"
    echo " Order: Sequential (Locus 1 to 2000)"
    echo " Output file: $master_trees"
    echo "-------------------------------------------------------"
else
    echo "-------------------------------------------------------"
    echo " ERROR: Final tree count ($actual_count) does not match"
    echo " expected count ($expected_count)."
    echo " Please check $out_dir/retry_log.txt for clues."
    echo "-------------------------------------------------------"
    exit 1
fi

date
---------------------------------
sbatch 2.2_run_simulations.sh

Git commit message:
feat: implement auto-recovery and sequential harvesting in 2.2_run_simulations.sh

- Add --constraint=avx512 and specific parallel/20240822 module for cluster compatibility.
- Implement a while-loop for auto-recovery that detects and re-runs failed/empty simulations.
- Replace wildcard/find harvesting with a sequential for-loop to preserve locus order (1-2000).
- Add a final grep-based audit to verify the master tree file contains exactly 2000 Newick strings.
- Integrate --joblog and --progress flags for better job monitoring and crash resiliency.

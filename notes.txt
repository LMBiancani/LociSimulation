NOTES - LociSimulation
github: https://github.com/LMBiancani/LociSimulation.git
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"

2025.10.09

ws_allocate -m biancani@uri.edu -r 5 -G pi_rsschwartz_uri_edu LociSimulation 30

Info: creating workspace.
/scratch4/workspace/biancani_uri_edu-LociSimulation
remaining extensions  : 5
remaining time in days: 30

Starting with mammal loci from https://github.com/LMBiancani/PlacentalPolytomy
Original Path to aligned loci:
"/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"

2025.10.09

cd /scratch4/workspace/biancani_uri_edu-LociSimulation

copy mammal loci to scratch space:

transfer.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="tar_transfer"
#SBATCH --time=48:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -p uri-cpu
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=6G

# Define source and destination directories
SOURCE_DIR="/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"
DEST_DIR="/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci"

# Make sure the destination exists
mkdir -p "$DEST_DIR"
echo "Started at $(date)"

# Stream the directory (preserving its name) to scratch
tar -cf - -C "$(dirname "$SOURCE_DIR")" "$(basename "$SOURCE_DIR")" | pv | tar -xf - -C "$DEST_DIR"

echo "Finished at $(date)"
#---------------------------------
sbatch transfer.sh
Submitted batch job 45655769

sacct -j 45655769 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
45655769          0:0   00:01:48
45655769.ba+      0:0   00:01:48   2114772K
45655769.ex+      0:0   00:01:48          0

mkdir /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation

github: https://github.com/LMBiancani/LociSimulation.git

2025.10.20

Use git subtree to copy Molly's updated simulation/machine learning scripts (and preserve commit history)
Molly's repo: https://github.com/mollyodonnellan/PML.git

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
git subtree add --prefix=Scripts https://github.com/mollyodonnellan/PML.git main

Mammal data is too large for AMAS so script needs updating to run through loci in batches.

run_amas.py is a custom batch concatenation wrapper around AMAS.
It processes FASTA alignments in chunks of 1000 files at a time (to avoid overloading AMAS input limitations), concatenates them, and appends the results into cumulative files.
Outputs:
concatenated.fasta
partitions.txt

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano run_amas.py
#---------------------------------
#!/usr/bin/env python3
"""
run_amas.py
------------
Concatenates FASTA alignments in batches using AMAS (to avoid overloading AMAS input limitations).
Creates a concatenated alignment file and a corresponding partition file:
concatenated.fasta
partitions.txt

Usage:
    python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>
"""

import sys
import glob
import subprocess
import os

# --- Input arguments ---
fasta_folder = sys.argv[1]
total_cores = sys.argv[2]
amas = sys.argv[3]

# --- Collect all fasta files (.fasta or .fas) ---
files = [f for f in glob.glob(os.path.join(fasta_folder, "*")) if f.endswith((".fasta", ".fas"))]
files.sort()  # ensure consistent, reproducible order

batch_size = 1000
batch_outputs = []
batch_parts = []

count = 0
fileList = []
batch_num = 1

# --- Process loci in batches ---
for f in files:
    fileList.append(f)
    count += 1

    if count == batch_size:
        batch_fasta = f"amas_batch_{batch_num}.fasta"
        batch_part = f"partitions_batch_{batch_num}.txt"

        cmd = (
            f"python3 {amas} concat "
            f"-f fasta -d dna --out-format fasta --part-format raxml "
            f"-i {' '.join(fileList)} "
            f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
        )
        print(f"\n=== Running AMAS on batch {batch_num} ({len(fileList)} files) ===")
        subprocess.call(cmd, shell=True)

        batch_outputs.append(batch_fasta)
        batch_parts.append(batch_part)
        fileList = []
        count = 0
        batch_num += 1

# --- Final partial batch (if any) ---
if len(fileList) > 0:
    batch_fasta = f"amas_batch_{batch_num}.fasta"
    batch_part = f"partitions_batch_{batch_num}.txt"

    cmd = (
        f"python3 {amas} concat "
        f"-f fasta -d dna --out-format fasta --part-format raxml "
        f"-i {' '.join(fileList)} "
        f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
    )
    print(f"\n=== Running AMAS on final batch {batch_num} ({len(fileList)} files) ===")
    subprocess.call(cmd, shell=True)

    batch_outputs.append(batch_fasta)
    batch_parts.append(batch_part)

# --- Final concatenation across all batches ---
print("\n=== Performing final concatenation across batches ===")

cmd_final = (
    f"python3 {amas} concat "
    f"-f fasta -d dna --out-format fasta --part-format raxml "
    f"-i {' '.join(batch_outputs)} "
    f"-c {total_cores} -t concatenated.fasta -p partitions.txt"
)
subprocess.call(cmd_final, shell=True)

# --- Cleanup temporary batch files ---
print("\n=== Cleaning up intermediate files ===")
for f in batch_outputs + batch_parts:
    try:
        os.remove(f)
    except OSError:
        print(f"Warning: could not remove {f}")

print("\n=== run_amas.py execution completed. ===")
print("Output files:")
print(" - concatenated.fasta")
print(" - partitions.txt\n")
#---------------------------------

0.0_amas_concat.sh runs AMAS on an empirical dataset to concatenate input fasta files and prepare partitions ahead of IQTree run.
Uses a helper Python script (run_amas.py), which wraps around the AMAS.py concat command.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano 0.0_amas_concat.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="AMAS"
#SBATCH --time=2:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory (will be created if necessary)
Output="$Project/output/mammals"
# Path to aligned loci in fasta format:
Data="$Project/mammal_loci/01_SISRS_loci_filtered"
# Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"
# Number of processor cores per node:
Cores=$(echo $SLURM_TASKS_PER_NODE | sed 's/(x.*)//')

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

date
mkdir -p ${Output}/0.0_concatenated
cd ${Output}/0.0_concatenated

#Concatenate input fasta files and prepare partitions ahead of IQTree run
python3 ${Scripts}/0_data_prep/run_amas.py ${Data} ${Cores} ${AMAS}

date
#---------------------------------
sbatch 0.0_amas_concat.sh
Submitted batch job 49112018
sacct -j 49112018 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
49112018          0:0   00:01:39
49112018.ba+      0:0   00:01:39   5584204K
49112018.ex+      0:0   00:01:40          0

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

0.1_iqtree_empirical.sh will use iqtree to infer an empirical tree.

nano 0.1_iqtree_empirical.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="IQTREE"
#SBATCH --time=96:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=24
#SBATCH --mem=250G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to output directory
Output="$Project/output/mammals"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"
# Path to output files from 0.0_amas_concat.sh
Input="$Output/0.0_concatenated"
# Number of cpus per task:
Threads=${SLURM_CPUS_PER_TASK}

module purge

date
mkdir -p ${Output}/0.1_empirical_tree
cd ${Output}/0.1_empirical_tree

# --- Check for input files produced by 0.0_amas_concat.sh---

if [[ ! -f "$Input/concatenated.fasta" || ! -f "$Input/partitions.txt" ]]; then
    echo "Error: concatenated.fasta or partitions.txt not found in ${Input}"
    exit 1
fi

# --- Run IQ-TREE ---
# Flags:
#   -nt: number of CPU threads
#   -spp: partition file allowing different evolutionary rates per partition
#   -pre: prefix for output files
#   -m MFP: ModelFinder Plus for best-fit model selection
#   -bb: ultrafast bootstrap replicates
#   -alrt: SH-like approximate likelihood test replicates

${IQTREE} -nt ${Threads} \
    -s $Input/concatenated.fasta \
    -spp $Input/partitions.txt \
    -pre inferenceEmpirical \
    -m MFP -bb 1000 -alrt 1000

date
#---------------------------------
sbatch 0.1_iqtree_empirical.sh
Submitted batch job 47065851
sacct -j 47065851 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
47065851          0:0 2-17:22:51
47065851.ba+      0:0 2-17:22:51 149134624K
47065851.ex+      0:0 2-17:22:51       256K

2025.11.21

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano install.packages.R
#---------------------------------
# Capture command-line arguments (package names to check and install)
args <- commandArgs(trailingOnly = TRUE)

if (length(args) == 0) {
  stop("Error: No package names provided as arguments. Exiting.", call. = FALSE)
}

# The packages to install are now in the 'args' vector
packages_to_install <- args

# Function to check, install, and load packages
install_and_load <- function(pkg) {
  # Check if the package is installed
  if (!requireNamespace(pkg, quietly = TRUE)) {
    message(paste("Package", pkg, "not found. Installing..."))
    # Install the package from CRAN
    install.packages(pkg, dependencies = TRUE, repos = "https://cloud.r-project.org")
  } else {
    message(paste("Package", pkg, "is already installed."))
  }

  # Load the package (library function)
  library(pkg, character.only = TRUE)
  message(paste("Package", pkg, "loaded successfully."))
}

# Apply the function to the list of packages
invisible(sapply(packages_to_install, install_and_load))
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano empirical_tree_processor.R
### ---------------------------------
### ## Capture Command Line Arguments
###
### # Arguments must be passed in the following order:
### # [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
### # [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
### # [3] format_tree_out     (Base directory for output files)
### # [4] out_tip             (The tip name to use as the outgroup for rooting)
### # [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
### # [6] gen_time            (Estimated generation time in years for all species in the tree)
### # [7] simphy_seed1        (First random number seed for generate_params.txt)
### # [8] simphy_seed2        (Second random number seed for generate_params.txt)
###
### args <- commandArgs(trailingOnly = TRUE)
###
### # Check if the correct number of arguments was provided
### if (length(args) < 8) {
###   stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
### }
###
### # Assign arguments to variables
### mod_write_tree2 <- args[1]
### treefile <- args[2]
### format_tree_out <- args[3]
### out_tip <- args[4]
### simphy_seed1 <- args[7]
### simphy_seed2 <- args[8]
###
### # Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
### scale_factor <- as.numeric(args[5])/as.numeric(args[6])
###
### # --- 2. Load Libraries and Setup ---
###
### library(ape)
### library(ggplot2)
### library(geiger)
### library(ggtree)
###
### ## Force a headless friendly bitmap device for the session
### options(bitmapType = "cairo")
###
### # Adjust path to modified.write.tree2.R
### source(mod_write_tree2)
### assignInNamespace(".write.tree2", .write.tree2, "ape")
###
### # --- 3. Process and Save Empirical Tree ---
###
### # Empirical tree:
### tree <- read.tree(treefile)
###
### # Root the tree and save the check image
### tree <- root(tree, outgroup = out_tip)
### p <- ggtree(tree) + theme_tree2() + geom_tiplab()
### ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))
###
### # Check the tree rooted correctly. Then transform to ultrametric and rescale
###
### tree_um <- chronos(tree)
### class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
### tree_um <- rescale(tree_um, model = "depth", scale_factor)
###
### # Save the ultrametric tree check image
### q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
### ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))
###
### # --- 4. Prepare for SimPhy and Write Outputs ---
###
### # Replace labels with numbers, strip node labels, and write out the tree
### tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
### tree_um$node.label <- NULL
### write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)
###
### # Write the seeds for subsequent dataset parameter simulations
### write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))
###
### print("Tree processing complete and output files saved.")
### ---------------------------------

## INPUTS for empirical_tree_processor.R

# generation scaling factor: Number of generations in the tree: absolute tree age in years divided by average generation time of species in the tree.
total tree depth (years) / average generation time (years)

# total tree depth: age of most recent common ancestor of all species in the tree.
For this mammal data, this would be the age of Theria or age of the common ancestor of placental mammals (ingroup) and marsupials (outgroup)
total tree depth (years) is estimated to be 168 mya
Citations:
2019 - Upham NS, Esselstyn JA, Jetz W (2019) Inferring the mammal tree: Species-level sets of phylogenies for questions in ecology, evolution, and conservation. PLoS Biol 17(12): e3000494.https://doi.org/10.1371/journal.pbio.3000494
168 Fig 1
2017 - Thomas L. Dunwell, Jordi Paps, Peter W. H. Holland; Novel and divergent genes in the evolution of placental mammals. Proc Biol Sci 1 October 2017; 284 (1864): 20171357. https://doi.org/10.1098/rspb.2017.1357
140-191 "The common ancestor of placentals and marsupials dates to approximately 140 – 191 million years ago (Ma)"

tree_depth=168000000

# average generation time (years)
We used a estimated generation time of 4.5 years for Theria. This value is chosen not as the arithmetic mean or median of all extant species (which is skewed by small-bodied rodents and bats), but as a phylogenetically-informed effective generation time that is consistent with the estimated rate of mutation at the deepest nodes of the mammalian tree. This approach accounts for the allometric relationship between body mass and generation length, a relationship thoroughly characterized across Mammalia [cite Pacifici et al., 2013]. Our chosen rate falls within the range utilized by major phylogenomic studies (e.g., Meredith et al., 2011), which established evolutionary rates for ancestral nodes that are consistent with a moderately sized stem mammal ancestor and a generation time in the 4–5 year range."
Citation:
Pacifici, Michela, et al. "Generation length for mammals." Nature Conservation 5 (2013): 89-94.
Meredith, Robert W., et al. "Impacts of the Cretaceous Terrestrial Revolution and KPg extinction on mammal diversification." science 334.6055 (2011): 521-524.

gen_time=4.5

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

### nano 1.0_format_tree.sh - OLD VERSION
### ---------------------------------
### #!/bin/bash
### #SBATCH --job-name="process_tree"
### #SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
### #SBATCH --nodes=1                      # Number of nodes
### #SBATCH --ntasks=1                     # Total number of tasks (processes)
### #SBATCH --cpus-per-task=2              # Number of CPU cores per task
### #SBATCH --mem-per-cpu=6G               # Memory per cpu
### #SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
### #SBATCH --mail-type=ALL
### #SBATCH -p uri-cpu                     # Partition/queue to submit job to
###
### # --- Variables ---
### # Path to project directory:
### Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
### # Path to scripts directory:
### Scripts="$Project/LociSimulation/Scripts"
### # Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
### mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
### # Path to output directory
### Output="$Project/output/mammals"
### # Path to treefile generated by 0.1_iqtree_empirical.sh
### treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
### # List of necessary R packages (separated by spaces)
### R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger"
### # Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
### tree_depth=168000000
### # Generation time: Estimated generation time (in years) of all species in the tree:
### gen_time=4.5
### # Taxon (tip name) to use as the outgroup for rooting
### out_tip="Didelphis_virginiana"
### # Random number seeds for generate_params.txt
### simphy_seed1=12345
### simphy_seed2=67890
###
### # create output subdirectory:
### format_tree_out=$Output/1.0_formatted_empirical_tree
### mkdir -p $format_tree_out
### cd $format_tree_out
###
### module purge
### module load uri/main
### module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
### module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
### module load R/4.3.2-gfbf-2023a # Loads updated R version
### # This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
### # This must be done AFTER module loading to override potential downgrades.
### export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
### export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
###
### ## Install R packages
###
### # add local space for R packages (won't ask about install location):
### mkdir -p ~/R-packages
### export R_LIBS=~/R-packages
###
### # install R packages
### Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages
###
###
### ## Process Empirical tree
### Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $out_tip $tree_depth $gen_time $simphy_seed1 $simphy_seed2
###
### date
### ---------------------------------
### sbatch 1.0_format_tree.sh

Submitted batch job 50314567 - module Error
Error: failing to load the Rcpp.so library due to a missing GLIBCXX_3.4.32 symbol, was caused by a compiler mismatch on the HPC cluster. The R package Rcpp (required by ape) had been compiled using a newer version of the GNU Compiler Collection (GCC) than the one available to R at runtime (GCCcore/11.2.0). This was resolved by updating the Slurm script's module load strategy: we first load the newest available toolchain (foss/2024a) to provide the necessary, updated C++ shared libraries (libstdc++.so.6), and then load a compatible R module (R/4.3.2-gfbf-2023a). Crucially, we also forced the deletion and reinstallation of all local R packages (rm -rf ~/R-packages/*) to ensure they were compiled cleanly using the newly loaded, compatible foss/2024a toolchain.
FIX: Resolve GLIBCXX compiler mismatch in R environment - Updates 1.0_format_tree.sh to load the latest foss toolchain and forces R package reinstallation to fix dependency loading error.

sbatch 1.0_format_tree.sh
Submitted batch job 50314943 - module error
Error: The R package installation failed for geiger and phytools because two of their indirect dependencies, MASS and clusterGeneration, were not found or did not install correctly during the automated dependency resolution. This kind of failure is common on HPC systems when packages have many dependencies. The fix was to explicitly include the missing top-level dependency packages (MASS, clusterGeneration, and phytools) alongside the target packages (ape, ggplot2, geiger, ggtree) in the $R\_packages variable in the Slurm script, forcing the clean reinstallation of all necessary components.
FIX: Explicitly list R package dependencies - Adds MASS, clusterGeneration, and phytools to R_packages variable in 1.0_format_tree.sh to resolve dependency failures for geiger during installation.

sbatch 1.0_format_tree.sh
Submitted batch job 50315543 - module Error
Error: The R package installation failed because the latest version of the MASS package requires R version ≥4.4.0, which is incompatible with the cluster's available R 4.3.2 module. The previous attempt to pin the version failed because install.packages() ignores the version argument when using the main CRAN mirror.
The definitive fix required opening an interactive session (see below) to install MASS version 7.3-60 (a compatible version) by pointing directly to its source file on the CRAN Archive URL (repos = NULL). This forces the installation of the correct package version, resolving the R version compatibility conflict and allowing dependent packages like geiger to install successfully.

# --- Install MASS separately with a pinned, compatible version for R 4.3.2 ---
# Version 7.3-60 is compatible with R 4.3.x
salloc # open interactive session
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export R_LIBS=~/R-packages
R
# --- Install MASS using the direct archive URL (CRITICAL FIX) ---
# This forces the download of the compatible version (7.3-60) for R 4.3.2.
install.packages("https://cran.r-project.org/src/contrib/Archive/MASS/MASS_7.3-60.tar.gz",
                 repos = NULL, # MUST be NULL when installing from a URL
                 type = "source",
                 dependencies = TRUE)

sbatch 1.0_format_tree.sh
Submitted batch job 50315977 - Error
The installation of the R package magick failed because the compiler could not find the external system development files (Magick++.h and the corresponding libraries).
Error: <stdin>:1:10: fatal error: Magick++.h: No such file or directory
Cause: The ImageMagick system library and its development headers are not in your environment's default search paths (neither via a simple module nor the base system).
The R package installation encountered two simultaneous failures:
System Dependency Failure: The R package magick failed to compile because its external system dependency, the ImageMagick development library (Magick++.h), was not found in the environment's default search path.
Recurring Compiler Mismatch: The GLIBCXX_3.4.32 error reappeared for phytools (via igraph) because the conflicting uri/main module was loaded before the necessary foss/2024a toolchain, resulting in a library downgrade.
The fix required a final comprehensive adjustment to the module load order in the Slurm script: explicitly loading the ImageMagick/7.1.1-15-GCCcore-12.3.0 module to provide the missing system headers, and ensuring the foss/2024a toolchain is loaded last to correctly define the newest C++ library environment. This ensures all low-level, system, and R-version-specific dependencies are correctly satisfied.
FIX: Resolve final compilation/dependency errors - Loads ImageMagick module to provide system headers for R 'magick' package, and finalizes module load order (foss/2024a last) to fix recurring GLIBCXX compiler downgrade error

sbatch 1.0_format_tree.sh
Submitted batch job 50316544 - Error
More dependency issues. Try adding igraph to list of R packages

sbatch 1.0_format_tree.sh
Submitted batch job 50316710 - Error
The core problem was a persistent compiler linking error (GLIBCXX_3.4.32 not found), which manifested across multiple R package installations (igraph, phangorn, phytools). Although the correct, newest compiler toolchain (foss/2024a) was loaded, the conflicting uri/main system module or standard library search paths kept defaulting to an older, incompatible library (GCCcore/12.3.0). The definitive fix was to use the path retrieved via module show to manually update the linker path. The Slurm script now explicitly sets LD\_LIBRARY\_PATH to prioritize the directory containing the newest C++ libraries (/modules/uri\_apps/software/GCCcore/13.3.0/lib64), forcing the system to link all new R packages correctly and resolving the chronic loading error.
FIX: Resolve chronic GLIBCXX linking error - Forces the LD_LIBRARY_PATH in 1.0_format_tree.sh to the newest GCCcore path (/modules/uri_apps/software/GCCcore/13.3.0/lib64). This overrides system defaults and fixes the recurring GLIBCXX_3.4.32 linker failure across all compiled R packages.

sbatch 1.0_format_tree.sh
Submitted batch job 50316861 - Error

# --- Install devtools and ggtree (via interactive session) ---
salloc
module purge
module load uri/main
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages
R
# 1. Install devtools (needed for install_github)
if (!requireNamespace("devtools", quietly = TRUE)) {
    install.packages("devtools", dependencies = TRUE, repos = "https://cloud.r-project.org")
}
# 2. Load devtools
library(devtools)
# 3. Install ggtree from GitHub (this often bypasses strict CRAN/Bioconductor version checks)
# Note: You may also need to install the 'BiocManager' package first if this fails.
install_github("YuLab-SMU/ggtree")

sbatch 1.0_format_tree.sh
Submitted batch job 50317295 - worked!!

sacct -j 50317295 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50317295          0:0   00:00:24
50317295.ba+      0:0   00:00:24    428496K
50317295.ex+      0:0   00:00:24          0

* Check re-rooted and ultrametric trees! - Issue. only rooting with one taxa instead of entire outgroup.
Thoughts:
As of now the tree is being rooted by specifying out_tip (tip name to use as the outgroup for rooting). However, many trees have a clade as an outgroup, not a single taxa. I know ggtree can root using a node instead of a taxon:
rooted_tree <- root(tree, node=109, resolve.root = TRUE, edgelabel=TRUE)
However, I don't know the node number for my outgroup.
The following code would produce a tree where the nodes are labeled with their node numbers, but I'd need to manually look at the tree and see what the outgroup node is labeled.
t <- ggtree(tree, layout="rectangular", size=1, branch.length="none") + geom_text(aes(label=node)) + geom_tiplab(align=TRUE, hjust=-.15)
What I would like to do, is write an R ggtree function or similar that takes a list of all the outgroup taxa and returns the number of the node the represents the common ancestor for those taxa (which would subsequently be used for rooting the tree)
To root a phylogenetic tree using an entire outgroup clade instead of a single taxon, the manual lookup of node numbers in a ggtree plot is unnecessary. The required functionality is available in the ape package, which is already loaded in the script. The function to use is getMRCA() (Most Recent Common Ancestor). This function efficiently takes two arguments: the phylogenetic tree object and a vector containing the names of all the outgroup tip labels. It then automatically returns the specific node index corresponding to the common ancestor of that entire outgroup clade. This node index can then be passed directly to the root() function for accurate clade-based rooting.

To transition from rooting the phylogenetic tree using a single taxon to using an entire outgroup clade, the script was updated to leverage the ape::getMRCA() function. This involved two main changes:
Input Change: The command-line argument $out_tip was replaced with a string of comma-separated outgroup taxa (e.g., Didelphis_virginiana,Monodelphis_domestica).
R Logic: The empirical_tree_processor.R script was modified to:
Parse the input string into an R vector of tip labels using strsplit().
Use getMRCA(tree, tip = outgroup_taxa) to programmatically find the integer node index corresponding to the most recent common ancestor of all specified outgroup tips.
Replace the single-taxon root(..., outgroup=...) call with the root(..., node=outgroup_node) call, ensuring the tree is rooted correctly on the ancestral node of the entire outgroup clade.

Making updates scripts...

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.0_prep_R_env.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="R_pkgs"
#SBATCH --time=72:00:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=1              # Number of CPU cores per task
#SBATCH --mem-per-cpu=6G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# List of necessary R packages (separated by spaces)
R_packages="igraph phangorn MASS clusterGeneration ape ggplot2 phytools geiger MultiRNG EnvStats extraDistr tidyverse castor"

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

## Install R packages

# add local space for R packages (won't ask about install location):
mkdir -p ~/R-packages
export R_LIBS=~/R-packages

# install R packages
Rscript ${Scripts}/1_prep_empirical_tree/install.packages.R $R_packages

date
#---------------------------------
sbatch 1.0_prep_R_env.sh
Submitted batch job 50606562
sacct -j 50606562 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50606562          0:0   00:17:31
50606562.ba+      0:0   00:17:31   1498740K
50606562.ex+      0:0   00:17:31       256K

FEAT: Implement clade-based tree rooting via MRCA
Replaces single-taxon rooting with support for multi-taxa outgroups. Uses ape::getMRCA() to find the common ancestor node index for a comma-separated list of outgroup tips, ensuring accurate clade-based tree rooting.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano empirical_tree_processor.R
#---------------------------------
## Capture Command Line Arguments

# Arguments must be passed in the following order:
# [1] mod_write_tree2     (Path to modified.write.tree2.R - script that adjusts how ape handles tree files)
# [2] treefile            (Path to the input .treefile - generated by 0.1_iqtree_empirical.sh)
# [3] format_tree_out     (Base directory for output files)
# [4] outgroup            (List of comma separated outgroup taxa for tree rooting)
# [5] tree_depth          (Estimated age in years of most recent common ancestor of all species in the tree)
# [6] gen_time            (Estimated generation time in years for all species in the tree)
# [7] simphy_seed1        (First random number seed for generate_params.txt)
# [8] simphy_seed2        (Second random number seed for generate_params.txt)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 8) {
  stop("Error: Not enough arguments provided. Expected 8.", call. = FALSE)
}

# Assign arguments to variables
mod_write_tree2 <- args[1]
treefile <- args[2]
format_tree_out <- args[3]
outgroup <- args[4]
simphy_seed1 <- args[7]
simphy_seed2 <- args[8]

# Calculate the scale factor (Total Generations) from args[5] (depth) and args[6] (gen_time)
scale_factor <- as.numeric(args[5])/as.numeric(args[6])

# --- 2. Load Libraries and Setup ---

library(ape)
library(ggplot2)
library(geiger)
library(ggtree)

## Force a headless friendly bitmap device for the session
options(bitmapType = "cairo")

# Adjust path to modified.write.tree2.R
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# --- 3. Process and Save Empirical Tree ---

# Empirical tree:
tree <- read.tree(treefile)

# 1. Convert the comma-separated string into a vector of tip labels
outgroup_taxa <- unlist(strsplit(outgroup, ","))

# 2. Check that the taxa are actually in the tree
if (!all(outgroup_taxa %in% tree$tip.label)) {
    # Check if any taxa in the list are missing from the tree
    missing_taxa <- setdiff(outgroup_taxa, tree$tip.label)
    stop(paste("Error: The following outgroup taxa were not found in the tree:", paste(missing_taxa, collapse=", ")), call.=FALSE)
}

# 3. Find the Most Recent Common Ancestor (MRCA) node index
# getMRCA returns the node index that is ancestral to all tips in the vector
outgroup_node <- getMRCA(phy = tree, tip = outgroup_taxa)

# 4. Root the tree using the identified MRCA node index
# Use the node= argument instead of outgroup=
tree <- root(tree, node = outgroup_node, resolve.root = TRUE)

# Save the check image
p <- ggtree(tree) + theme_tree2() + geom_tiplab()
ggsave(p, file=paste0(format_tree_out, "/tree_check_root.png"))

# Check the tree rooted correctly. Then transform to ultrametric and rescale

tree_um <- chronos(tree)
class(tree_um) <- "phylo" # Ensure it is a 'phylo' object after chronos
tree_um <- rescale(tree_um, model = "depth", scale_factor)

# Save the ultrametric tree check image
q <- ggtree(tree_um) + theme_tree2() + geom_tiplab()
ggsave(q, file=paste0(format_tree_out, "/tree_um.png"))

# --- 4. Prepare for SimPhy and Write Outputs ---

taxon_map <- data.frame(number = 1:length(tree_um$tip.label), name = tree_um$tip.label)
write.csv(taxon_map, paste0(format_tree_out, "/taxon_map.csv"), row.names = FALSE)

# Replace labels with numbers, strip node labels, and write out the tree
tree_um$tip.label <- as.character(1:length(tree_um$tip.label))
tree_um$node.label <- NULL
write.tree(tree_um, paste0(format_tree_out, "/s_tree.trees"), digits=8)

# Write the seeds for subsequent dataset parameter simulations
write(c(simphy_seed1, simphy_seed2), paste0(format_tree_out, "/generate_params.txt"))

print("Tree processing complete and output files saved.")
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/1_prep_empirical_tree

nano 1.1_format_tree.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="process_tree"
#SBATCH --time=00:20:00                # Walltime limit (HH:MM:SS)
#SBATCH --nodes=1                      # Number of nodes
#SBATCH --ntasks=1                     # Total number of tasks (processes)
#SBATCH --cpus-per-task=1              # Number of CPU cores per task
#SBATCH --mem-per-cpu=4G               # Memory per cpu
#SBATCH --mail-user="biancani@uri.edu" # CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu                     # Partition/queue to submit job to

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to treefile generated by 0.1_iqtree_empirical.sh
treefile=$Output/0.1_empirical_tree/inferenceEmpirical.treefile
# List of outgroup taxa for tree rooting (comma separated)
outgroup="Wallabia_bicolor,Potorous_gilbertii,Pseudochirops_corinnae,Gymnobelideus_leadbeateri,Phalanger_gymnotis,Vombatus_ursinus,Phascolarctos_cinereus,Thylacinus_cynocephalus,Sarcophilus_harrisii,Didelphis_virginiana"
# Total tree depth: Estimated age (in years) of most recent common ancestor of all species in the tree
tree_depth=168000000
# Generation time: Estimated generation time (in years) of all species in the tree:
gen_time=4.5
# Random number seeds for generate_params.txt
simphy_seed1=12345
simphy_seed2=67890

# create output subdirectory:
format_tree_out=$Output/1.1_formatted_empirical_tree
mkdir -p $format_tree_out
cd $format_tree_out

module purge
module load uri/main
module load ImageMagick/7.1.1-15-GCCcore-12.3.0 # system dependency for the R 'magick' package
module load foss/2024a # Loads an updated toolchain to provide the required C++ library: GLIBCXX_3.4.32 (fixes GLIBCXX error)
module load R/4.3.2-gfbf-2023a # Loads updated R version
# This forces the linker to find the correct, newest C++ library that contains GLIBCXX_3.4.32.
# This must be done AFTER module loading to override potential downgrades.
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

## Process Empirical tree
Rscript $Scripts/1_prep_empirical_tree/empirical_tree_processor.R $mod_write_tree2 $treefile $format_tree_out $outgroup $tree_depth $gen_time $simphy_seed1 $simphy_seed2

date
#---------------------------------
sbatch 1.1_format_tree.sh
Submitted batch job 50824267
sacct -j 50824267 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824267          0:0   00:00:35
50824267.ba+      0:0   00:00:35    465276K
50824267.ex+      0:0   00:00:35

2025.12.11

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

Skipping simulation_prep.sh and run_sptree_SimPhy.R because they generate simulated species trees which I don’t need for now. I just need loci simulated from the empirical tree above.

2025.12.18

I streamlined the 2.0_prep_simphy.sh bash script by replacing nested loops and relative pathing with a variable-driven structure that utilizes absolute paths and accepts three specific command-line arguments: the rescaled treefile, the parameter file, and the ape patch script.
I restructured the generate_sim_properties.R script to capture these arguments via commandArgs(). I updated the average branch length (abl) sampling range to a natural log scale of -20 to -18, which aligns with estimated mammalian substitution rates.

FEAT: Refactor SimPhy prep workflow for single-tree execution
Streamlined the `2.0_prep_simphy.sh` bash script and `generate_sim_properties.R` to remove nested loops and relative path dependencies. Key updates include:
- Implemented `commandArgs()` in R to ingest tree, parameter, and patch files directly from Bash variables.
- Updated substitution rate (ABL) sampling range to -20 to -18 (natural log) to match mammalian empirical data.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano generate_sim_properties.R
#---------------------------------
# generate_sim_properties.R
# Purpose: Generate stochastic evolutionary parameters for 2000 simulated loci
# based on an empirical species tree and mammalian rate ranges.
# The resulting df.csv will serve as the master "blueprint" for Run_SimPhy.R
# df.csv contains every stochastic variable needed to generate gene trees

# --- 1. Capture Command Line Arguments ---
# Arguments must be passed in the following order:
# [1] treefile         (Path to ultrametric species tree rescaled in generations)
# [2] params           (Path to generate_params.txt containing reproducibility seed and Ne)
# [3] mod_write_tree2  (Path to modified.write.tree2.R - custom ape patch for NEXUS output)

args <- commandArgs(trailingOnly = TRUE)

# Check if the correct number of arguments was provided
if (length(args) < 3) {
  stop("Error: Missing arguments. Expected: [1] treefile [2] params [3] mod_write_tree2", call. = FALSE)
}

# Assign arguments to variables
tree_path       <- args[1]
params_path     <- args[2]
mod_write_tree2 <- args[3]

# --- 2. Load Simulation Libraries ---
library(ape)        # Phylogenetics core
library(geiger)     # Tree transformation
library(MultiRNG)   # Multivariate random number generation
library(EnvStats)   # Statistical distributions
library(extraDistr) # Zero-inflated Poisson (rzip) for paralogs/contaminants

# --- 3. Setup and Data Loading ---

# Load the scaled, ultrametric species tree
sptree <- read.tree(tree_path)
ntaxa <- length(sptree$tip.label)

# Apply ape patch to ensure SimPhy-compatible NEXUS formatting
source(mod_write_tree2)
assignInNamespace(".write.tree2", .write.tree2, "ape")

# Extract simulation-wide constants from parameter input file (Seed and Population Size)
params_vals <- unlist(strsplit(readLines(params_path), " "))
random_seed <- as.numeric(params_vals[1])
Ne          <- as.numeric(params_vals[2])

# Set global seed for exact replication of the 2000-locus parameter set
set.seed(random_seed)

# --- 4. Parameter Generation ---
## Parameters:              Value/Range:     Scientific Context:
## Ave Branch Length (abl)  -20 to -18       natural log range estimated for mammals
## Loci Count (nloci)       2000             Standard for the PML training/testing datasets.
## Locus Length	            200–2000 bp      Longer loci are identified as a top feature for higher RF similarity.
## Heterotachy (vbl)        0.5–2.5          Introduces substitution rate variance, a key interaction factor in wRF models.
## ILS Level (Ne)           in params file   Scales discordance based on the effective population size of the empirical group.
## random seed              in params file   For reproducibility

nloci <- 2000
df <- data.frame(loci = paste0("loc_", as.character(1:nloci)))

# [ABL] Average Branch Length (Substitution Rate)
# Using natural log range -20 to -18 as identified for mammals
abl <- round(runif(nloci, min = -20, max = -18), 3)
df$abl <- abl

# Write Species Tree to NEXUS with the [&R] Rooted tag required by SimPhy
write("#NEXUS", file="sptree.nex")
write("begin trees;", file="sptree.nex", append=T)
write(paste0("\ttree tree_1 = [&R] ", write.tree(sptree, digits=8, file="")), file="sptree.nex", append=T)
write("end;", file="sptree.nex", append=T)

# [VBL] Variance in branch length: Heterotachy (Rate Variance)
# Models rate variation across the tree
vbl <- round(runif(nloci,min=0.5,max=2.5),3)
df <- cbind(df, vbl)

# [Coding Status] CDS? Determines if locus evolves under NUCLEOTIDE or CODON models
proteinCoding <- sample(c(TRUE,FALSE), nloci, TRUE)
df <- cbind(df, proteinCoding)

# [Model Seed] Unique seed for individual locus evolution models
modelseed <- sample(10000:99999,nloci, replace=F)
df <- cbind(df, modelseed)

# [Locus Length] 200-2000 bp.
# Longer loci strongly correlate with improved RF similarity/utility
loclen <- sample(200:2000,nloci, replace=T)
df <- cbind(df, loclen)

# [LambdaPS] Pagel's Lambda
# Scales proportion of phylogenetic signal on internal branches
lambdaPS <- round(runif(nloci,min=0.75,max=1.0),5)
df <- cbind(df, lambdaPS)

# [ILS] Proportional to Effective Population Size (Ne)
# Dictates the level of Incomplete Lineage Sorting
Ne <- rep(Ne, nloci)
df <- cbind(df, Ne)

# [Seeds] Specific seeds for downstream software execution
df$seed1 <- sample(10000:99999, nloci, replace = FALSE) # SimPhy seed
df$seed2 <- ifelse(df$proteinCoding, 54321, 12345)      # INDELible seed

# --- 5. Missing Data Simulation ---
# Simulates stochastic and systematic data loss

# Entirely missing taxa (up to 50% of total taxa)
ntaxa_missing <- sample(0:round(ntaxa/2), nloci, replace = TRUE)
taxa_missing <- list()
remaining_taxa <- list()

for (f in ntaxa_missing){
  txm <- sample(c(1:ntaxa), f, replace = FALSE)
	taxa_missing <- c(taxa_missing, list(txm))
	remaining_taxa <- c(remaining_taxa, list(setdiff(c(1:ntaxa), txm)))
}
df$remaining_taxa <- remaining_taxa
df$taxa_missing <- taxa_missing

# Partially missing segments within remaining taxa
taxa_missing_segments <- lapply(remaining_taxa, function(x) sample(x, round(length(x)/2)))
df$taxa_missing_segments <- taxa_missing_segments
df$missing_segments_prop <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0.2, 0.6), 3))
df$missing_segments_bias <- lapply(taxa_missing_segments, function(x) round(runif(length(x), 0, 1), 2))

# --- 6. Paralogy and Contamination ---
# Simulates non-orthologous signal that can confound species tree error[cite: 35, 36, 556].

# Deep paralogs via Zero-Inflated Poisson distribution
nremaining_taxa <- lapply(remaining_taxa, length)
df$paralog_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$paralog_branch_mod <- round(runif(nloci, 1.0, 10.0), 2)
df$paralog_taxa <- apply(df, 1, function(x) sample(x$remaining_taxa, x$paralog_cont))

# Contaminant groups
df$cont_pair_cont <- rzip(nloci, unlist(nremaining_taxa)/(unlist(nremaining_taxa)/2), 0.5)
df$cont_pairs <- apply(df, 1, function(x) sample(x$remaining_taxa, x$cont_pair_cont * 2))

# --- 7. Save Blueprint ---
# Convert lists to character strings to prevent CSV formatting errors
df <- as.data.frame(df)
df$remaining_taxa <- gsub("\n", " ", as.character(df$remaining_taxa))
df_out <- apply(df, 2, as.character)

write.csv(df_out, "df.csv", row.names = FALSE)
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.0_prep_simphy.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="prep_simphy"
#SBATCH --time=00:20:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL
#SBATCH -p uri-cpu
#SBATCH -c 1
#SBATCH --mem-per-cpu=2G

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to modified.write.tree2.R (script that adjusts how ape handles tree files)
mod_write_tree2=$Scripts/2_simulation_scripts/modified.write.tree2.R
# Path to output directory
Output="$Project/output/mammals"
# Path to reformatted ultrametric treefile and parameter file generated by 1.1_format_tree.sh
treefile=$Output/1.1_formatted_empirical_tree/s_tree.trees
params=$Output/1.1_formatted_empirical_tree/generate_params.txt

# create output subdirectory:
out_dir=$Output/2.0_simphy_prep
mkdir -p $out_dir
cd $out_dir

# --- Module Management ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

# --- Critical Environment Fixes ---
# 1. Fix for the C++ library (GLIBCXX) errors
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

# 2. Point R to your custom package library
export R_LIBS=~/R-packages

# --- generate_sim_properties.R ---

Rscript $Scripts/2_simulation_scripts/generate_sim_properties.R $treefile $params $mod_write_tree2

date
#---------------------------------
sbatch 2.0_prep_simphy.sh
Submitted batch job 50824315
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824315          0:0   00:00:17
50824315.ba+      0:0   00:00:17    290704K
50824315.ex+      0:0   00:00:17

2025.12.19

Install SimPhy:
Installation instructions for SimPhy can be found at https://github.com/adamallo/SimPhy/wiki/Manual

# 1. Create the directory:
mkdir -p /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy
cd /project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy

# 2. Download the 64-bit Linux binary directly from the official release
wget https://github.com/adamallo/SimPhy/releases/download/v1.0.2/SimPhy_1.0.2.tar.gz

# 3. Extract the package
tar -xzvf SimPhy_1.0.2.tar.gz

# 4.
cd SimPhy_1.0.2/bin/

# 5. Make the 64-bit Linux binary executable
chmod +x simphy_lnx64

# 2. Test it
./simphy_lnx64 -h

Path:
/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

2025.12.23

Summary of Changes to run_SimPhy.R:
The refactored run_SimPhy.R has been transformed from an execution script into a dedicated Command Factory.
I removed the internal system() calls and the file-cleaning logic to focus strictly on generating simulation parameters.
The script now accepts exactly five command-line arguments, allowing it to be fully controlled by a parent Bash script.
By utilizing a tidyverse approach with mutate and paste0, it maps the variables from the parameter data frame (df.csv) directly into raw SimPhy shell commands.
This output is written to a plain-text file, which serves as a "blueprint" for parallel execution, increasing the flexibility and scalability of the pipeline.

nano run_SimPhy.R
#---------------------------------
# ==============================================================================
# Script: Run_SimPhy.R
# Purpose: Generate a list of SimPhy commands based on generated parameters.
# ==============================================================================

library(tidyverse)

args <- commandArgs(trailingOnly = TRUE)

# Check for the 5 arguments passed from the shell script
if (length(args) < 5) {
  stop("Usage: Rscript Run_SimPhy.R <sptree_path> <df_path> <output_list> <loci_dir> <simphy_path>", call. = FALSE)
}

species_tree_path <- args[1]
df_path           <- args[2]
output_list_path  <- args[3]
loci_dir          <- args[4]
simphy            <- args[5]


# Load the parameter blueprint generated in the previous step
df <- read.csv(df_path)
nloci <- nrow(df)

# Generate the command strings
# Using mutate and paste0 for a clean 'tidyverse' approach
df_cmds <- df %>%
  mutate(command = paste0(
    simphy,
    " -rl f:1",                          # Replicates per locus
    " -sr ", species_tree_path,          # Input species tree
    " -sp f:", Ne,                       # Population size for ILS
    " -su ln:", abl, ",0.1",             # Subst. rate (ln scale)
    " -hs ln:", vbl, ",1",               # Heterotachy (vbl)
    " -cs ", seed1,                      # Random seed
    " -o ", loci_dir, loci               # Locus-specific output folder
  ))

# Write the command list to the text file
writeLines(df_cmds$command, con = output_list_path)

cat(paste("Successfully generated  ", nloci, "SimPhy commands in:", output_list_path, "\n"))
#---------------------------------

Summary of Changes to 2.1_simphy_commands.sh:
The 2.1_simphy_commands.sh script was redesigned to serve as the Bridge between parameter generation and execution.
I cleaned up the environment setup by adding cluster-specific fixes, such as the GLIBCXX library path export and the R_LIBS pointer.
I replaced the complex "grep-and-split" logic used in older versions with a clean hand-off to the R generator.
The script now handles directory infrastructure (creating the 2.1 and 2.2 output folders) and passes absolute paths to the R script to avoid any ambiguity.
This separation ensures that the 2,000 commands are validated and stored safely before a single simulation core is even engaged.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.1_simphy_commands.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_cmd
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH -p uri-cpu
#SBATCH --time=0:20:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu" #CHANGE TO user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts"
# Path to output directory
Output="$Project/output/mammals"
# Path to SimPhy executable:
simphy_exe=/project/pi_rsschwartz_uri_edu/Biancani/Software/SimPhy/SimPhy_1.0.2/bin/simphy_lnx64

# Input files (Generated by 2.0_prep_simphy.sh)
tree_file=$Output/2.0_simphy_prep/sptree.nex
df_file=$Output/2.0_simphy_prep/df.csv

# create output subdirectory:
out_dir=$Output/2.1_commands_simphy
mkdir -p $out_dir
cd $out_dir

# Output files (To be generated)
cmd_list=$out_dir/simphy_command_list.txt
loci_out_dir=$Output/2.2_simulated_loci/ ## note: terminal "/" is necessary! ##

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
# Fix for the C++ library (GLIBCXX) errors:
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
# Point R to your custom package library
export R_LIBS=~/R-packages

# --- run_SimPhy.R ---

Rscript "$Scripts/2_simulation_scripts/run_SimPhy.R" \
    "$tree_file" \
    "$df_file" \
    "$cmd_list" \
    "$loci_out_dir" \
    "$simphy_exe"

date
#---------------------------------
sbatch 2.1_simphy_commands.sh
Submitted batch job 50824346
sacct -j 50824346 -o JobID,ExitCode,Elapsed,MaxRSS
sacct -j 50824346 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824346          0:0   00:00:14
50824346.ba+      0:0   00:00:14    189344K
50824346.ex+      0:0   00:00:14

Git commit message:
feat: refactor SimPhy pipeline for modular parallel execution

- Refactor Run_SimPhy.R to act as a command generator instead of an executor.
- Update Run_SimPhy.R to accept 5 CLI arguments for better path management.
- Standardize 2.1_simphy_commands.sh to utilize absolute paths and HPC environment fixes.
- Remove legacy grep/split logic in favor of a unified command list for GNU Parallel.
- Align Bash-to-R variable hand-off for Ne, seed, and branch length parameters.

Summary of Changes to 2.2_run_simulations.sh:
The 2.2_run_simulations.sh script has been transformed from an execution script into a self-correcting pipeline.
Key changes include the integration of GNU Parallel with a --joblog and --progress monitor to maximize CPU efficiency.
I also implemented a Validation and Auto-Recovery Loop that audits each of the 2,000 expected outputs to ensure files are present and non-zero in size; any failures are automatically extracted from the master command list and re-executed.
Finally, I replaced the find harvesting method with a Sequential Harvesting Loop.
This ensures that the final gene_trees.tre file is perfectly ordered from Locus 1 to 2000, creating a reliable 1:1 mapping between the tree data and the simulation metadata (seeds and Ne)


cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.2_run_simulations.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=simphy_run
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # Number of simulations to run in parallel
#SBATCH --mem=8G                 # Adjust based on node availability
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512       # Required for foss/2024a on this cluster
#SBATCH --time=00:30:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to project output directory
Output="$Project/output/mammals"

# Input file (generated by 2.1_simphy_commands.sh):
cmd_list="$Output/2.1_commands_simphy/simphy_command_list.txt"

# Output subdirectory for the simulations:
out_dir="$Output/2.2_simulated_loci"
mkdir -p "$out_dir"

# Output: final combined gene tree file
master_trees="$out_dir/gene_trees.tre"
# Initialize/clear the gene tree log file
> "$master_trees"

# Log: Tracks which simulations succeeded
job_log="$out_dir/2.2_parallel_joblog.log"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load parallel/20240822

# --- 1. Initial Parallel Execution ---
echo "Starting initial SimPhy simulations..."

parallel --jobs $SLURM_CPUS_PER_TASK --joblog "$job_log" --progress < "$cmd_list"
# --jobs $SLURM_CPUS_PER_TASK: Uses the cores requested above
# --joblog: Records completion; if job fails, re-running will skip finished tasks
# --progress: Prints a progress bar and ETA to the slurm log

# --- 2. Validation & Auto-Recovery Loop ---
echo "Verifying simulation integrity..."

# We will loop until all files exist, or set a max number of retry attempts
MAX_RETRIES=2
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    FAILED_LOCI=()

    # Check each of the 2000 expected tree files
    for i in {1..2000}; do
        tree_file="$out_dir/loc_$i/1/g_trees1.trees"

        # If file doesn't exist (-f) OR is empty (! -s)
        if [[ ! -f "$tree_file" || ! -s "$tree_file" ]]; then
            FAILED_LOCI+=($i)
        fi
    done

    # If no failures, break the loop
    if [ ${#FAILED_LOCI[@]} -eq 0 ]; then
        echo "All 2,000 simulations verified successfully."
        break
    else
        RETRY_COUNT=$((RETRY_COUNT + 1))
        echo "WARNING: ${#FAILED_LOCI[@]} simulations failed. Starting Retry #$RETRY_COUNT..."

        # Log the failures
        echo "$(date): Attempt $RETRY_COUNT - Failed Loci: ${FAILED_LOCI[*]}" >> "$out_dir/retry_log.txt"

        # Re-run only the failed commands
        for locus_id in "${FAILED_LOCI[@]}"; do
            # Extract the specific line from the original command list
            # Since loci are loc_1 to loc_2000, we pull line $locus_id
            cmd=$(sed -n "${locus_id}p" "$cmd_list")

            echo "Re-running Locus $locus_id..."
            eval "$cmd"
        done
    fi
done

echo "Final check after retries..."
if [ ${#FAILED_LOCI[@]} -gt 0 ]; then
    echo "CRITICAL: ${#FAILED_LOCI[@]} loci still failing after $MAX_RETRIES retries. Check retry_log.txt"
    exit 1
fi

# --- 3. Sequential Harvesting ---
echo "All simulations verified. Starting sequential harvest into $master_trees..."

# Initialize/clear the master file
> "$master_trees"

# Loop from 1 to 2000 to ensure perfect numerical order
# Line 1 in the tree file will correspond to Locus 1 (Row 1 in df.csv)
for i in {1..2000}
do
    # Define the path to the replicate 1 tree file
    tree_file="$out_dir/loc_$i/1/g_trees1.trees"

    # Append the tree to the master file
    cat "$tree_file" >> "$master_trees"
done

echo "Harvesting complete at $(date)"

# --- 4. Final Verification ---
echo "Performing final audit of $master_trees..."

# Count the number of Newick trees (lines starting with '(')
actual_count=$(grep -c "(" "$master_trees")
expected_count=2000

if [ "$actual_count" -eq "$expected_count" ]; then
    echo "-------------------------------------------------------"
    echo " SUCCESS: 2.2_run_simulations.sh finished successfully."
    echo " Total Trees: $actual_count"
    echo " Order: Sequential (Locus 1 to 2000)"
    echo " Output file: $master_trees"
    echo "-------------------------------------------------------"
else
    echo "-------------------------------------------------------"
    echo " ERROR: Final tree count ($actual_count) does not match"
    echo " expected count ($expected_count)."
    echo " Please check $out_dir/retry_log.txt for clues."
    echo "-------------------------------------------------------"
    exit 1
fi

date
#---------------------------------
sbatch 2.2_run_simulations.sh
Submitted batch job 50824418
sacct -j 50824418 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824418          0:0   00:00:29
50824418.ba+      0:0   00:00:29    189884K
50824418.ex+      0:0   00:00:29

Replicate 1 of 1: Simulating 1 gene trees from 1 locus trees... Done
All 2,000 simulations verified successfully.
Final check after retries...
All simulations verified. Starting sequential harvest into /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre...
Harvesting complete at Mon Dec 29 03:39:19 AM UTC 2025
Performing final audit of /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre...
-------------------------------------------------------
 SUCCESS: 2.2_run_simulations.sh finished successfully.
 Total Trees: 2000
 Order: Sequential (Locus 1 to 2000)
 Output file: /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.2_simulated_loci/gene_trees.tre
-------------------------------------------------------
Mon Dec 29 03:39:19 AM UTC 2025

Git commit message:
feat: implement auto-recovery and sequential harvesting in 2.2_run_simulations.sh

- Add --constraint=avx512 and specific parallel/20240822 module for cluster compatibility.
- Implement a while-loop for auto-recovery that detects and re-runs failed/empty simulations.
- Replace wildcard/find harvesting with a sequential for-loop to preserve locus order (1-2000).
- Add a final grep-based audit to verify the master tree file contains exactly 2000 Newick strings.
- Integrate --joblog and --progress flags for better job monitoring and crash resiliency.

Workflow Check

2.2 creates the trees.
2.3 (Bash) sets up the environment and runs Parallel.
2.3 (Prep R) creates 2,000 control files.
2.3 (Parallel) runs INDELible and renames outputs to locus_i.phy.
2.3 (Post R) reads those .phy files, adds contamination/gaps, and saves as .fas.

Summary of Changes to INDELible scripts:
The simulation pipeline was overhauled to transition from a serialized, directory-dependent structure to a parallelized Control-File Architecture.

2.3_INDELible.sh: I implemented a "Thread-Safe" execution model using GNU Parallel.
By creating unique temporary working directories (tmp_$i) for each of the 2,000 tasks, I bypassed INDELible’s limitation of requiring a fixed filename (control.txt).
This allows 32 simulations to run concurrently without file-access collisions, reducing runtime.

prep_INDELible.R: The script was refactored to be fully modular and portable.
It now accepts dynamic paths for helper scripts and output directories as arguments.
The core logic was modified to output 2,000 individual control files, and the Nucleotide Substitution Model logic was restored to ensure that the mathematical parameters (GTR, HKY, etc.) are correctly mapped to INDELible’s internal commands.

post_INDELible.R: This script was synchronized with the new naming convention (locus_f.phy) and updated to handle the massive output in a single pass.
I updated the column-removal logic and ensured the final output is saved in a standardized FASTA format within a dedicated alignments_final directory.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano prep_INDELible.R
#---------------------------------
library(ape)
library(geiger)
library(extraDistr)
library(MultiRNG)
library(EnvStats)
library(castor)
library(phangorn)
library(tidyverse)

# --- 1. Capture Arguments ---
args = commandArgs(trailingOnly=TRUE)
gene_trees_path <- args[1]
df_path         <- args[2]
out_dir         <- args[3]
script_dir      <- args[4] # This is the $Scripts/2_simulation_scripts path

# --- 2. Dynamic Sourcing ---
# Use file.path for cross-platform compatibility
source(file.path(script_dir, "modified.write.tree2.R"))
source(file.path(script_dir, "modify_gene_tree.R"))

# Apply the custom tree-writing function to the ape namespace
assignInNamespace(".write.tree2", .write.tree2, "ape")
options(scipen = 999)

# --- 3. Data Loading ---
gene_trees <- read.tree(gene_trees_path)
df <- read.csv(df_path)
nloci <- length(df[,1])

# --- 4. Main Processing Loop ---
# We loop through each locus and create a UNIQUE control file for it
df2_list <- list()

for (f in 1:nloci) {
  # Define the specific control file for this locus
  locus_control <- file.path(out_dir, paste0("control_", f, ".txt"))

  # Set seed for model parameters
  set.seed(df$modelseed[f])

  # --- Model Block Initialization ---
  if (df$proteinCoding[f] == "TRUE") {
    write("[TYPE] CODON 1", file=locus_control)
    write("[SETTINGS]", file=locus_control, append=T)
    write(paste("\t[randomseed]", df$seed2[f]), file=locus_control, append=T)
  } else {
    write("[TYPE] NUCLEOTIDE 1", file=locus_control)
    write("[SETTINGS]", file=locus_control, append=T)
    write(paste("\t[randomseed]", df$seed2[f]), file=locus_control, append=T)
  }

  # --- Tree Modification Logic (Paralogs/Shifts) ---
  # (Keeping your original logic for modifying the tree structure)
  new_tree <- modify_tree(gene_trees[[f]], df$lambdaPS[f], df$paralog_taxa[f], df$paralog_branch_mod[f])
  new_tree2 <- new_tree
  new_tree2$node.label <- rep("", new_tree2$Nnode)

  # Model shift logic...
  edges0 <- new_tree$edge.length
  names(edges0) <- 1:length(edges0)
  edges1 <- sort(edges0, decreasing = T)
  edges2 <- edges1[which(cumsum(edges1)<sum(edges1)/4)]
  edges3 <- as.numeric(names(edges2))
  nEdges <- rtpois(1, 0.5, -1,length(edges3))
  edges4 <- sample(edges3,nEdges)

  modelnames <- c()
  traversal <- get_tree_traversal_root_to_tips(new_tree2, T)

  for (x in traversal$queue){
    if (Ancestors(new_tree2,x,'parent') == 0) {
      current_model <- paste0("#mRoot")
      modelnames <- c(modelnames, current_model)
      new_tree2$node.label[x-length(new_tree2$tip.label)] <- current_model
    } else {
      current_branch <- which(new_tree2$edge[,2]==x)
      parent_model <- new_tree2$node.label[Ancestors(new_tree2,x,'parent')-length(new_tree2$tip.label)]
      if (current_branch %in% edges4) {
        current_model <- paste0("#m", x)
        modelnames <- c(modelnames, current_model)
      } else {
        if (current_model != parent_model) { current_model <- parent_model }
      }
      if (x <= length(new_tree2$tip.label)) {
        new_tree2$tip.label[x] <- paste0(new_tree2$tip.label[x], current_model)
      } else {
        new_tree2$node.label[x-length(new_tree2$tip.label)] <- current_model
      }
    }
  }

  # --- Write MODEL definitions to the file ---
  clean_modelnames <- gsub('#','', modelnames)

  if (df$proteinCoding[f] == "TRUE") {
    # Codon Model Parameters...
    pInv <- round(runif(1,0,0.25),3); pNeutral <- round(runif(1,0,1-pInv),3)
    for (m in clean_modelnames) {
      basefreqs <- draw.dirichlet(1,61,rep(10,61),1)[1,]
      basefreqs <- c(basefreqs[1:10], 0, 0, basefreqs[11:12], 0, basefreqs[13:61])
      kappa <- round(rlnormTrunc(1,log(4), log(2.5),max=14),3)
      omegaSelect <- round(runif(1,0,3),3)

      write(paste("[MODEL]", m), file=locus_control, append=T)
      write(paste("\t[statefreq]", paste(basefreqs, collapse=" ")), file=locus_control, append=T)
      write(paste("\t[submodel]", paste(kappa, pInv, pNeutral, 0, 1, omegaSelect, collapse=" ")), file=locus_control, append=T)
      write(paste("\t[indelmodel] POW", round(runif(1,1.5,2),3), "10"), file=locus_control, append=T)
      write(paste("\t[indelrate]", round(runif(1,0.001,0.002),5)), file=locus_control, append=T)
    }
    } else {
      # Nucleotide Model Parameters...
      pInv <- round(runif(1,0,0.25),5)
      ngamcat <- sample(c(0,1),1)
      alpha <- if(ngamcat==0) round(rlnormTrunc(1,log(0.3), log(2.5),max=1.4),5) else 0

      for (m in clean_modelnames) {
        modelType <- sample(c("GTR", "SYM", "TVM", "TVMef", "TIM", "TIMef", "K81uf", "K81", "TrN", "TrNef", "HKY", "K80", "F81", "JC"),1)
        paramvector <- get_param_vector(modelType)

        # Determine base frequencies for specific models
        basefreqs <- NA
        if (modelType %in% c("GTR", "TVM", "TIM", "K81uf", "TrN", "HKY", "F81")) {
          basefreqs <- draw.dirichlet(1,4,c(10,10,10,10),1)[1,]
        }

        # --- BUILD THE SUBMODEL STRING ---
        # This part is mandatory for INDELible to understand the modelType
        if (modelType %in% c("GTR", "SYM")) {
          modelstring <- paste(modelType, paste(paramvector[1:5], collapse=" "))
        } else if (modelType %in% c("TVM", "TVMef")) {
          modelstring <- paste(modelType, paste(paramvector[2:5], collapse=" "))
        } else if (modelType %in% c("TIM", "TIMef")) {
          modelstring <- paste(modelType, paste(paramvector[1:3], collapse=" "))
        } else if (modelType %in% c("K81uf", "K81")) {
          modelstring <- paste(modelType, paste(paramvector[2:3], collapse=" "))
        } else if (modelType %in% c("TrN", "TrNef")) {
          modelstring <- paste(modelType, paste(paramvector[c(1,6)], collapse=" "))
        } else if (modelType %in% c("HKY", "K80")) {
          modelstring <- paste(modelType, paramvector[1])
        } else {
          modelstring <- modelType # Covers JC and F81
        }

        # --- WRITE TO CONTROL FILE ---
        write(paste("[MODEL]", m), file=locus_control, append=T)
        write(paste("\t[submodel]", modelstring), file=locus_control, append=T)

        if (!all(is.na(basefreqs))) {
          write(paste("\t[statefreq]", paste(basefreqs, collapse=" ")), file=locus_control, append=T)
        }

        write(paste("\t[rates]", pInv, alpha, ngamcat), file=locus_control, append=T)
        write(paste("\t[indelmodel] POW", paramvector[7], "10"), file=locus_control, append=T)
        write(paste("\t[indelrate]", paramvector[8]), file=locus_control, append=T)
      }
    }

  # --- Write TREE, BRANCHES, and PARTITIONS to the file ---
  write(paste0("[TREE] t1 ", write.tree(new_tree, file="")), file=locus_control, append=T)

  new_tree2$edge.length <- NULL
  write(paste0("[BRANCHES] b1 ", write.tree(new_tree2, file="")), file=locus_control, append=T)

  seq_len <- if(df$proteinCoding[f]=="TRUE") round(df$loclen[f]/3) else df$loclen[f]
  write(paste0("[PARTITIONS] p1 [t1 b1 ", seq_len, "]"), file=locus_control, append=T)

  # --- Final EVOLVE block ---
  write("[EVOLVE] p1 1 output", file=locus_control, append = T)
}

# (Optional: df2.csv logging logic remains here) # logging simulated model params: the original script, was tracking things like modelkappasd and modelratesd.
# Since we are now using a loop, keeping that log would require initialize a data frame before the loop and fill it as you go.
# Not sure yet if we strictly need those specific standard deviation metrics for downstream analysis, so leaving it out for now to keep the script faster.
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano post_INDELible.R
#---------------------------------
library(ape)

# --- 1. Capture Arguments ---
args = commandArgs(trailingOnly=TRUE)
alignment_folder_path <- args[1] # This is $indel_dir from the .sh
df_path <- args[2]
df <- read.csv(df_path)

nloci <- length(df[,1])

# Create output directory for modified fasta files
# We'll use file.path for safety
final_aln_dir <- file.path(alignment_folder_path, "alignments_final")
if (!dir.exists(final_aln_dir)) dir.create(final_aln_dir)

# --- 2. Modification Loop ---
for (f in 1:nloci){
    # Identify the simulated file from INDELible
    # Matches the 'mv' command in our bash script: locus_i.phy
    seqpath <- file.path(alignment_folder_path, paste0("locus_", f, ".phy"))

    if (!file.exists(seqpath)) {
        message(paste("Warning: File missing for Locus", f))
        next
    }

    # Read DNA (Sequential Phylip format)
    locus <- read.dna(seqpath, format="sequential", as.character = T)

    # Filter for remaining taxa
    remaining_taxa <- as.character(eval(parse(text=df$remaining_taxa[f])))
    locus <- locus[rownames(locus) %in% remaining_taxa,]

    # --- Contamination Processing ---
    # Swaps sequence data between pairs to simulate lab contamination
    contaminant_tips <- eval(parse(text=df$cont_pairs[f]))
    lencon <- length(contaminant_tips)
    if (lencon > 0) {
        for (t in seq(2, lencon, 2)) {
            # Taxon t gets the sequence of Taxon t-1
            locus[which(rownames(locus) == contaminant_tips[t]), ] <-
                locus[which(rownames(locus) == contaminant_tips[t-1]), ]
        }
    }

    # --- Missing Data Simulation ---
    # Adds gaps (-) to the 5' or 3' ends based on prop and bias in df.csv
    loclen <- ncol(locus)
    taxa_vector <- as.character(eval(parse(text=df$taxa_missing_segments[f])))
    missing_segments_prop_vector <- eval(parse(text=df$missing_segments_prop[f]))
    missing_segments_bias_vector <- eval(parse(text=df$missing_segments_bias[f]))

    if (length(taxa_vector) > 0) {
        for (t in 1:length(taxa_vector)) {
            taxon_name <- taxa_vector[t]
            taxon_index <- which(rownames(locus) == taxon_name)

            if (length(taxon_index) > 0) {
                gapLen <- loclen * missing_segments_prop_vector[t]
                gapLen5 <- round(gapLen * missing_segments_bias_vector[t])
                gapLen3 <- round(gapLen - gapLen5)

                if (gapLen5 > 0) locus[taxon_index, 1:gapLen5] <- "-"
                if (gapLen3 > 0) locus[taxon_index, (loclen - gapLen3 + 1):loclen] <- "-"
            }
        }
    }

    # --- Cleanup: Remove empty columns ---
    # If all taxa have a gap at a site, delete the site
    badpos <- which(apply(locus, 2, function(col) all(col == "-")))
    if (length(badpos) > 0) {
        locus <- locus[, -badpos]
    }

    # --- Write Result ---
    # Save as FASTA in the new alignments_final folder
    locus_bin <- as.DNAbin(locus)
    write.FASTA(locus_bin, file.path(final_aln_dir, paste0("loc_", f, ".fas")))
}

message("Post-processing complete. Modified alignments are in: ", final_aln_dir)
#---------------------------------

Install INDELibleV1.03:

cd /project/pi_rsschwartz_uri_edu/Biancani/Software/
git clone https://github.com/kloetzl/indelible.git

/indelible.git
Cloning into 'indelible'...
remote: Enumerating objects: 91, done.
remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91 (from 1)
Receiving objects: 100% (91/91), 1.19 MiB | 14.72 MiB/s, done.
Resolving deltas: 100% (27/27), done.

cd /project/pi_rsschwartz_uri_edu/Biancani/Software/indelible/src
g++ -o indelible -O3 -m64 -std=gnu++98 -include unistd.h indelible.cpp -lm

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.3_INDELible.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=indelible_para
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32        # Number of INDELible instances to run at once
#SBATCH --mem=4G
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512
#SBATCH --time=00:20:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"
# Path to INDELible executable
indel_exe="/project/pi_rsschwartz_uri_edu/Biancani/Software/indelible/src/indelible"

# Input from 2.0 and 2.2
tree_file="$Output/2.2_simulated_loci/gene_trees.tre"
df_file="$Output/2.0_simphy_prep/df.csv"

# Output subdirectory:
indel_dir="$Output/2.3_indelible"
mkdir -p "$indel_dir"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
module load parallel/20240822
# Fix for the C++ library (GLIBCXX) errors:
export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
# Point R to your custom package library
export R_LIBS=~/R-packages

# --- 1. Prep Control Files ---
# Ensure your R script creates 'control_1.txt' through 'control_2000.txt' in $indel_dir
echo "Generating 2,000 INDELible control files..."
# Passing: [1]tree_file [2]df_file [3]output_dir [4]sim_script_dir
Rscript "$Scripts/2_simulation_scripts/prep_INDELible.R" \
    "$tree_file" \
    "$df_file" \
    "$indel_dir" \
    "$Scripts/2_simulation_scripts"

# --- 2. Parallel Execution ---
echo "Running INDELible in parallel across $SLURM_CPUS_PER_TASK cores..."
cd "$indel_dir"

# Generate a list of tasks for parallel
# We create a unique temporary folder for each locus to prevent control.txt collisions
for i in {1..2000}; do
    echo "mkdir -p tmp_$i && cp control_$i.txt tmp_$i/control.txt && cd tmp_$i && $indel_exe > /dev/null && mv output_TRUE.phy ../locus_$i.phy && cd .. && rm -rf tmp_$i"
done > indelible_tasks.txt

# Run the tasks
parallel --jobs $SLURM_CPUS_PER_TASK < indelible_tasks.txt

# --- 3. Post-Processing ---
echo "Running post-simulation sequence modification..."
Rscript "$Scripts/2_simulation_scripts/post_INDELible.R" "$indel_dir" "$df_file"

date
#---------------------------------
sbatch 2.3_INDELible.sh
Submitted batch job 50824809
sacct -j 50824809 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50824809          0:0   00:01:58
50824809.ba+      0:0   00:01:58    412352K
50824809.ex+      0:0   00:01:58

Running INDELible in parallel across 32 cores...
Running post-simulation sequence modification...
Post-processing complete. Modified alignments are in: /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final
Mon Dec 29 06:14:50 AM UTC 2025

Checking...
# Count raw Phylip files from INDELible
ls /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/locus_*.phy | wc -l
2000
# Count final FASTA files from Post-Processing
ls /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/*.fas | wc -l
2000
# visually inspect a random file:
head -n 20 /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/loc_500.fas
(looks good)
# Sometimes simulations fail silently for specific loci. This command searches for files that are too small (e.g., under 100 bytes) which might indicate a failed simulation:
find /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/2.3_indelible/alignments_final/ -name "*.fas" -size -100c
#If this returns nothing, all alignments have substantial data.

INDELible summary:
Paralogs: Subtrees were swapped as per df.csv.
Contamination: Taxon sequences were duplicated/overwritten to simulate lab error.
Trimming: Sequence ends were masked with "-" to simulate degraded or poorly sequenced DNA.

Git Commit Message:
feat: parallelize sequence simulation and modernize INDELible workflow

- Refactor 2.3_INDELible.sh to use GNU Parallel with thread-safe temporary directories.
- Update prep_INDELible.R to generate 2,000 individual control files and support dynamic script sourcing.
- Restore Nucleotide Substitution Model switch-case logic to ensure correct INDELible [MODEL] definitions.
- Update post_INDELible.R to handle the new locus naming convention and output processed FASTA alignments.
- Optimize I/O by implementing a centralized alignments_final directory for downstream analysis.

2025.12.29

Step 2.4: Taxon Name Restoration

The final step in the Phase 2 pipeline, Step 2.4, serves as the bridge between the simplified numerical space required by simulation engines and the human-readable requirements of downstream phylogenomic analysis.
Because SimPhy and INDELible are optimized to process taxa as integer-based indices (1, 2, 3, etc.), the biological identity of the sequences is temporarily obscured during the simulation.
Step 2.4 uses the taxon_map.csv—originally captured during the tree-formatting stage (Step 1.1)—to systematically map these integers back to their original empirical species names.
By iterating through the 2,000 final alignments and performing a 1:1 string replacement of the FASTA headers, this script ensures that the "ground truth" labels are restored with integrity, producing a final dataset that is ready for phylogenetic inference tools.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano restore_names.R
#---------------------------------
library(ape)

# --- 1. Capture Arguments ---
args <- commandArgs(trailingOnly=TRUE)
input_dir  <- args[1]
output_dir <- args[2]
map_path   <- args[3]

# --- 2. Load Taxon Map ---
taxon_map <- read.csv(map_path)
# Ensure columns are treated as strings to avoid matching errors
taxon_map$number <- as.character(taxon_map$number)

# --- 3. Renaming Loop ---
fas_files <- list.files(input_dir, pattern = "\\.fas$", full.names = TRUE)
message(paste("Renaming taxa in", length(fas_files), "alignments..."))

for (f in fas_files) {
  # Read alignment (as character matrix for easy renaming)
  aln <- read.dna(f, format = "fasta", as.character = TRUE)

  # Map numerical rownames back to names using the CSV
  # match() finds the row index in taxon_map where 'number' matches the rowname
  new_names <- taxon_map$name[match(rownames(aln), taxon_map$number)]

  # Apply new names
  rownames(aln) <- new_names

  # Save output
  out_name <- file.path(output_dir, basename(f))
  write.FASTA(as.DNAbin(aln), out_name)
}

message("All names restored successfully.")
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/2_simulation_scripts

nano 2.4_restore_names.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=restore_names
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=4G
#SBATCH -p uri-cpu
#SBATCH --time=00:20:00
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"

# Input: final alignments with numbered taxa (generated by 2.3_INDELible.sh)
Input_Aln="$Output/2.3_indelible/alignments_final"
# Output path for final alignments with restored names (will be created if needed)
Final_Aln="$Output/2.4_final_named_alignments"
# Taxon map (generated by 1.1_format_tree.sh)
Taxon_Map="$Output/1.1_formatted_empirical_tree/taxon_map.csv"

mkdir -p "$Final_Aln"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Execute Renaming ---
echo "Restoring empirical taxon names using taxon_map.csv..."
Rscript "$Scripts/2_simulation_scripts/restore_names.R" \
    "$Input_Aln" \
    "$Final_Aln" \
    "$Taxon_Map"

echo "Process complete. Final alignments located in: $Final_Aln"
date
#---------------------------------
sbatch 2.4_restore_names.sh
Submitted batch job 50825498
sacct -j 50825498 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50825498          0:0   00:00:29
50825498.ba+      0:0   00:00:29    210536K
50825498.ex+      0:0   00:00:29

Git Commit Message:
feat: add Step 2.4 taxon name restoration script

- Adds 2.4_restore_names.sh and restore_names.R to the pipeline.
- Implements 1:1 mapping of numerical labels back to empirical species names.
- Uses taxon_map.csv from Step 1.1 to ensure naming integrity.

Phase 3: Filter_by_Known_Clades
Filter all loci for support for Established Clades

Clone the 01_iqtree directory while preserving specific commit history (via git subtree):
# 1. Navigate to root of current repository:
cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
# 2. Add the external repo as a "Remote"
# This tells your local Git where to look for the source code without actually downloading everything yet.
git remote add filter_repo https://github.com/LMBiancani/FilterByKnownClades.git
git fetch filter_repo
# 3. Pull the directory in as a Subtree
# This command "slices" the 01_iqtree folder out of the external history and grafts it into a new folder in your repo.
# --prefix Scripts/3_filter_by_known_clades: This names the new folder in your current repo
# filter_repo main: This specifies the remote name and the branch you are pulling from.
git subtree add --prefix Scripts/3_filter_by_known_clades filter_repo main


The requires a taxon-to-group correspondence table, groups.csv, is a csv file with the following format:
Group,Taxa
group1,taxonName1
group1,taxonName2
group2,taxonName3
group2,taxonName4

copy that file from my PlacentalPolytomy directory:
cp /project/pi_rsschwartz_uri_edu/Biancani/PlacentalPolytomy/01_FilterByTaxa/groups.csv /scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/

less /scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/groups.csv

Group,Taxa
Afrotheria,Amblysomus_hottentotus_longiceps
Afrotheria,Chrysochloris_asiatica
Afrotheria,Echinops_telfairi
Afrotheria,Elephantulus_edwardii
Afrotheria,Hydrodamalis_gigas
Afrotheria,Loxodonta_africana
Afrotheria,Microgale_talazaci
Afrotheria,Orycteropus_afer
Afrotheria,Procavia_capensis
Afrotheria,Trichechus_manatus_latirostris
Boreoeutheria,Ceratotherium_simum
Boreoeutheria,Condylura_cristata
Boreoeutheria,Galeopterus_variegatus
Boreoeutheria,Hippopotamus_amphibius
Boreoeutheria,Manis_javanica
Boreoeutheria,Odobenus_rosmarus_divergens
Boreoeutheria,Pan_troglodytes
Boreoeutheria,Pteropus_vampyrus
Boreoeutheria,Rattus_norvegicus
Boreoeutheria,Tupaia_tana
Metatheria,Didelphis_virginiana
Metatheria,Gymnobelideus_leadbeateri
Metatheria,Phalanger_gymnotis
Metatheria,Phascolarctos_cinereus
Metatheria,Potorous_gilbertii
Metatheria,Pseudochirops_corinnae
Metatheria,Sarcophilus_harrisii
Metatheria,Thylacinus_cynocephalus
Metatheria,Vombatus_ursinus
Metatheria,Wallabia_bicolor
Xenarthra,Chaetophractus_vellerosus
Xenarthra,Choloepus_didactylus
Xenarthra,Dasypus_novemcinctus
Xenarthra,Mylodon_darwinii
Xenarthra,Myrmecophaga_tridactyla
Xenarthra,Tamandua_tetradactyla
Xenarthra,Tolypeutes_matacus

In Step 3.0, we developed a portable framework to generate topological constraints for the mammal simulation dataset.
We created a generic R engine, generate_constraints.R, which utilizes the ape library to ingest a standardized clade definition file (groups.csv) and a directory of alignments.
For each of the 2,000 loci, the script dynamically identifies which taxa are present and prunes the "established clades" accordingly, ensuring that a constraint is only generated if a clade contains at least two representative taxa.
These pruned clades are then nested into a multifurcating Newick string—representing our "known" mammalian backbone—and saved as individual .newick files.
This step is critical for our experimental validation, as it provides the necessary "Skeleton" trees for the subsequent constrained IQ-TREE inferences, ultimately allowing us to filter loci based on their statistical support for biologically certain nodes.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano generate_constraints.R
#---------------------------------
library(ape)

# Capture arguments from shell
args <- commandArgs(trailingOnly = TRUE)
ALN_DIR     <- args[1]
OUTPUT_DIR  <- args[2]
CLADE_FILE  <- args[3]

dir.create(OUTPUT_DIR, recursive = TRUE, showWarnings = FALSE)

# Load Clade Definitions (Group, Taxa)
clade_df <- read.csv(CLADE_FILE, header = TRUE)
all_clades <- split(clade_df$Taxa, clade_df$Group)

# Process all alignments
loci_files <- list.files(ALN_DIR, pattern = "\\.fas$")

for (f in loci_files) {
  locus_path <- file.path(ALN_DIR, f)
  fasta <- read.FASTA(locus_path)
  present_taxa <- names(fasta)

  # Intersect defined clades with what is actually in this locus
  pruned_clades <- lapply(all_clades, function(x) intersect(x, present_taxa))
  valid_clades <- pruned_clades[sapply(pruned_clades, length) > 1]

  if (length(valid_clades) > 0) {
    # Create the constraint string
    clade_strings <- sapply(valid_clades, function(x) paste0("(", paste(x, collapse = ","), ")"))
    final_constraint <- paste0("(", paste(clade_strings, collapse = ","), ");")

    # Save as locus_i_constraint.newick
    out_name <- gsub("\\.fas$", "_constraint.newick", f)
    write(final_constraint, file = file.path(OUTPUT_DIR, out_name))
  }
}
cat(paste("Generated", length(list.files(OUTPUT_DIR)), "constraint files.\n"))
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano 3.0_generate_constraints.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=constraints
#SBATCH --partition=uri-cpu
#SBATCH --time=00:15:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --constraint=avx512
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
# Paths to project directories:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals"

# Input alignment (generated by 2.4_restore_names.sh):
Alignments=$Output/2.4_final_named_alignments

# Input clades:
clades=/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci/groups.csv
# a taxon-to-group correspondence CSV table with the following format:
# Group,Taxa
# group1,taxonName1
# group1,taxonName2
# group2,taxonName3
# group2,taxonName4

# Output subdirectory for the constraint tree:
out_dir="$Output/3.0_constraint_trees"
mkdir -p "$out_dir"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Execute ---
Rscript $Scripts/3_filter_by_known_clades/generate_constraints.R "$Alignments" "$out_dir" "$clades"

# --- Audit & Confirmation ---
echo "------------------------------------------------"
echo "Post-Processing Audit:"
expected_count=$(ls -1 "$Alignments"/*.fas | wc -l)
actual_count=$(ls -1 "$out_dir"/*_constraint.newick 2>/dev/null | wc -l)

echo "Expected constraints: $expected_count"
echo "Generated constraints: $actual_count"

if [ "$expected_count" -eq "$actual_count" ]; then
    echo "SUCCESS: All constraint files were generated correctly."
else
    echo "WARNING: Count mismatch! Some loci may not have enough taxa to form a constraint."
fi
echo "------------------------------------------------"
#---------------------------------
sbatch 3.0_generate_constraints.sh
Submitted batch job 50827749
sacct -j 50827749 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50827749          0:0   00:00:13
50827749.ba+      0:0   00:00:13    195460K
50827749.ex+      0:0   00:00:13

git commit message:
feat: implement automated topological constraint generation for LRT filtering

- Added generate_constraints.R to dynamically build Newick constraints.
- Integrated groups.csv to define established mammalian clades (Afrotheria, etc.).
- Added 3.0_generate_constraints.sh
- Logic automatically prunes constraints for loci with missing taxa.
- Added post-processing audit to verify 1:1 alignment-to-constraint ratio.

3.1

In Step 3.1, we implemented a high-throughput phylogenetic inference pipeline to calculate the log-likelihoods (lnL) required for the Likelihood Ratio Test (LRT).
Using GNU Parallel, we distributed 4,000 independent IQ-TREE 2 runs across 30 CPU cores to optimize computational efficiency on the Unity cluster.
For each of the 2,000 loci, we performed two distinct maximum likelihood searches under a GTR+G substitution model:
an unconstrained search to identify the best-supported topology for that specific locus,
and a constrained search where the topology was forced to adhere to the established mammalian backbone generated in Step 3.0.
By comparing the lnL of these two runs, we can quantify the "likelihood penalty" incurred when a locus is forced to match known biological clades.
To manage the high volume of output data, the script includes automated cleanup logic that removes transient checkpoint and auxiliary files while preserving the critical .treefile and .iqtree summary reports for final statistical filtering.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano 3.1_iqtree_likelihoods.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=iqtree
#SBATCH --partition=uri-cpu
#SBATCH --time=01:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=30
#SBATCH --cpus-per-task=1
#SBATCH --mem=2G
#SBATCH --constraint=avx512
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Alignments="$Project/output/mammals/2.4_final_named_alignments"
Constraints="$Project/output/mammals/3.0_constraint_trees"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"

OutDir="$Project/output/mammals/3.1_likelihoods"
mkdir -p "$OutDir"

# --- Environment Setup ---
module purge
module load uri/main
module load foss/2024a
module load parallel/20240822

# --- Command Factory Logic ---
run_iqtree() {
    locus_fas=$1
    locus_id=$(basename "$locus_fas" .fas)
    out_path=$2
    const_path=$3
    iqtree_exe=$4  # Receive the path variable here

    # 1. Unconstrained Run
    $iqtree_exe -s "$locus_fas" -m GTR+G -pre "$out_path/${locus_id}_uncon" -nt 1 -redo

    # 2. Constrained Run
    $iqtree_exe -s "$locus_fas" -m GTR+G -g "$const_path/${locus_id}_constraint.newick" -pre "$out_path/${locus_id}_con" -nt 1 -redo

    # Cleanup auxiliary files
    rm -f "$out_path/${locus_id}_"*".ckp.gz" "$out_path/${locus_id}_"*".bionj" "$out_path/${locus_id}_"*".mldist"
}

export -f run_iqtree

# --- Execute with GNU Parallel ---
# We pass $IQTREE as the 4th argument to the function
ls "$Alignments"/*.fas | parallel -j 30 --progress run_iqtree {} "$OutDir" "$Constraints" "$IQTREE"

echo "IQ-TREE inferences complete. Likelihoods saved in $OutDir"
#---------------------------------
Submitted batch job 50827835
sacct -j 50827835 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50827835          0:0   00:10:12
50827835.ba+      0:0   00:10:12    436328K
50827835.ex+      0:0   00:10:12

Git commit message:
feat: implement parallel IQ-TREE inference for LRT filte h
- Added 3.1_iqtree_likelihoods.sh to perform dual tree inference per locus.
- Utilizes GNU Parallel to distribute 4,000 IQ-TREE runs across 30 cores.
- Compares unconstrained ML searches vs. backbone-constrained searches.
- Standardized model selection to GTR+G for cross-locus likelihood comparisons.
- Integrated automated cleanup of auxiliary files to optimize storage.

In Step 3.2, we implemented a Likelihood Ratio Test (LRT) post-processing pipeline to statistically evaluate the phylogenetic signal of each locus against our established mammalian backbone.
We developed a custom R script, LRT_filter.R, which parses the log-likelihood (lnL) scores from the 4,000 IQ-TREE output files generated in the previous step.
By calculating the "likelihood penalty"—defined as the difference between the unconstrained ML tree and the constrained backbone tree (ΔlnL)—the script quantifies the degree of topological conflict present in each alignment.
Using a χ2 distribution with one degree of freedom, we assigned p-values to each locus, identifying those that significantly deviate from known mammalian relationships.
This step is the "quality control" heart of the workflow, providing a quantitative basis for filtering out simulated noise and high-conflict loci before proceeding to final species tree estimation.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano LRT_filter.R
#---------------------------------
#!/usr/bin/env Rscript

# --- Load Libraries ---
library(stringr)

# --- Argument Handling ---
args <- commandArgs(trailingOnly = TRUE)
if (length(args) < 2) {
  stop("Usage: Rscript 3.2_LRT_filter.R <IQ_DIR> <OUT_CSV>")
}

IQ_DIR  <- args[1]
OUT_CSV <- args[2]

# Get list of all unconstrained iqtree files
uncon_files <- list.files(IQ_DIR, pattern = "_uncon.iqtree", full.names = TRUE)

cat("Found", length(uncon_files), "loci to process...\n")

# --- Extraction Function ---
get_lnl <- function(filepath) {
  if(!file.exists(filepath)) return(NA)
  lines <- tryCatch(readLines(filepath), error = function(e) return(NULL))
  if(is.null(lines)) return(NA)

  # Search for the Likelihood line
  lnl_line <- lines[grep("Log-likelihood of the tree:", lines)]
  if(length(lnl_line) == 0) return(NA)

  # Extract numeric value using regex (matches number before the space/bracket)
  val <- str_extract(lnl_line, "-?[0-9.]+(?= \\()")
  return(as.numeric(val))
}

# --- Build Results Table ---
results <- data.frame(
  Locus = gsub("_uncon.iqtree", "", basename(uncon_files)),
  lnL_Uncon = sapply(uncon_files, get_lnl),
  stringsAsFactors = FALSE
)

cat("Extracting constrained likelihoods...\n")
results$lnL_Con <- sapply(results$Locus, function(x) {
  get_lnl(file.path(IQ_DIR, paste0(x, "_con.iqtree")))
})

# --- Statistics ---
# Calculate Delta lnL (Likelihood Penalty)
results$delta_lnL <- results$lnL_Uncon - results$lnL_Con

# Calculate LRT p-value (df=1 for a simple topological constraint)
results$LRT_stat <- 2 * results$delta_lnL
results$p_value <- pchisq(results$LRT_stat, df = 1, lower.tail = FALSE)

# Sort by delta_lnL (highest penalty at the top)
results <- results[order(results$delta_lnL, decreasing = TRUE), ]

# --- Save Output ---
write.csv(results, OUT_CSV, row.names = FALSE)

cat("\n--- Summary ---\n")
cat("Total loci processed:", nrow(results), "\n")
cat("Loci failing backbone (p < 0.05):", sum(results$p_value < 0.05, na.rm=TRUE), "\n")
cat("Results saved to:", OUT_CSV, "\n")
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano 3.2_LRT_filter.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=3.2_LRT_filter
#SBATCH --partition=uri-cpu
#SBATCH --time=00:10:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=1G
#SBATCH --constraint=avx512
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts/3_filter_by_known_clades"
Output="$Project/output/mammals"

# input (generate by 3.1_iqtree_likelihoods.sh)
IQ_DIR="$Output/3.1_likelihoods"

OutDir="$Output/3.2_lrt_results"
mkdir -p "$OutDir"
OUT_FILE="$OutDir/lrt_results.csv"

# --- Environment Setup (Proven Unity Config) ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Execute ---
Rscript $Scripts/LRT_filter.R "$IQ_DIR" "$OUT_FILE"
#---------------------------------
sbatch 3.2_LRT_filter.sh
Submitted batch job 50834337
sacct -j 50834337 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50834337          0:0   00:00:18
50834337.ba+      0:0   00:00:18    199604K
50834337.ex+      0:0   00:00:18

--- Summary ---
Total loci processed: 2000
Loci failing backbone (p < 0.05): 1367
Results saved to: /scratch4/workspace/biancani_uri_edu-LociSimulation/output/mammals/3.2_lrt_results/lrt_results.csv

1,367 out of 2,000 loci (~68%) failed the backbone constraint at p<0.05
this confirms that the "noise" or mutations we introduced in Step 2.3 successfully created significant phylogenetic conflict.

Git commit message:
feat: implement LRT statistical filtering and likelihood extraction

- Added LRT_filter.R to parse log-likelihoods from IQ-TREE output files.
- Implemented Delta lnL calculation to quantify topological conflict.
- Applied Chi-square test (df=1) to assign p-values for backbone adherence.
- Created 3.2_LRT_filter.sh wrapper.
- Generated ranked results CSV for downstream locus selection and QC.

In Step 3.3, we finalized the locus-filtering pipeline by implementing a dual-collection assembly strategy that categorized all 2,000 candidate loci based on their statistical adherence to the mammalian backbone.
Using a custom R script, collect_filtered_loci.R, we applied a significance threshold of P≥0.05 to bin the alignments into two distinct subsets:
a "pass" group containing 633 loci with high-fidelity evolutionary signal and a "fail" group containing 1,367 loci where simulated noise significantly obscured the biological relationships.
By automating the migration of these files into structured subdirectories (pass_0.05 and fail_0.05), we secured a curated "Golden Set" for high-confidence species tree inference while simultaneously preserving the conflicting data as a negative control for future sensitivity analyses.
This bifurcation ensures that the final phylogenomic results are derived from statistically validated signal rather than stochastic simulation artifacts.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano collect_filtered_loci.R
#---------------------------------
#!/usr/bin/env Rscript

# --- Setup ---
args <- commandArgs(trailingOnly = TRUE)
LRT_CSV    <- args[1]
IN_DIR     <- args[2]
OUT_BASE   <- args[3] # Base output path
P_THRESH   <- as.numeric(args[4])

# Load results
df <- read.csv(LRT_CSV)

# Define paths for both groups using paste0 to keep the name clean
PASS_DIR <- file.path(OUT_BASE, paste0("pass_", P_THRESH))
FAIL_DIR <- file.path(OUT_BASE, paste0("fail_", P_THRESH))

# Create directories
if(!dir.exists(PASS_DIR)) dir.create(PASS_DIR, recursive = TRUE)
if(!dir.exists(FAIL_DIR)) dir.create(FAIL_DIR, recursive = TRUE)

# Categorize loci
passing_loci <- df$Locus[df$p_value >= P_THRESH]
failing_loci <- df$Locus[df$p_value <  P_THRESH]

cat("Threshold:", P_THRESH, "\n")
cat("Loci to copy to PASS:", length(passing_loci), "\n")
cat("Loci to copy to FAIL:", length(failing_loci), "\n")

# Function to copy files
move_files <- function(locus_list, destination) {
  for(locus in locus_list) {
    file_name <- paste0(locus, ".fas")
    file.copy(from = file.path(IN_DIR, file_name),
              to   = destination)
  }
}

# Execute copies
move_files(passing_loci, PASS_DIR)
move_files(failing_loci, FAIL_DIR)

cat("Assembly complete.\n")
cat("  Pass folder:", PASS_DIR, "\n")
cat(length(passing_loci), " loci passed.\n")
cat("  Fail folder:", FAIL_DIR, "\n")
cat(length(failing_loci), " loci failed.\n")
#---------------------------------

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/3_filter_by_known_clades

nano 3.3_collect_filtered_loci.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=filter_loci
#SBATCH --partition=uri-cpu
#SBATCH --time=00:10:00
#SBATCH --mem=1G

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts/3_filter_by_known_clades"
Output="$Project/output/mammals"

# Inputs
# LRT csv generated by 3.2_LRT_filter.sh:
LRT_CSV="$Output/3.2_lrt_results/lrt_results.csv"
# Path to aligned loci generated by 2.4_restore_names.sh:
Alignments="$Output/2.4_final_named_alignments"
# p threshold:
Pvalue=0.05

# Output
FinalDir="$Output/3.3_loci_filtered_by_known_clades"
mkdir -p "$FinalDir"

# --- Environment ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH

# --- Execute ---
# Arguments: <CSV> <SourceDir> <DestDir> <P-Threshold>
Rscript $Scripts/collect_filtered_loci.R "$LRT_CSV" "$Alignments" "$FinalDir" "$Pvalue"
#---------------------------------
sbatch 3.3_collect_filtered_loci.sh
Submitted batch job 50835403
sacct -j 50835403 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50835403          0:0   00:00:24
50835403.ba+      0:0   00:00:24    261340K
50835403.ex+      0:0   00:00:24

git commit message:
feat: implement dual-category locus collection for pass/fail subsets

- Added collect_filtered_loci.R to sort loci based on LRT p-value.
- Implemented automatic subdirectory creation for 'pass_0.05' and 'fail_0.05'.
- Isolated high-signal loci for downstream species tree inference.
- Preserved conflicting loci as a negative control for sensitivity analysis.
- Optimized 3.3_collect_filtered_loci.sh with finalized directory paths and thresholds.

2025.12.30

Starting Module 4: branch length correlation

step 4.0:

Purpose: To construct a comprehensive Maximum Likelihood "unfiltered" reference species tree using the total pool of simulated data.
This tree serves as the topological and branch-length baseline for the Module 4 Branch Length Correlation filter.
Methods: All 2,000 loci from Step 2.4 (containing 37 taxa each) were concatenated into a single phylogenomic super-matrix using a batch-processing Python wrapper for AMAS.
The resulting matrix (2,205,174 bp) was analyzed in IQ-TREE 2.1.2 using an unpartitioned model.
Model selection was performed via ModelFinder Plus (-m MFP), and branch support was assessed using 1,000 replicates of both UltraFast Bootstraps (-bb) and SH-aLRT.

Git commit message:
feat: build 2,000-locus unfiltered reference tree (Step 4.0)
- Edited AMAS Python wrapper (from step 0.0) to handle .fas/.fasta extensions.
- Concatenated all Step 2.4 simulated loci into a single 2.2M bp super-matrix.
- Configured IQ-TREE 2.1.2 ML search with MFP and 1000 UFB/SH-aLRT replicates.
- Established baseline branch lengths for downstream correlation filtering.

Following an initial timeout during the reference tree construction, the Step 4.0 pipeline was optimized for computational efficiency by streamlining the substitution model selection process.
The original approach utilized ModelFinder Plus (-m MFP), which incurred a significant time bottleneck during the exhaustive evaluation of site patterns across the 2.2 million base-pair super-matrix.
To resolve this, the search was constrained to a fixed GTR+G model, which provides a biologically robust approximation for mammalian phylogenomic data while bypassing the hours of sequential model-testing overhead.
The Maximum Likelihood search was executed using the full stochastic NNI heuristic (removing the -fast flag from previous test iterations) to ensure high-accuracy branch length estimation, which is critical for the subsequent branch length correlation filter.

Output from Modelfinder Run:
slurm-50841624.out
NOTE: ModelFinder requires 20635 MB RAM!
ModelFinder will test up to 286 DNA models (sample size: 2205174) ...
 No. Model         -LnL         df  AIC          AICc         BIC
  1  GTR+F         19949687.620 79  39899533.239 39899533.245 39900529.138
  2  GTR+F+I       19230163.998 80  38460487.996 38460488.002 38461496.501
  3  GTR+F+G4      18961886.006 80  37923932.013 37923932.019 37924940.518
  4  GTR+F+I+G4    18948335.995 81  37896833.990 37896833.996 37897855.102
  5  GTR+F+R2      19010927.020 81  38022016.040 38022016.046 38023037.152
  6  GTR+F+R3      18906929.184 83  37814024.367 37814024.373 37815070.691
  7  GTR+F+R4      18893786.020 85  37787742.040 37787742.047 37788813.577
  8  GTR+F+R5      18893109.065 87  37786392.130 37786392.137 37787488.879
  9  GTR+F+R6      18892860.508 89  37785899.016 37785899.023 37787020.978
 10  GTR+F+R7      18892805.085 91  37785792.171 37785792.178 37786939.345
 11  GTR+F+R8      18892742.700 93  37785671.400 37785671.408 37786843.787
 12  GTR+F+R9      18892736.159 95  37785662.318 37785662.326 37786859.918
 24  SYM+R8        18892929.411 90  37786038.822 37786038.829 37787173.390
 37  TVM+F+R8      18896564.556 92  37793313.112 37793313.120 37794472.893
 50  TVMe+R8       18898147.430 89  37796472.861 37796472.868 37797594.823
 63  TIM3+F+R8     18904950.986 91  37810083.972 37810083.980 37811231.147
 76  TIM3e+R8      18905501.614 88  37811179.228 37811179.235 37812288.584
 89  TIM2+F+R8     18906998.831 91  37814179.663 37814179.670 37815326.837
102  TIM2e+R8      18906449.437 88  37813074.873 37813074.880 37814184.229
115  TIM+F+R8      18896153.312 91  37792488.623 37792488.631 37793635.798
128  TIMe+R8       18895930.437 88  37792036.873 37792036.880 37793146.229
141  TPM3u+F+R8    18908812.459 90  37817804.918 37817804.926 37818939.487
154  TPM3+F+R8     18908812.460 90  37817804.919 37817804.927 37818939.488
167  TPM2u+F+R8    18910863.413 90  37821906.825 37821906.833 37823041.394
180  TPM2+F+R8     18910863.411 90  37821906.821 37821906.829 37823041.390
193  K3Pu+F+R8     18900011.031 90  37800202.062 37800202.069 37801336.630
206  K3P+R8        18901167.009 87  37802508.017 37802508.024 37803604.767
219  TN+F+R8       18907863.912 90  37815907.824 37815907.831 37817042.392
232  TNe+R8        18907628.523 87  37815431.046 37815431.053 37816527.795
245  HKY+F+R8      18911720.173 89  37823618.346 37823618.353 37824740.308
258  K2P+R8        18912862.390 86  37825896.781 37825896.788 37826980.924
271  F81+F+R8      19042882.482 88  38085940.963 38085940.970 38087050.319
284  JC+R8         19044319.216 85  38088808.431 38088808.438 38089879.968
Akaike Information Criterion:           GTR+F+R9
Corrected Akaike Information Criterion: GTR+F+R9
Bayesian Information Criterion:         GTR+F+R8
Best-fit model: GTR+F+R8 chosen according to BIC

All model information printed to 2000_loci_ref_tree.model.gz
CPU time for ModelFinder: 1019454.914 seconds (283h:10m:54s)
Wall-clock time for ModelFinder: 79132.357 seconds (21h:58m:52s)

Tree finding timed out, so running again but without model finding again:

What the ModelFinder results tell us:
The Substitution Matrix: GTR (General Time Reversible) is indeed the winner across all criteria (AIC, BIC, etc.). This confirms our choice of GTR in your current script was correct.
State Frequencies: It chose +F (Empirical frequencies). IQ-TREE 2 uses empirical frequencies by default when you specify GTR, so our -m GTR+G is already capturing the core of this.
Rate Heterogeneity (The "Big" Difference): It chose +R8 (FreeRate model with 8 categories).
Our current script uses +G (Gamma distribution).
+R models are often a better fit for large phylogenomic datasets because they don't assume a symmetric bell-curve (Gamma) for site rates; they are more flexible.
To be as precise as possible and "honor" the ModelFinder results without the time penalty, you could change your command to: $IQTREE -s concatenated.fasta -m GTR+F+R8 -nt ${SLURM_CPUS_PER_TASK} -pre 2000_loci_ref_tree

perf: optimize Step 4.0 reference tree via fixed GTR+G model
- Updated 4.0_concat_tree.sh to bypass ModelFinder Plus bottleneck.
- Implemented fixed -m GTR+G to reduce initial overhead on the 2.2M bp matrix.
- Restored standard ML search intensity (removed -fast) for accurate branch lengths.
- Retained 20-thread parallelization for the final ML tree search.

The reference tree construction was further refined to incorporate the optimal substitution parameters identified by the initial ModelFinder run.
Although the original automated search timed out, the log files revealed that a GTR+F+R8 model (General Time Reversible with Empirical State Frequencies and a FreeRate model with 8 categories) provided the best fit according to the Bayesian Information Criterion (BIC).
By hard-coding these parameters, the pipeline now circumvents the ~22-hour wall-clock bottleneck of the ModelFinder phase while maintaining maximum statistical accuracy for branch length estimation.
A 1,000-replicate UltraFast Bootstrap was retained to provide support values for the baseline topology.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/4_filter_by_branch_length/

nano 4.0_concat_tree.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name="unfiltered_tree"
#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=20
#SBATCH --mem=62G
#SBATCH -p uri-cpu
#SBATCH --constraint=avx512
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts"
Output="$Project/output/mammals/"
# Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"
# Path to run_amas.py (from step 0.0)
amas_py="$Scripts/0_data_prep/run_amas.py"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"

# Input
Data="$Output/2.4_final_named_alignments"

# Output directory
OutDir="$Output/4.0_unfiltered_concat_tree"
mkdir -p ${OutDir}
cd ${OutDir}

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

# 1. Run the universal AMAS script
python3 $amas_py ${Data} ${SLURM_CPUS_PER_TASK} ${AMAS}

# 2. Run IQ-TREE on the result
$IQTREE -s concatenated.fasta \
    -m MFP \
    -nt ${SLURM_CPUS_PER_TASK} \
    -pre 2000_loci_ref_tree

# 2. (Alt) Run IQ-TREE on the result (with model specified)
#$IQTREE -s concatenated.fasta \
#    -m GTR+F+R8 \
#    -nt ${SLURM_CPUS_PER_TASK} \
#    -pre 2000_loci_ref_tree
#---------------------------------
sbatch 4.0_concat_tree.sh
Submitted batch job 50867451
sacct -j 50867451 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50867451        127:0   03:27:18
50867451.ba+    127:0   03:27:18  15444080K
50867451.ex+      0:0   03:27:18
(ExitCode 127 due to error in bootstrap line. bootstrap analysis subsequently removed so ouptputs are fine)

Output:
2000_loci_ref_tree.bionj   2000_loci_ref_tree.mldist    concatenated.fasta.model.gz
2000_loci_ref_tree.ckp.gz  2000_loci_ref_tree.treefile  partitions.txt
2000_loci_ref_tree.iqtree  concatenated.fasta
2000_loci_ref_tree.log     concatenated.fasta.log


feat: adjusted Step 4.0 reference tree with ModelFinder results
- Updated 4.0_concat_tree.sh to use the optimized GTR+F+R8 model.
- Bypassed ~280 CPU-hours of model testing by hard-coding BIC-best parameters.
- Optimized walltime and resource allocation for the 2.2M bp super-matrix.

refactor: remove bootstrap calculation from Step 4.0 reference tree
- Streamlined IQ-TREE command to focus exclusively on ML branch length estimation.
- Removed -bb 1000 flag to significantly reduce wall-clock time on 2.2M bp matrix.
- Confirmed baseline topology and branch lengths are sufficient for downstream
  Branch Length Correlation (BLC) filtering without support values.

2026.01.01

BLC step 4.1:

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/4_filter_by_branch_length/

trimConstraintTree.R: Prunes the 37-taxa reference tree to match the taxa actually present in each locus.
4.1_constrained_gtree.sh: The Slurm wrapper to run IQ-TREE for each locus.

4.1
The R Script: trimConstraintTree.R
This script ensures that if a locus is missing a taxon
(e.g., if a simulation step removed a species),
the reference tree is "pruned" to match that locus before IQ-TREE tries to use it as a constraint.

Step 4.1 Project Notes
Purpose: To perform high-throughput, constrained branch length estimation across 2,000 simulated loci.
By forcing each gene tree to match the "unfiltered" genomic reference topology (Step 4.0),
we isolate rate variation (branch lengths) as the primary variable for downstream correlation filtering.
Methods: * Dynamic Pruning: An R script (trimConstraintTree.R) utilized the ape library to programmatically prune the 37-taxon reference tree to match the specific taxon occupancy of each individual locus alignment.
Model Synchronization: To ensure mathematical consistency, the substitution model (GTR+F+R8) was dynamically extracted from the Step 4.0 .iqtree report and passed to all 2,000 IQ-TREE 2.1.2 runs.
Parallel Execution: Distributed the workload across 64 cores using GNU Parallel. A staggered start (sleep) was implemented within the command factory to prevent resource contention during R library loading.
Performance: The optimized pipeline completed 2,000 Maximum Likelihood inferences in 4 minutes and 19 seconds, demonstrating high scaling efficiency on the Unity cluster.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/4_filter_by_branch_length/

nano trimConstraintTree.R
#---------------------------------
#!/usr/bin/env Rscript
# trimConstraintTree.R
# Purpose: Prune a reference species tree to match the taxa present in a specific locus alignment.

library(ape)

# Capture command line arguments
args <- commandArgs(trailingOnly = TRUE)
if (length(args) < 3) {
  stop("Usage: Rscript trimConstraintTree.R <alignment.fas> <reference.treefile> <output.tre>")
}

# 1. Load the alignment to see which taxa are present
alignment <- read.dna(args[1], format="fasta")
taxapresent <- rownames(alignment)

# 2. Load the Reference Species Tree (from Step 4.0)
tree <- read.tree(args[2])
treetips <- tree$tip.label

# 3. Identify tips in the tree that are NOT in the alignment
tipstoremove <- treetips[!(treetips %in% taxapresent)]

# 4. Prune the tree and write to file
# drop.tip removes the missing taxa while maintaining the relative branch lengths of the rest
outtree <- drop.tip(tree, tipstoremove)
write.tree(outtree, args[3])

cat("Successfully pruned tree to", length(outtree$tip.label), "taxa.\n")
#---------------------------------

4.1_constrained_gtrees.sh
This script uses a Command Factory function to handle the R-pruning and the IQ-TREE inference in one go, distributed across your CPU cores.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/4_filter_by_branch_length/

nano 4.1_constrained_gtrees.sh
#---------------------------------
#!/bin/bash
#SBATCH --job-name=BLC_gtrees
#SBATCH --partition=uri-cpu
#SBATCH --time=00:30:00
#SBATCH --nodes=1
#SBATCH --ntasks=32
#SBATCH --cpus-per-task=1
#SBATCH --mem=8G
#SBATCH --constraint=avx512
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL

# --- Variables ---
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
Scripts="$Project/LociSimulation/Scripts/"
Scripts4="$Scripts/4_filter_by_branch_length"
Output="$Project/output/mammals/"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"

# Inputs
Alignments="$Output/2.4_final_named_alignments"
# The Reference Tree from Step 4.0:
RefTree="$Output/4.0_unfiltered_concat_tree/2000_loci_ref_tree.treefile"

# Output
OutDir="$Output/4.1_constrained_gtrees"
mkdir -p "$OutDir"

# --- Extract the Model Name from 4.0 output ---
MODEL_FILE="$Output/4.0_unfiltered_concat_tree/2000_loci_ref_tree.iqtree"
GEN_MODEL=$(grep "Model of substitution:" "$MODEL_FILE" | cut -d ":" -f 2 | tr -d '[:space:]')

# Verify we actually got a model string
if [ -z "$GEN_MODEL" ]; then echo "Error: Model extraction failed"; exit 1; fi

# --- Environment ---
module purge
module load uri/main
module load foss/2024a
module load R/4.3.2-gfbf-2023a
module load parallel/20240822

export GLIBCXX_PATH="/modules/uri_apps/software/GCCcore/13.3.0/lib64"
export LD_LIBRARY_PATH=$GLIBCXX_PATH:$LD_LIBRARY_PATH
export R_LIBS=~/R-packages

# --- Command Factory Logic ---
run_constrained_inference() {
    locus_fas=$1
    locus_id=$(basename "$locus_fas" .fas)
    out_path=$2
    ref_tree=$3
    iqtree_exe=$4
    script_dir=$5
    model_name=$6

    # Optional: add this right before the Rscript line to stagger starts (preventing library loading issues)
    sleep $(( (RANDOM % 5) + 1 ))

    # 1. Prune the reference tree to match this locus
    Rscript "$script_dir/trimConstraintTree.R" "$locus_fas" "$ref_tree" "$out_path/${locus_id}.constraint.tre"

    # 2. Run IQ-TREE constrained to that pruned topology
    $iqtree_exe -s "$locus_fas" -m "$model_name" -g "$out_path/${locus_id}.constraint.tre" -pre "$out_path/${locus_id}" -nt 1 -redo

    # Cleanup auxiliary files
    rm -f "$out_path/${locus_id}."{ckp.gz,bionj,mldist,log}
}

export -f run_constrained_inference

# --- Execute with GNU Parallel ---
echo "Starting constrained branch length estimation..."
# Added --no-notice to silence the tty warning
find "$Alignments" -name "*.fas" | parallel -j 32 --no-notice --progress run_constrained_inference {} "$OutDir" "$RefTree" "$IQTREE" "$Scripts4" "$GEN_MODEL"
echo "Step 4.1 complete. Constrained trees are in $OutDir"
#---------------------------------
sbatch 4.1_constrained_gtrees.sh
Submitted batch job 50868336
sacct -j 50868336 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
50868336          0:0   00:04:19
50868336.ba+      0:0   00:04:19   2057536K
50868336.ex+      0:0   00:04:19

feat: implement high-throughput constrained branch length estimation (Step 4.1)
- Automated per-locus reference tree pruning via trimConstraintTree.R.
- Implemented dynamic model extraction from Step 4.0 logs to ensure model consistency.
- Scaled inference to 32 cores using GNU Parallel for a shorter execution time.
- Integrated staggered start logic to manage concurrent R namespace loading.
- Standardized output structure for subsequent Branch Length Correlation (BLC) analysis.

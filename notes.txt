NOTES - LociSimulation
github: https://github.com/LMBiancani/LociSimulation.git
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"

2025.10.09

ws_allocate -m biancani@uri.edu -r 5 -G pi_rsschwartz_uri_edu LociSimulation 30

Info: creating workspace.
/scratch4/workspace/biancani_uri_edu-LociSimulation
remaining extensions  : 5
remaining time in days: 30

Starting with mammal loci from https://github.com/LMBiancani/PlacentalPolytomy
Original Path to aligned loci:
"/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"

2025.10.09

cd /scratch4/workspace/biancani_uri_edu-LociSimulation

copy mammal loci to scratch space:

transfer.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="tar_transfer"
#SBATCH --time=48:00:00  # walltime limit (HH:MM:SS)
#SBATCH --mail-user="biancani@uri.edu"
#SBATCH --mail-type=ALL
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH -p uri-cpu
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=6G

# Define source and destination directories
SOURCE_DIR="/data/schwartzlab/Biancani/PlacentalPolytomy/output/01_SISRS_loci_filtered"
DEST_DIR="/scratch4/workspace/biancani_uri_edu-LociSimulation/mammal_loci"

# Make sure the destination exists
mkdir -p "$DEST_DIR"
echo "Started at $(date)"

# Stream the directory (preserving its name) to scratch
tar -cf - -C "$(dirname "$SOURCE_DIR")" "$(basename "$SOURCE_DIR")" | pv | tar -xf - -C "$DEST_DIR"

echo "Finished at $(date)"
---------------------------------
sbatch transfer.sh
Submitted batch job 45655769

sacct -j 45655769 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
45655769          0:0   00:01:48
45655769.ba+      0:0   00:01:48   2114772K
45655769.ex+      0:0   00:01:48          0

mkdir /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation

github: https://github.com/LMBiancani/LociSimulation.git

2025.10.20

Use git subtree to copy Molly's updated simulation/machine learning scripts (and preserve commit history)
Molly's repo: https://github.com/mollyodonnellan/PML.git

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation
git subtree add --prefix=Scripts https://github.com/mollyodonnellan/PML.git main

Mammal data is too large for AMAS so script needs updating to run through loci in batches.

run_amas.py is a custom batch concatenation wrapper around AMAS.
It processes FASTA alignments in chunks of 1000 files at a time (to avoid overloading AMAS input limitations), concatenates them, and appends the results into cumulative files.
Outputs:
concatenatedTrain.fasta
partitionsTrain.txt

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano run_amas.py
---------------------------------
#!/usr/bin/env python3
"""
run_amas.py
------------
Concatenates FASTA alignments in batches using AMAS (to avoid overloading AMAS input limitations).
Creates a concatenated alignment file and a corresponding partition file:
concatenatedTrain.fasta
partitionsTrain.txt

Usage:
    python run_amas.py <fasta_folder> <num_cores> <path_to_AMAS.py>
"""

import sys
import glob
import subprocess
import os

# --- Input arguments ---
fasta_folder = sys.argv[1]
total_cores = sys.argv[2]
amas = sys.argv[3]

# --- Collect all fasta files ---
files = glob.glob(os.path.join(fasta_folder, "*.fasta"))
files.sort()  # ensure consistent, reproducible order

batch_size = 1000
batch_outputs = []
batch_parts = []

count = 0
fileList = []
batch_num = 1

# --- Process loci in batches ---
for f in files:
    fileList.append(f)
    count += 1

    if count == batch_size:
        batch_fasta = f"amas_batch_{batch_num}.fasta"
        batch_part = f"partitions_batch_{batch_num}.txt"

        cmd = (
            f"python3 {amas} concat "
            f"-f fasta -d dna --out-format fasta --part-format raxml "
            f"-i {' '.join(fileList)} "
            f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
        )
        print(f"\n=== Running AMAS on batch {batch_num} ({len(fileList)} files) ===")
        subprocess.call(cmd, shell=True)

        batch_outputs.append(batch_fasta)
        batch_parts.append(batch_part)
        fileList = []
        count = 0
        batch_num += 1

# --- Final partial batch (if any) ---
if len(fileList) > 0:
    batch_fasta = f"amas_batch_{batch_num}.fasta"
    batch_part = f"partitions_batch_{batch_num}.txt"

    cmd = (
        f"python3 {amas} concat "
        f"-f fasta -d dna --out-format fasta --part-format raxml "
        f"-i {' '.join(fileList)} "
        f"-c {total_cores} -t {batch_fasta} -p {batch_part}"
    )
    print(f"\n=== Running AMAS on final batch {batch_num} ({len(fileList)} files) ===")
    subprocess.call(cmd, shell=True)

    batch_outputs.append(batch_fasta)
    batch_parts.append(batch_part)

# --- Final concatenation across all batches ---
print("\n=== Performing final concatenation across batches ===")

cmd_final = (
    f"python3 {amas} concat "
    f"-f fasta -d dna --out-format fasta --part-format raxml "
    f"-i {' '.join(batch_outputs)} "
    f"-c {total_cores} -t concatenated.fasta -p partitions.txt"
)
subprocess.call(cmd_final, shell=True)

# --- Cleanup temporary batch files ---
print("\n=== Cleaning up intermediate files ===")
for f in batch_outputs + batch_parts:
    try:
        os.remove(f)
    except OSError:
        print(f"Warning: could not remove {f}")

print("\n=== run_amas.py execution completed. ===")
print("Output files:")
print(" - concatenated.fasta")
print(" - partitions.txt\n")
---------------------------------

1_run_amas.sh runs AMAS on an empirical dataset to concatenate input fasta files and prepare partitions ahead of IQTree run.
Uses a helper Python script (run_amas.py), which wraps around the AMAS.py concat command.

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

nano 1_run_amas.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="AMAS"
#SBATCH --time=2:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1   # number of nodes
#SBATCH --ntasks-per-node=1   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=10G
#SBATCH -p uri-cpu
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts/0_data_prep"
# Path to output directory (will be created if necessary)
Output="$Project/output/mammals"
# Path to aligned loci in fasta format:
Data="$Project/mammal_loci/01_SISRS_loci_filtered"
# Path to AMAS executable:
AMAS="/project/pi_rsschwartz_uri_edu/Biancani/Software/AMAS/amas/AMAS.py"
# Number of processor cores per node:
Cores=$(echo $SLURM_TASKS_PER_NODE | sed 's/(x.*)//')

module purge
module load uri/main Python/3.7.4-GCCcore-8.3.0

date
mkdir -p ${Output}
cd ${Output}

#Concatenate input fasta files and prepare partitions ahead of IQTree run
python3 ${Scripts}/run_amas.py ${Data} ${Cores} ${AMAS}

date
---------------------------------
sbatch 1_run_amas.sh
Submitted batch job 47034650
sacct -j 47034650 -o JobID,ExitCode,Elapsed,MaxRSS
JobID        ExitCode    Elapsed     MaxRSS
------------ -------- ---------- ----------
47034650          0:0   00:01:56
47034650.ba+      0:0   00:01:56   6601112K
47034650.ex+      0:0   00:01:56          0

cd /scratch4/workspace/biancani_uri_edu-LociSimulation/LociSimulation/Scripts/0_data_prep

2_run_iqtree.sh will use iqtree to infer an empirical tree.

nano 2_run_iqtree.sh
---------------------------------
#!/bin/bash
#SBATCH --job-name="IQTREE"
#SBATCH --time=96:00:00  # walltime limit (HH:MM:SS)
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=24   # processor core(s) per node
#SBATCH --cpus-per-task=1
#SBATCH --mem=250G
#SBATCH -p uri-cpu
#SBATCH --exclusive
#SBATCH --mail-user="biancani@uri.edu" #CHANGE THIS to your user email address
#SBATCH --mail-type=ALL

# --- Variables ---
# Path to project directory:
Project="/scratch4/workspace/biancani_uri_edu-LociSimulation"
# Path to scripts directory:
Scripts="$Project/LociSimulation/Scripts/0_data_prep"
# Path to output directory
Output="$Project/output/mammals"
# Path to IQTREE executable:
IQTREE="/project/pi_rsschwartz_uri_edu/Biancani/Software/iqtree-2.1.2-Linux/bin/iqtree2"
# Number of processor cores per node:
Cores=${SLURM_NTASKS_PER_NODE}

module purge

date
cd ${Output}

# --- Check input files ---
if [[ ! -f "concatenated.fasta" || ! -f "partitions.txt" ]]; then
    echo "Error: concatenated.fasta or partitions.txt not found in ${Output}"
    exit 1
fi

# --- Run IQ-TREE ---
# Flags:
#   -nt: number of CPU threads
#   -spp: partition file allowing different evolutionary rates per partition
#   -pre: prefix for output files
#   -m MFP: ModelFinder Plus for best-fit model selection
#   -bb: ultrafast bootstrap replicates
#   -alrt: SH-like approximate likelihood test replicates

${IQTREE} -nt ${Cores} \
    -s concatenated.fasta \
    -spp partitions.txt \
    -pre inferenceEmpirical \
    -m MFP -bb 1000 -alrt 1000

date
---------------------------------
sbatch 2_run_iqtree.sh
Submitted batch job 47034955
sacct -j 47034955 -o JobID,ExitCode,Elapsed,MaxRSS
